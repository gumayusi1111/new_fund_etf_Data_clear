#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾“å‡ºç®¡ç†å™¨
è´Ÿè´£å°†ç­›é€‰åçš„ETFæ•°æ®ä¿å­˜åˆ°å¯¹åº”çš„å¤æƒç›®å½•ç»“æ„ä¸­
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
import json
from datetime import datetime

from ..utils.config import get_config
from ..utils.logger import get_logger


class OutputManager:
    """è¾“å‡ºç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger()
        self.output_base = self.config.get_output_base()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.config.ensure_directories()
    
    def save_simple_results(self, processing_results: Dict[str, Any], fuquan_type: str) -> bool:
        """
        ç®€åŒ–ä¿å­˜ï¼šä¿å­˜ä¸¤ä¸ªä¸åŒé—¨æ§›çš„ç­›é€‰ç»“æœ
        
        Args:
            processing_results: å¤„ç†ç»“æœ
            fuquan_type: å¤æƒç±»å‹
        
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.output_base.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜å½“å‰é—¨æ§›çš„ç»“æœï¼ˆ5000ä¸‡ï¼‰
            self._save_threshold_results(processing_results, "5000ä¸‡é—¨æ§›")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç®€åŒ–ç»“æœå¤±è´¥: {e}")
            return False
    
    def save_dual_threshold_results(self, results_5000w: Dict[str, Any], results_3000w: Dict[str, Any]) -> bool:
        """
        ä¿å­˜åŒé—¨æ§›ç­›é€‰ç»“æœï¼š5000ä¸‡å’Œ3000ä¸‡
        
        Args:
            results_5000w: 5000ä¸‡é—¨æ§›ç­›é€‰ç»“æœ
            results_3000w: 3000ä¸‡é—¨æ§›ç­›é€‰ç»“æœ
        
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.output_base.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜5000ä¸‡é—¨æ§›ç»“æœ
            self._save_threshold_results(results_5000w, "5000ä¸‡é—¨æ§›")
            
            # ä¿å­˜3000ä¸‡é—¨æ§›ç»“æœ  
            self._save_threshold_results(results_3000w, "3000ä¸‡é—¨æ§›")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜åŒé—¨æ§›ç»“æœå¤±è´¥: {e}")
            return False
    
    def _save_threshold_results(self, processing_results: Dict[str, Any], threshold_name: str):
        """
        ä¿å­˜ç‰¹å®šé—¨æ§›çš„ç­›é€‰ç»“æœ
        
        Args:
            processing_results: å¤„ç†ç»“æœ
            threshold_name: é—¨æ§›åç§°ï¼ˆå¦‚"5000ä¸‡é—¨æ§›"ï¼‰
        """
        passed_etf_list = processing_results.get("é€šè¿‡ETF", [])
        candidate_etf_list = processing_results.get("æœ€ç»ˆç»“æœ", {}).get("å€™é€‰ETFåˆ—è¡¨", [])
        
        # åˆ›å»ºé—¨æ§›ç›®å½•
        threshold_dir = self.output_base / threshold_name
        threshold_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é€šè¿‡ç­›é€‰çš„ETFä»£ç 
        if passed_etf_list:
            passed_file = threshold_dir / "é€šè¿‡ç­›é€‰ETF.txt"
            
            with open(passed_file, 'w', encoding='utf-8') as f:
                f.write(f"# é€šè¿‡ç­›é€‰ETF - {threshold_name} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - å…±{len(passed_etf_list)}ä¸ª\n")
                for etf_code in passed_etf_list:
                    # ç§»é™¤.SZ/.SHåç¼€ï¼Œåªä¿ç•™6ä½ä»£ç 
                    clean_code = etf_code.split('.')[0] if '.' in etf_code else etf_code
                    f.write(f"{clean_code}\n")
            
            self.logger.info(f"âœ… ä¿å­˜{threshold_name}é€šè¿‡ç­›é€‰ETF: {passed_file} ({len(passed_etf_list)}ä¸ª)")
        
        # ä¿å­˜å€™é€‰ETFä»£ç 
        if candidate_etf_list:
            candidate_file = threshold_dir / "å€™é€‰ETF.txt"
            
            with open(candidate_file, 'w', encoding='utf-8') as f:
                f.write(f"# å€™é€‰ETF - {threshold_name} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - å…±{len(candidate_etf_list)}ä¸ª\n")
                for etf_code in candidate_etf_list:
                    # ç§»é™¤.SZ/.SHåç¼€ï¼Œåªä¿ç•™6ä½ä»£ç 
                    clean_code = etf_code.split('.')[0] if '.' in etf_code else etf_code
                    f.write(f"{clean_code}\n")
            
            self.logger.info(f"âœ… ä¿å­˜{threshold_name}å€™é€‰ETF: {candidate_file} ({len(candidate_etf_list)}ä¸ª)")
    
    def save_filtered_results(self, processing_results: Dict[str, Any],
                            etf_data: Dict[str, pd.DataFrame],
                            fuquan_type: str) -> bool:
        """
        ä¿å­˜ç­›é€‰ç»“æœåˆ°å¯¹åº”çš„å¤æƒç›®å½•
        
        Args:
            processing_results: å¤„ç†ç»“æœ
            etf_data: åŸå§‹ETFæ•°æ®
            fuquan_type: å¤æƒç±»å‹
        
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            passed_etf_list = processing_results.get("é€šè¿‡ETF", [])
            candidate_etf_list = processing_results.get("æœ€ç»ˆç»“æœ", {}).get("å€™é€‰ETFåˆ—è¡¨", [])
            
            # ä¿å­˜é€šè¿‡çš„ETFæ•°æ®
            success_count = 0
            if passed_etf_list:
                success_count += self._save_etf_data_to_directory(
                    passed_etf_list, etf_data, fuquan_type, "é€šè¿‡ç­›é€‰"
                )
            
            # ä¿å­˜å€™é€‰ETFæ•°æ®ï¼ˆå¯é€‰ï¼‰
            if candidate_etf_list:
                success_count += self._save_etf_data_to_directory(
                    candidate_etf_list, etf_data, fuquan_type, "å€™é€‰ETF", save_to_main=False
                )
            
            # ä¿å­˜ç­›é€‰ç»“æœæŠ¥å‘Š
            self._save_processing_report(processing_results, fuquan_type)
            
            self.logger.info(f"âœ… æˆåŠŸä¿å­˜ {success_count} ä¸ªETFæ•°æ®åˆ° {fuquan_type}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return False
    
    def save_all_processed_data(self, processing_results: Dict[str, Any],
                              source_data_loader) -> bool:
        """
        ä¿å­˜æ‰€æœ‰ä¸‰ç§å¤æƒçš„ç­›é€‰ç»“æœ
        
        Args:
            processing_results: å¤„ç†ç»“æœ
            source_data_loader: æ•°æ®åŠ è½½å™¨å®ä¾‹
        
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            fuquan_type_list = self.config.get_fuquan_types()
            passed_etf_list = processing_results.get("é€šè¿‡ETF", [])
            
            if not passed_etf_list:
                self.logger.warning("âš ï¸ æ²¡æœ‰é€šè¿‡ç­›é€‰çš„ETFï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
                return True
            
            # ä¸ºæ¯ç§å¤æƒç±»å‹ä¿å­˜æ•°æ®
            for fuquan_type in fuquan_type_list:
                self.logger.info(f"ğŸ“ ä¿å­˜ {fuquan_type} æ•°æ®...")
                
                # åŠ è½½é€šè¿‡ETFçš„è¯¥å¤æƒç±»å‹æ•°æ®
                etf_data = source_data_loader.load_multiple_etfs(passed_etf_list, fuquan_type)
                
                if etf_data:
                    # ä¿å­˜åˆ°å¯¹åº”å¤æƒç›®å½•
                    self._save_etf_data_to_directory(
                        passed_etf_list, etf_data, fuquan_type, "ç­›é€‰ç»“æœ"
                    )
                else:
                    self.logger.warning(f"âš ï¸ {fuquan_type} æ•°æ®åŠ è½½å¤±è´¥")
            
            # ä¿å­˜ç»Ÿä¸€çš„å¤„ç†æŠ¥å‘Š
            self._save_unified_report(processing_results)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ‰€æœ‰å¤æƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def _save_etf_data_to_directory(self, etf_codes: List[str],
                                  etf_data: Dict[str, pd.DataFrame],
                                  fuquan_type: str, 
                                  description: str,
                                  save_to_main: bool = True) -> int:
        """
        ä¿å­˜ETFæ•°æ®åˆ°æŒ‡å®šå¤æƒç›®å½•
        
        Args:
            etf_codes: ETFä»£ç åˆ—è¡¨
            etf_data: ETFæ•°æ®å­—å…¸
            fuquan_type: å¤æƒç±»å‹
            description: æè¿°ä¿¡æ¯
            save_to_main: æ˜¯å¦ä¿å­˜åˆ°ä¸»ç›®å½•
        
        Returns:
            æˆåŠŸä¿å­˜çš„æ–‡ä»¶æ•°
        """
        output_dir = self.output_base / fuquan_type
        output_dir.mkdir(parents=True, exist_ok=True)
        
        success_count = 0
        keep_days = self.config.get_è¾“å‡ºè®¾ç½®().get("ä¿ç•™å¤©æ•°", 252)
        
        for etf_code in etf_codes:
            if etf_code in etf_data:
                try:
                    df = etf_data[etf_code]
                    
                    # é™åˆ¶æ•°æ®é•¿åº¦
                    if len(df) > keep_days:
                        df = df.tail(keep_days)
                    
                    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
                    if 'æ—¥æœŸ' in df.columns:
                        df = df.sort_values('æ—¥æœŸ')
                    
                    # ä¿å­˜åˆ°CSVæ–‡ä»¶
                    output_file = output_dir / f"{etf_code}.csv"
                    df.to_csv(output_file, index=False, encoding='utf-8')
                    
                    success_count += 1
                    self.logger.debug(f"âœ… ä¿å­˜ETFæ•°æ®: {etf_code} -> {output_file}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ ä¿å­˜ETFæ•°æ®å¤±è´¥ {etf_code}: {e}")
        
        self.logger.info(f"ğŸ“Š {description}: ä¿å­˜ {success_count}/{len(etf_codes)} ä¸ªETFåˆ° {fuquan_type}")
        return success_count
    
    def _save_processing_report(self, processing_results: Dict[str, Any], 
                              fuquan_type: str):
        """
        ä¿å­˜å¤„ç†æŠ¥å‘Š
        
        Args:
            processing_results: å¤„ç†ç»“æœ
            fuquan_type: å¤æƒç±»å‹
        """
        try:
            report_dir = self.output_base / "reports"
            report_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"ç­›é€‰æŠ¥å‘Š_{fuquan_type}_{timestamp}.json"
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report_data = {
                "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
                "å¤æƒç±»å‹": fuquan_type,
                "å¤„ç†æ‘˜è¦": processing_results.get("å¤„ç†æ‘˜è¦", {}),
                "ç­›é€‰ç»Ÿè®¡": processing_results.get("æœ€ç»ˆç»“æœ", {}).get("ç­›é€‰ç»Ÿè®¡", {}),
                "é€šè¿‡ETF": processing_results.get("é€šè¿‡ETF", []),
                "å€™é€‰ETF": processing_results.get("æœ€ç»ˆç»“æœ", {}).get("å€™é€‰ETFåˆ—è¡¨", []),
                "ç­›é€‰å™¨é…ç½®": self._get_filter_config_summary()
            }
            
            # ä¿å­˜æŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“‹ ç­›é€‰æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å¤„ç†æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _save_unified_report(self, processing_results: Dict[str, Any]):
        """
        ä¿å­˜ç»Ÿä¸€å¤„ç†æŠ¥å‘Š
        
        Args:
            processing_results: å¤„ç†ç»“æœ
        """
        try:
            # ä¿å­˜æœ€æ–°çš„ç­›é€‰ç»“æœæ‘˜è¦
            summary_file = self.output_base / "latest_filter_results.json"
            
            summary_data = {
                "æ›´æ–°æ—¶é—´": datetime.now().isoformat(),
                "ç­›é€‰æ¦‚è¦": {
                    "é€šè¿‡ETFæ•°é‡": len(processing_results.get("é€šè¿‡ETF", [])),
                    "å€™é€‰ETFæ•°é‡": len(processing_results.get("æœ€ç»ˆç»“æœ", {}).get("å€™é€‰ETFåˆ—è¡¨", [])),
                    "æ€»å¤„ç†ETFæ•°": processing_results.get("å¤„ç†æ‘˜è¦", {}).get("æ•°æ®åŠ è½½", {}).get("æˆåŠŸåŠ è½½æ•°", 0)
                },
                "é€šè¿‡ETFåˆ—è¡¨": processing_results.get("é€šè¿‡ETF", []),
                "å¤æƒæ•°æ®å¯ç”¨æ€§": {
                    fuquan_type: True for fuquan_type in self.config.get_fuquan_types()
                }
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š ç»Ÿä¸€ç­›é€‰æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»Ÿä¸€æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _get_filter_config_summary(self) -> Dict[str, Any]:
        """è·å–ç­›é€‰å™¨é…ç½®æ‘˜è¦"""
        try:
            filter_conditions = self.config.get_ç­›é€‰æ¡ä»¶()
            return {
                "ç­›é€‰å™¨ç±»å‹": ["è´¨é‡ç­›é€‰", "æˆäº¤é‡ç­›é€‰", "è¶‹åŠ¿ç­›é€‰"],
                "ä¸»è¦æ¡ä»¶": {
                    "æœ€å°äº¤æ˜“å¤©æ•°": filter_conditions.get("åŸºç¡€æ¡ä»¶", {}).get("æœ€å°äº¤æ˜“å¤©æ•°"),
                    "æœ€å°å¹³å‡æˆäº¤é‡": filter_conditions.get("åŸºç¡€æ¡ä»¶", {}).get("æœ€å°å¹³å‡æˆäº¤é‡_æ‰‹"),
                    "æœ€å¤§å›æ’¤é™åˆ¶": filter_conditions.get("è¶‹åŠ¿æ¡ä»¶", {}).get("æœ€å¤§å›æ’¤é™åˆ¶"),
                    "æœ€å°å¤æ™®æ¯”ç‡": filter_conditions.get("è¶‹åŠ¿æ¡ä»¶", {}).get("æœ€å°å¤æ™®æ¯”ç‡")
                }
            }
        except Exception:
            return {"error": "é…ç½®æ‘˜è¦ç”Ÿæˆå¤±è´¥"}
    
    def get_output_summary(self) -> Dict[str, Any]:
        """
        è·å–è¾“å‡ºç›®å½•æ‘˜è¦
        
        Returns:
            è¾“å‡ºç›®å½•çŠ¶æ€æ‘˜è¦
        """
        try:
            summary = {
                "è¾“å‡ºåŸºç¡€è·¯å¾„": str(self.output_base),
                "å¤æƒç›®å½•çŠ¶æ€": {},
                "æŠ¥å‘Šæ–‡ä»¶": []
            }
            
            # æ£€æŸ¥å„å¤æƒç›®å½•
            for fuquan_type in self.config.get_fuquan_types():
                fuquan_dir = self.output_base / fuquan_type
                if fuquan_dir.exists():
                    csv_files = list(fuquan_dir.glob("*.csv"))
                    summary["å¤æƒç›®å½•çŠ¶æ€"][fuquan_type] = {
                        "å­˜åœ¨": True,
                        "ETFæ–‡ä»¶æ•°": len(csv_files),
                        "æœ€æ–°æ›´æ–°": self._get_directory_latest_mtime(fuquan_dir)
                    }
                else:
                    summary["å¤æƒç›®å½•çŠ¶æ€"][fuquan_type] = {
                        "å­˜åœ¨": False,
                        "ETFæ–‡ä»¶æ•°": 0,
                        "æœ€æ–°æ›´æ–°": None
                    }
            
            # æ£€æŸ¥æŠ¥å‘Šæ–‡ä»¶
            reports_dir = self.output_base / "reports"
            if reports_dir.exists():
                report_files = list(reports_dir.glob("*.json"))
                summary["æŠ¥å‘Šæ–‡ä»¶"] = [f.name for f in sorted(report_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]]
            
            return summary
            
        except Exception as e:
            self.logger.error(f"è·å–è¾“å‡ºæ‘˜è¦å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _get_directory_latest_mtime(self, directory: Path) -> str:
        """è·å–ç›®å½•ä¸­æœ€æ–°æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´"""
        try:
            if not directory.exists():
                return None
            
            files = list(directory.glob("*"))
            if not files:
                return None
            
            latest_file = max(files, key=lambda x: x.stat().st_mtime)
            mtime = datetime.fromtimestamp(latest_file.stat().st_mtime)
            return mtime.isoformat()
            
        except Exception:
            return None
    
    def clean_old_data(self, days_to_keep: int = 30) -> bool:
        """
        æ¸…ç†æ—§çš„ç­›é€‰æ•°æ®
        
        Args:
            days_to_keep: ä¿ç•™å¤©æ•°
        
        Returns:
            æ¸…ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç† {days_to_keep} å¤©å‰çš„æ•°æ®...")
            
            cutoff_time = datetime.now().timestamp() - (days_to_keep * 24 * 3600)
            cleaned_count = 0
            
            # æ¸…ç†æŠ¥å‘Šæ–‡ä»¶
            reports_dir = self.output_base / "reports"
            if reports_dir.exists():
                for report_file in reports_dir.glob("*.json"):
                    if report_file.stat().st_mtime < cutoff_time:
                        report_file.unlink()
                        cleaned_count += 1
            
            self.logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {cleaned_count} ä¸ªæ—§æ–‡ä»¶")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return False 