#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SMAç¼“å­˜ç®¡ç†å™¨ - æ™ºèƒ½å¢é‡ç¼“å­˜ç³»ç»Ÿ
=================================

åŸºäºåŸç³»ç»Ÿçš„å®Œæ•´ç¼“å­˜åŠŸèƒ½é‡æ„
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from .utils import normalize_date_format, compare_dates_safely


class SMACacheManager:
    """SMAç¼“å­˜ç®¡ç†å™¨ - æ™ºèƒ½å¢é‡ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, base_output_dir: str = "data"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼ˆç›¸å¯¹äºæ¨¡å—ç›®å½•ï¼‰
        """
        
        # ğŸ—‚ï¸ ç³»ç»Ÿç‹¬ç«‹ç¼“å­˜ç›®å½•ç»“æ„è®¾ç½®
        module_dir = os.path.dirname(__file__)
        self.cache_base_dir = os.path.join(module_dir, "..", "..", "cache")
        self.meta_dir = os.path.join(self.cache_base_dir, "meta")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_base_dir, exist_ok=True)
        os.makedirs(self.meta_dir, exist_ok=True)
        
        print("ğŸ—‚ï¸ SMAç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_base_dir}")
        print(f"   ğŸ“Š Metaç›®å½•: {self.meta_dir}")
    
    def get_cache_dir(self, threshold: str) -> str:
        """è·å–æŒ‡å®šé—¨æ§›çš„ç¼“å­˜ç›®å½•"""
        cache_dir = os.path.join(self.cache_base_dir, threshold)
        os.makedirs(cache_dir, exist_ok=True)
        return cache_dir
    
    def get_meta_file(self, threshold: str) -> str:
        """è·å–æŒ‡å®šé—¨æ§›çš„Metaæ–‡ä»¶è·¯å¾„"""
        if threshold:
            return os.path.join(self.meta_dir, f"{threshold}_meta.json")
        else:
            return os.path.join(self.meta_dir, "cache_global_meta.json")
    
    def load_meta(self, threshold: str = None) -> Dict:
        """
        åŠ è½½Metaä¿¡æ¯
        
        Args:
            threshold: é—¨æ§›ç±»å‹ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨å±€Meta
            
        Returns:
            Dict: Metaä¿¡æ¯
        """
        meta_file = self.get_meta_file(threshold)
        
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½Metaå¤±è´¥: {str(e)}")
                
        # è¿”å›é»˜è®¤Metaç»“æ„
        if threshold:
            return {
                "threshold": threshold,
                "last_update": "",
                "total_etfs": 0,
                "cached_etfs": [],
                "cache_created": datetime.now().isoformat(),
                "update_history": []
            }
        else:
            return {
                "cache_version": "1.0.0",
                "last_global_update": "",
                "total_cache_size_mb": 0,
                "total_cached_etfs": 0,
                "thresholds": ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]
            }
    
    def save_meta(self, meta_data: Dict, threshold: str = None):
        """
        ä¿å­˜Metaä¿¡æ¯
        
        Args:
            meta_data: Metaæ•°æ®
            threshold: é—¨æ§›ç±»å‹ï¼ŒNoneè¡¨ç¤ºå…¨å±€Meta
        """
        try:
            meta_file = self.get_meta_file(threshold)
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Metaå·²ä¿å­˜: {os.path.basename(meta_file)}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜Metaå¤±è´¥: {str(e)}")
    
    def get_cached_etfs(self, threshold: str) -> Set[str]:
        """
        è·å–å·²ç¼“å­˜çš„ETFä»£ç åˆ—è¡¨
        
        Args:
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Set[str]: å·²ç¼“å­˜çš„ETFä»£ç é›†åˆ
        """
        cache_dir = self.get_cache_dir(threshold)
        
        if not os.path.exists(cache_dir):
            return set()
        
        # æ‰«æç¼“å­˜ç›®å½•ä¸­çš„CSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(cache_dir) if f.endswith('.csv')]
        etf_codes = set()
        
        for csv_file in csv_files:
            # ä»æ–‡ä»¶åæå–ETFä»£ç ï¼Œå¹¶æ·»åŠ äº¤æ˜“æ‰€åç¼€
            etf_code = csv_file.replace('.csv', '')
            
            # æ™ºèƒ½æ·»åŠ äº¤æ˜“æ‰€åç¼€ä»¥åŒ¹é…ç­›é€‰ç»“æœæ ¼å¼
            if not etf_code.endswith(('.SH', '.SZ')):
                if etf_code.startswith('5'):
                    etf_code += '.SH'
                elif etf_code.startswith('1'):
                    etf_code += '.SZ'
                else:
                    # é»˜è®¤å¤„ç†å…¶ä»–ä»£ç 
                    if len(etf_code) == 6 and etf_code.isdigit():
                        # 6ä½æ•°å­—ï¼Œæ ¹æ®é¦–ä½åˆ¤æ–­
                        if etf_code[0] in '567':
                            etf_code += '.SH'
                        else:
                            etf_code += '.SZ'
            
            etf_codes.add(etf_code)
        
        return etf_codes
    
    def analyze_etf_changes(self, current_etfs: List[str], threshold: str) -> Dict[str, Set[str]]:
        """
        åˆ†æETFå˜åŒ–æƒ…å†µ
        
        Args:
            current_etfs: å½“å‰ç­›é€‰å‡ºçš„ETFåˆ—è¡¨
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Dict: åŒ…å«same_etfs, new_etfs, old_etfsçš„å­—å…¸
        """
        current_set = set(current_etfs)
        cached_set = self.get_cached_etfs(threshold)
        
        analysis = {
            'same_etfs': current_set & cached_set,     # ç›¸åŒETF - å¢é‡è®¡ç®—
            'new_etfs': current_set - cached_set,      # æ–°å¢ETF - å…¨é‡è®¡ç®—
            'old_etfs': cached_set - current_set       # æ—§ETF - ä¿æŒä¸åŠ¨
        }
        
        print(f"ğŸ“Š {threshold} ETFå˜åŒ–åˆ†æ:")
        print(f"   ğŸ”„ ç›¸åŒETF: {len(analysis['same_etfs'])} ä¸ª (å¢é‡è®¡ç®—)")
        print(f"   ğŸ†• æ–°å¢ETF: {len(analysis['new_etfs'])} ä¸ª (å…¨é‡è®¡ç®—)")
        print(f"   ğŸ“¦ ä¿ç•™ETF: {len(analysis['old_etfs'])} ä¸ª (ä¿æŒä¸åŠ¨)")
        
        return analysis
    
    def get_cached_etf_latest_date(self, etf_code: str, threshold: str) -> Optional[str]:
        """
        è·å–ç¼“å­˜ä¸­ETFçš„æœ€æ–°æ—¥æœŸ
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Optional[str]: æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸²(YYYYMMDDæ ¼å¼)ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            cache_dir = self.get_cache_dir(threshold)
            clean_etf_code = etf_code.replace('.SH', '').replace('.SZ', '')
            cache_file = os.path.join(cache_dir, f"{clean_etf_code}.csv")
            
            if not os.path.exists(cache_file):
                return None
            
            # è¯»å–ç¼“å­˜æ–‡ä»¶çš„ç¬¬ä¸€è¡Œï¼ˆé™¤äº†headerï¼‰ï¼Œå› ä¸ºæ˜¯æŒ‰æ—¶é—´å€’åº
            df = pd.read_csv(cache_file, encoding='utf-8')
            
            if df.empty:
                return None
            
            # è·å–ç¬¬ä¸€è¡Œçš„æ—¥æœŸï¼ˆæœ€æ–°æ—¥æœŸï¼‰
            latest_date = df.iloc[0]['æ—¥æœŸ']
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥æœŸæ ¼å¼åŒ–å‡½æ•°
            return normalize_date_format(latest_date)
            
        except Exception as e:
            print(f"âš ï¸ è·å–{etf_code}æœ€æ–°æ—¥æœŸå¤±è´¥: {str(e)}")
            return None
    
    def load_cached_etf_data(self, etf_code: str, threshold: str) -> Optional[pd.DataFrame]:
        """
        åŠ è½½ETFçš„ç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Optional[pd.DataFrame]: ç¼“å­˜æ•°æ®æˆ–None
        """
        try:
            cache_dir = self.get_cache_dir(threshold)
            clean_etf_code = etf_code.replace('.SH', '').replace('.SZ', '')
            cache_file = os.path.join(cache_dir, f"{clean_etf_code}.csv")
            
            if not os.path.exists(cache_file):
                return None
            
            df = pd.read_csv(cache_file, encoding='utf-8')
            
            if df.empty:
                return None
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½{etf_code}ç¼“å­˜æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def save_etf_cache(self, etf_code: str, df: pd.DataFrame, threshold: str) -> bool:
        """
        ä¿å­˜ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            df: æ•°æ®DataFrame (åŒ…å«SMAè®¡ç®—ç»“æœ)
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            cache_dir = self.get_cache_dir(threshold)
            clean_etf_code = etf_code.replace('.SH', '').replace('.SZ', '')
            cache_file = os.path.join(cache_dir, f"{clean_etf_code}.csv")
            
            # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼ˆä¸dataè¾“å‡ºä¿æŒä¸€è‡´ï¼‰
            if 'æ—¥æœŸ' in df.columns:
                df = df.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)
            
            # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
            df.to_csv(cache_file, index=False, encoding='utf-8')
            
            file_size = os.path.getsize(cache_file)
            print(f"ğŸ’¾ {etf_code}: ç¼“å­˜å·²ä¿å­˜ ({len(df)}è¡Œ, {file_size} å­—èŠ‚)")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜{etf_code}ç¼“å­˜å¤±è´¥: {str(e)}")
            return False
    
    def update_threshold_meta(self, threshold: str, analysis: Dict, processing_stats: Dict):
        """
        æ›´æ–°é—¨æ§›Metaä¿¡æ¯
        
        Args:
            threshold: é—¨æ§›ç±»å‹
            analysis: ETFå˜åŒ–åˆ†æç»“æœ
            processing_stats: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            meta = self.load_meta(threshold)
            
            # æ›´æ–°åŸºæœ¬ä¿¡æ¯
            meta["last_update"] = datetime.now().isoformat()
            meta["total_etfs"] = len(analysis['same_etfs']) + len(analysis['new_etfs'])
            meta["cached_etfs"] = list(analysis['same_etfs'] | analysis['new_etfs'])
            
            # æ·»åŠ æœ¬æ¬¡æ›´æ–°è®°å½•
            update_record = {
                "update_time": datetime.now().isoformat(),
                "same_etfs_count": len(analysis['same_etfs']),
                "new_etfs_count": len(analysis['new_etfs']),
                "old_etfs_count": len(analysis['old_etfs']),
                "processing_stats": processing_stats
            }
            
            if "update_history" not in meta:
                meta["update_history"] = []
            
            meta["update_history"].append(update_record)
            
            # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡50æ¡
            if len(meta["update_history"]) > 50:
                meta["update_history"] = meta["update_history"][-50:]
            
            # ä¿å­˜Meta
            self.save_meta(meta, threshold)
            
        except Exception as e:
            print(f"âŒ æ›´æ–°{threshold} Metaå¤±è´¥: {str(e)}")
    
    def get_cache_statistics(self) -> Dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡
        """
        stats = {
            "global": self.load_meta(None),
            "thresholds": {}
        }
        
        for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
            threshold_meta = self.load_meta(threshold)
            cache_dir = self.get_cache_dir(threshold)
            
            # ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶æ•°é‡
            cache_files = 0
            if os.path.exists(cache_dir):
                cache_files = len([f for f in os.listdir(cache_dir) if f.endswith('.csv')])
            
            stats["thresholds"][threshold] = {
                "meta": threshold_meta,
                "cache_files": cache_files
            }
        
        return stats 