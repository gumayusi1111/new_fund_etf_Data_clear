#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SMAä¸»æ§åˆ¶å™¨
=========

è´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶ï¼Œå¤„ç†ä¸»è¦ä¸šåŠ¡æµç¨‹
"""

import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from .etf_processor import ETFProcessor
from .batch_processor import BatchProcessor
from ..infrastructure.config import SMAConfig
from ..infrastructure.data_reader import ETFDataReader
from ..infrastructure.cache_manager import SMACacheManager
from ..engines.sma_engine import SMAEngine
from ..outputs.csv_handler import CSVOutputHandler
from ..outputs.display_formatter import DisplayFormatter


class SMAMainController:
    """SMAä¸»æ§åˆ¶å™¨ - åè°ƒæ‰€æœ‰ç»„ä»¶çš„ä¸šåŠ¡é€»è¾‘"""
    
    def __init__(self, adj_type: str = "å‰å¤æƒ", sma_periods: Optional[List[int]] = None, 
                 output_dir: Optional[str] = None, enable_cache: bool = True):
        """
        åˆå§‹åŒ–ä¸»æ§åˆ¶å™¨
        
        Args:
            adj_type: å¤æƒç±»å‹
            sma_periods: SMAå‘¨æœŸåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        # åˆå§‹åŒ–é…ç½®
        self.config = SMAConfig(
            adj_type=adj_type,
            sma_periods=sma_periods or [5, 10, 20, 60]
        )
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        module_dir = os.path.dirname(__file__)
        self.output_dir = output_dir or os.path.join(module_dir, "..", "..", "data")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.data_reader = ETFDataReader(config=self.config)
        self.sma_engine = SMAEngine(config=self.config)
        self.csv_handler = CSVOutputHandler()
        self.display_formatter = DisplayFormatter()
        
        # åˆå§‹åŒ–ç¼“å­˜ç»„ä»¶
        self.enable_cache = enable_cache
        self.cache_manager = None
        if enable_cache:
            self.cache_manager = SMACacheManager()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.etf_processor = ETFProcessor(
            data_reader=self.data_reader,
            sma_engine=self.sma_engine,
            config=self.config
        )
        
        self.batch_processor = BatchProcessor(
            etf_processor=self.etf_processor,
            cache_manager=self.cache_manager,
            csv_handler=self.csv_handler,
            enable_cache=enable_cache
        )
        
        print("ğŸ—‚ï¸ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿå·²å¯ç”¨" if enable_cache else "âš ï¸ ç¼“å­˜ç³»ç»Ÿå·²ç¦ç”¨")
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)
    
    def process_single_etf(self, etf_code: str, include_advanced_analysis: bool = False) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªETF
        
        Args:
            etf_code: ETFä»£ç 
            include_advanced_analysis: æ˜¯å¦åŒ…å«é«˜çº§åˆ†æ
            
        Returns:
            Optional[Dict]: å¤„ç†ç»“æœ
        """
        print(f"ğŸ”„ å¼€å§‹å¤„ç†ETF: {etf_code}")
        
        result = self.etf_processor.process_single_etf(etf_code, include_advanced_analysis)
        
        if result:
            # æ˜¾ç¤ºç»“æœ
            self.display_formatter.display_single_etf_result(result)
            return result
        else:
            print(f"âŒ ETF {etf_code} å¤„ç†å¤±è´¥")
            return None
    
    def process_screening_results(self, thresholds: List[str] = None, 
                                include_advanced_analysis: bool = False):
        """
        å¤„ç†ETFç­›é€‰ç»“æœ
        
        Args:
            thresholds: é—¨æ§›åˆ—è¡¨
            include_advanced_analysis: æ˜¯å¦åŒ…å«é«˜çº§åˆ†æ
            
        Returns:
            tuple: (æŒ‰é—¨æ§›ç»„ç»‡çš„å¤„ç†ç»“æœ, è¿è¡Œæ—¶ç¼“å­˜ç»Ÿè®¡)
        """
        if thresholds is None:
            thresholds = ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]
        
        print(f"ğŸ“Š å¤„ç†é—¨æ§›: {', '.join(thresholds)}")
        
        all_results = {}
        runtime_cache_stats = {
            'total_etfs_processed': 0,
            'total_cache_hits': 0,
            'total_incremental_updates': 0,
            'total_new_calculations': 0,
            'total_failed': 0,
            'thresholds_stats': {}
        }
        
        for threshold in thresholds:
            print(f"\nğŸ“Š {threshold}: è¯»å–ç­›é€‰ç»“æœ...")
            
            # è¯»å–ç­›é€‰ç»“æœ
            etf_codes = self._load_screening_results(threshold)
            
            if not etf_codes:
                print(f"âŒ {threshold}: æœªæ‰¾åˆ°ç­›é€‰ç»“æœ")
                continue
            
            print(f"ğŸ“Š {threshold}: æ‰¾åˆ° {len(etf_codes)} ä¸ªé€šè¿‡ç­›é€‰çš„ETF")
            
            # è®°å½•å¤„ç†å‰çš„çŠ¶æ€ï¼Œä»¥ä¾¿è®¡ç®—æœ¬æ¬¡è¿è¡Œçš„ç»Ÿè®¡
            threshold_start_etfs = len(etf_codes)
            
            # æ‰¹é‡å¤„ç†ETF
            results = self.batch_processor.process_etf_list(
                etf_codes=etf_codes,
                threshold=threshold,
                include_advanced_analysis=include_advanced_analysis
            )
            
            all_results[threshold] = results
            
            # ä»é—¨æ§›Metaä¸­è·å–æœ€æ–°çš„å¤„ç†ç»Ÿè®¡
            if self.enable_cache and self.cache_manager:
                threshold_meta = self.cache_manager.load_meta(threshold)
                if threshold_meta.get('update_history'):
                    latest_update = threshold_meta['update_history'][-1]
                    processing_stats = latest_update.get('processing_stats', {})
                    
                    # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
                    cache_hits = processing_stats.get('cache_hits', 0)
                    incremental_updates = processing_stats.get('incremental_updates', 0) 
                    new_calculations = processing_stats.get('new_calculations', 0)
                    failed_count = processing_stats.get('failed_count', 0)
                    
                    runtime_cache_stats['total_etfs_processed'] += threshold_start_etfs
                    runtime_cache_stats['total_cache_hits'] += cache_hits
                    runtime_cache_stats['total_incremental_updates'] += incremental_updates
                    runtime_cache_stats['total_new_calculations'] += new_calculations
                    runtime_cache_stats['total_failed'] += failed_count
                    
                    # ä¿å­˜æ¯ä¸ªé—¨æ§›çš„ç»Ÿè®¡
                    runtime_cache_stats['thresholds_stats'][threshold] = {
                        'etfs_processed': threshold_start_etfs,
                        'cache_hits': cache_hits,
                        'cache_hit_rate': processing_stats.get('cache_hit_rate', 0),
                        'incremental_updates': incremental_updates,
                        'new_calculations': new_calculations,
                        'failed_count': failed_count
                    }
        
        # è®¡ç®—æ€»ä½“ç¼“å­˜å‘½ä¸­ç‡ï¼ˆå®‰å…¨é™¤æ³•ï¼‰
        total_processed = runtime_cache_stats['total_etfs_processed']
        total_hits = runtime_cache_stats['total_cache_hits']
        
        if total_processed > 0 and total_hits >= 0:
            runtime_cache_stats['overall_cache_hit_rate'] = total_hits / total_processed
        else:
            runtime_cache_stats['overall_cache_hit_rate'] = 0.0
        
        return all_results, runtime_cache_stats
    
    def calculate_and_save_screening_results(self, thresholds: List[str] = None, 
                                           output_dir: Optional[str] = None,
                                           include_advanced_analysis: bool = False) -> Dict[str, Any]:
        """
        è®¡ç®—å¹¶ä¿å­˜åŸºäºç­›é€‰ç»“æœçš„SMAæ•°æ® - å®Œæ•´æµç¨‹
        
        Args:
            thresholds: é—¨æ§›åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            include_advanced_analysis: æ˜¯å¦åŒ…å«é«˜çº§åˆ†æ
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœç»Ÿè®¡
        """
        print("ğŸš€ å¼€å§‹åŸºäºETFç­›é€‰ç»“æœçš„SMAæ‰¹é‡è®¡ç®—...")
        start_time = datetime.now()
        
        try:
            # æ­¥éª¤1: å¤„ç†ç­›é€‰ç»“æœ
            screening_results, runtime_cache_stats = self.process_screening_results(
                thresholds=thresholds,
                include_advanced_analysis=include_advanced_analysis
            )
            
            if not screening_results:
                return {
                    'success': False,
                    'message': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç­›é€‰ç»“æœ'
                }
            
            # æ­¥éª¤2: ä¿å­˜ç»“æœ
            save_stats = self._save_threshold_results(screening_results, output_dir or self.output_dir)
            
            # æ­¥éª¤3: æ›´æ–°å…¨å±€Meta
            if self.enable_cache and self.cache_manager:
                self._update_global_cache_meta()
            
            # æ„å»ºè¿”å›ç»“æœ
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            total_etfs = sum(len(results) for results in screening_results.values())
            
            result_summary = {
                'success': True,
                'total_etfs_processed': total_etfs,
                'thresholds_processed': len(screening_results),
                'output_directory': output_dir or self.output_dir,
                'save_statistics': save_stats,
                'processing_time_seconds': processing_time,
                'timestamp': end_time.isoformat()
            }
            
            # æ·»åŠ è¿è¡Œæ—¶ç¼“å­˜ç»Ÿè®¡ï¼ˆä¿®å¤çš„éƒ¨åˆ†ï¼‰
            if self.enable_cache and self.cache_manager:
                result_summary['cache_statistics'] = {
                    'cache_hit_rate': runtime_cache_stats['overall_cache_hit_rate'] * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                    'total_cache_hits': runtime_cache_stats['total_cache_hits'],
                    'total_incremental_updates': runtime_cache_stats['total_incremental_updates'],
                    'total_new_calculations': runtime_cache_stats['total_new_calculations'],
                    'total_failed': runtime_cache_stats['total_failed'],
                    'threshold_details': runtime_cache_stats['thresholds_stats']
                }
            
            return result_summary
            
        except Exception as e:
            return {
                'success': False,
                'message': f'å¤„ç†å¤±è´¥: {str(e)}'
            }
    
    def _load_screening_results(self, threshold: str) -> List[str]:
        """è¯»å–ç­›é€‰ç»“æœæ–‡ä»¶"""
        try:
            # ä½¿ç”¨æ•°æ®è¯»å–å™¨çš„æ–¹æ³•æ¥è·å–ç­›é€‰ç»“æœ
            etf_codes = self.data_reader.get_screening_etf_codes(threshold)
            
            if not etf_codes:
                print(f"âš ï¸ {threshold}: æœªæ‰¾åˆ°ç­›é€‰ç»“æœ")
            else:
                print(f"ğŸ“Š {threshold}: æ‰¾åˆ° {len(etf_codes)} ä¸ªé€šè¿‡ç­›é€‰çš„ETF")
            
            return etf_codes
            
        except Exception as e:
            print(f"âŒ è¯»å–ç­›é€‰ç»“æœå¤±è´¥: {str(e)}")
            return []
    
    def _save_threshold_results(self, results: Dict[str, List[Dict]], output_base_dir: str) -> Dict:
        """ä¿å­˜é—¨æ§›ç»“æœåˆ°dataç›®å½•"""
        save_stats = {
            'total_files_saved': 0,
            'total_size_bytes': 0,
            'thresholds': {}
        }
        
        for threshold, results_list in results.items():
            if not results_list:
                continue
            
            # ä½¿ç”¨æ‰¹é‡å¤„ç†å™¨ä¿å­˜ç»“æœ
            threshold_stats = self.batch_processor.save_results_to_files(
                results_list, output_base_dir, threshold
            )
            
            save_stats['thresholds'][threshold] = threshold_stats
            save_stats['total_files_saved'] += threshold_stats['files_saved']
            save_stats['total_size_bytes'] += threshold_stats['total_size']
            
            print(f"âœ… {threshold}: æˆåŠŸä¿å­˜ {threshold_stats['files_saved']} ä¸ªå†å²æ–‡ä»¶")
            if threshold_stats['failed_saves'] > 0:
                print(f"âš ï¸  {threshold}: {threshold_stats['failed_saves']} ä¸ªæ–‡ä»¶ä¿å­˜å¤±è´¥")
        
        return save_stats
    
    def _update_global_cache_meta(self):
        """æ›´æ–°å…¨å±€ç¼“å­˜Metaä¿¡æ¯"""
        try:
            if self.cache_manager:
                # æ›´æ–°å…¨å±€Meta
                global_meta = self.cache_manager.load_meta(None)
                global_meta["last_global_update"] = datetime.now().isoformat()
                
                # ç»Ÿè®¡ç¼“å­˜æ€»å¤§å°
                total_size = 0
                total_etfs = 0
                
                for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                    cache_dir = self.cache_manager.get_cache_dir(threshold)
                    if os.path.exists(cache_dir):
                        for file in os.listdir(cache_dir):
                            if file.endswith('.csv'):
                                file_path = os.path.join(cache_dir, file)
                                total_size += os.path.getsize(file_path)
                                total_etfs += 1
                
                global_meta["total_cache_size_mb"] = round(total_size / 1024 / 1024, 2)
                global_meta["total_cached_etfs"] = total_etfs
                
                # ä¿å­˜å…¨å±€Meta
                self.cache_manager.save_meta(global_meta, None)
                
        except Exception as e:
            print(f"âŒ æ›´æ–°å…¨å±€Metaå¤±è´¥: {str(e)}")
    
    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            status = {
                'system_info': {
                    'version': '2.0.0',
                    'config': f'SMAç³»ç»Ÿ - {self.config.adj_type}',
                    'data_path': 'ETFæ•°æ®è·¯å¾„',
                    'output_path': self.output_dir
                },
                'data_status': {
                    'available_etfs_count': len(self.data_reader.get_available_etfs()),
                    'data_path_valid': True,
                    'sample_etfs': self.data_reader.get_available_etfs()[:5]
                },
                'components': {
                    'Data Reader': 'Ready',
                    'SMA Engine': 'Ready',
                    'CSV Handler': 'Ready',
                    'Cache Manager': 'Ready' if self.enable_cache else 'Disabled'
                }
            }
            
            return status
            
        except Exception as e:
            return {'error': f'ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}'}
    
    def calculate_historical_batch(self, thresholds: List[str] = None, 
                                 output_dir: Optional[str] = None,
                                 verbose: bool = False) -> Dict[str, Any]:
        """
        ğŸš€ è¶…é«˜æ€§èƒ½å†å²æ‰¹é‡è®¡ç®— - å®Œæ•´æµç¨‹
        
        ä½¿ç”¨ä¸“é—¨çš„SMAHistoricalCalculatorè¿›è¡Œå‘é‡åŒ–æ‰¹é‡è®¡ç®—
        é¢„æœŸæ€§èƒ½æå‡ï¼š50-100å€
        
        Args:
            thresholds: é—¨æ§›åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            verbose: è¯¦ç»†è¾“å‡º
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœç»Ÿè®¡
        """
        print("ğŸš€ å¼€å§‹è¶…é«˜æ€§èƒ½å†å²æ‰¹é‡è®¡ç®—...")
        print("âš¡ ä½¿ç”¨å‘é‡åŒ–è®¡ç®—å¼•æ“ï¼Œé¢„æœŸæ€§èƒ½æå‡50-100å€")
        start_time = datetime.now()
        
        try:
            # åˆå§‹åŒ–è¶…é«˜æ€§èƒ½å†å²è®¡ç®—å™¨
            from ..engines.sma_historical_calculator import SMAHistoricalCalculator
            historical_calculator = SMAHistoricalCalculator(self.config)
            
            if thresholds is None:
                thresholds = ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            output_directory = output_dir or self.output_dir
            
            total_etfs_processed = 0
            total_files_saved = 0
            total_size_bytes = 0
            processing_stats = {}
            
            for threshold in thresholds:
                print(f"\nğŸ“Š {threshold}: è¯»å–ç­›é€‰ç»“æœ...")
                
                # è¯»å–ç­›é€‰ç»“æœ
                etf_codes = self._load_screening_results(threshold)
                
                if not etf_codes:
                    print(f"âŒ {threshold}: æœªæ‰¾åˆ°ç­›é€‰ç»“æœ")
                    continue
                
                print(f"ğŸ“Š {threshold}: æ‰¾åˆ° {len(etf_codes)} ä¸ªé€šè¿‡ç­›é€‰çš„ETF")
                print(f"ğŸš€ å¼€å§‹è¶…é«˜æ€§èƒ½æ‰¹é‡è®¡ç®—ï¼ˆæ™ºèƒ½ç¼“å­˜æ¨¡å¼ï¼‰...")
                
                # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡å¤„ç†å™¨ï¼ˆåŒ…å«ç¼“å­˜é€»è¾‘ï¼‰è¿›è¡Œå¤„ç†
                calculation_start = datetime.now()
                results = self.batch_processor.process_etf_list(
                    etf_codes=etf_codes,
                    threshold=threshold,
                    include_advanced_analysis=False
                )
                calculation_end = datetime.now()
                calculation_time = (calculation_end - calculation_start).total_seconds()
                
                # è½¬æ¢ç»“æœæ ¼å¼ä»¥é€‚é…ç»Ÿè®¡é€»è¾‘
                results_dict = {}
                results_for_save = []
                for result in results:
                    if result and 'etf_code' in result:
                        etf_code = result['etf_code']
                        if 'sma_data' in result:
                            results_dict[etf_code] = result['sma_data']
                        # ä¸ºä¿å­˜å‡†å¤‡ç»“æœæ ¼å¼
                        if 'historical_data' in result:
                            results_for_save.append(result)
                
                # ä¿å­˜ç»“æœåˆ°dataç›®å½•
                save_start = datetime.now()
                save_stats = self.batch_processor.save_results_to_files(
                    results_for_save, output_directory, threshold
                )
                save_end = datetime.now()
                save_time = (save_end - save_start).total_seconds()
                
                # ç»Ÿè®¡ä¿¡æ¯
                threshold_etfs = len(results_dict)
                total_etfs_processed += threshold_etfs
                total_files_saved += save_stats.get('files_saved', 0)
                
                # è®¡ç®—æ–‡ä»¶å¤§å°
                threshold_dir = os.path.join(output_directory, threshold)
                threshold_size = 0
                if os.path.exists(threshold_dir):
                    for file in os.listdir(threshold_dir):
                        if file.endswith('.csv'):
                            file_path = os.path.join(threshold_dir, file)
                            threshold_size += os.path.getsize(file_path)
                total_size_bytes += threshold_size
                
                processing_stats[threshold] = {
                    'etfs_processed': threshold_etfs,
                    'files_saved': threshold_etfs,
                    'calculation_time': calculation_time,
                    'save_time': save_time,
                    'etfs_per_second': threshold_etfs / calculation_time if calculation_time > 0 else 0
                }
                
                if verbose:
                    print(f"ğŸ“Š {threshold} è¯¦ç»†ç»Ÿè®¡:")
                    print(f"   âš¡ è®¡ç®—æ—¶é—´: {calculation_time:.2f}ç§’")
                    print(f"   ğŸ’¾ ä¿å­˜æ—¶é—´: {save_time:.2f}ç§’") 
                    print(f"   ğŸš€ è®¡ç®—é€Ÿåº¦: {processing_stats[threshold]['etfs_per_second']:.1f} ETF/ç§’")
            
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'success': True,
                'total_etfs_processed': total_etfs_processed,
                'thresholds_processed': len([t for t in thresholds if t in processing_stats]),
                'processing_time_seconds': total_time,
                'etfs_per_second': total_etfs_processed / total_time if total_time > 0 else 0,
                'output_directory': output_directory,
                'save_statistics': {
                    'total_files_saved': total_files_saved,
                    'total_size_bytes': total_size_bytes
                },
                'processing_details': processing_stats
            }
            
            print(f"\nğŸ‰ è¶…é«˜æ€§èƒ½å†å²æ‰¹é‡è®¡ç®—å®Œæˆï¼")
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            print(f"   ğŸ“ å¤„ç†ETFæ•°: {total_etfs_processed}")
            print(f"   ğŸ“ å¤„ç†é—¨æ§›æ•°: {result['thresholds_processed']}")
            print(f"   â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   ğŸš€ å¹³å‡å¤„ç†é€Ÿåº¦: {result['etfs_per_second']:.1f} ETF/ç§’")
            print(f"   ğŸ’¾ ä¿å­˜æ–‡ä»¶: {total_files_saved}")
            print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {total_size_bytes/1024/1024:.2f}MB")
            
            return result
            
        except Exception as e:
            error_message = f"è¶…é«˜æ€§èƒ½å†å²æ‰¹é‡è®¡ç®—å¤±è´¥: {str(e)}"
            print(f"âŒ {error_message}")
            if verbose:
                import traceback
                traceback.print_exc()
            
            return {
                'success': False,
                'message': error_message,
                'total_etfs_processed': 0
            }
    
    def _get_etf_files_dict(self, etf_codes: List[str]) -> Dict[str, str]:
        """
        è·å–ETFæ–‡ä»¶è·¯å¾„å­—å…¸
        
        Args:
            etf_codes: ETFä»£ç åˆ—è¡¨
            
        Returns:
            Dict[str, str]: ETFä»£ç åˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        """
        etf_files_dict = {}
        
        for etf_code in etf_codes:
            try:
                # ä½¿ç”¨æ•°æ®è¯»å–å™¨è·å–æ–‡ä»¶è·¯å¾„
                file_path = self.data_reader.get_etf_file_path(etf_code)
                if file_path and os.path.exists(file_path):
                    etf_files_dict[etf_code] = file_path
            except Exception:
                # è·³è¿‡æ— æ³•è·å–è·¯å¾„çš„ETF
                continue
        
        return etf_files_dict 