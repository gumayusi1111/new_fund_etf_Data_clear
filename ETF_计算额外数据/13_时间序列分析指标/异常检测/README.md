# å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡

## æ¦‚è¿°
å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡ç”¨äºè¯†åˆ«ETFä»·æ ¼æˆ–æˆäº¤é‡ä¸­çš„å¼‚å¸¸äº‹ä»¶ï¼Œè¿™äº›å¼‚å¸¸å¾€å¾€å¯¹åº”é‡è¦çš„å¸‚åœºä¿¡æ¯ã€çªå‘äº‹ä»¶æˆ–äº¤æ˜“æœºä¼šã€‚é€šè¿‡ç»Ÿè®¡å­¦æ–¹æ³•è‡ªåŠ¨è¯†åˆ«åç¦»æ­£å¸¸æ¨¡å¼çš„æ•°æ®ç‚¹ã€‚

## ä¸»è¦æ£€æµ‹æ–¹æ³•

### 1. Z-Scoreå¼‚å¸¸æ£€æµ‹
```python
import pandas as pd
import numpy as np
from scipy import stats

def zscore_anomaly_detection(data, window=20, threshold=2.5):
    """
    åŸºäºZ-Scoreçš„å¼‚å¸¸æ£€æµ‹
    
    Parameters:
    data: æ•°æ®åºåˆ—
    window: æ»šåŠ¨çª—å£å¤§å°
    threshold: å¼‚å¸¸é˜ˆå€¼ (é€šå¸¸2-3)
    """
    # è®¡ç®—æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®
    rolling_mean = data.rolling(window).mean()
    rolling_std = data.rolling(window).std()
    
    # è®¡ç®—Z-Score
    z_scores = (data - rolling_mean) / rolling_std
    
    # è¯†åˆ«å¼‚å¸¸ç‚¹
    anomalies = abs(z_scores) > threshold
    
    return {
        'z_scores': z_scores,
        'anomalies': anomalies,
        'anomaly_values': data[anomalies],
        'anomaly_dates': data.index[anomalies],
        'anomaly_count': anomalies.sum()
    }
```

### 2. IQRå¼‚å¸¸æ£€æµ‹
```python
def iqr_anomaly_detection(data, window=20, k=1.5):
    """
    åŸºäºå››åˆ†ä½æ•°èŒƒå›´(IQR)çš„å¼‚å¸¸æ£€æµ‹
    """
    def detect_outliers(series):
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - k * IQR
        upper_bound = Q3 + k * IQR
        
        return (series < lower_bound) | (series > upper_bound)
    
    # æ»šåŠ¨IQRå¼‚å¸¸æ£€æµ‹
    anomalies = data.rolling(window).apply(
        lambda x: detect_outliers(x).iloc[-1] if len(x) == window else False
    ).astype(bool)
    
    return {
        'anomalies': anomalies,
        'anomaly_values': data[anomalies],
        'anomaly_dates': data.index[anomalies],
        'anomaly_count': anomalies.sum()
    }
```

### 3. ç§»åŠ¨ä¸­ä½æ•°ç»å¯¹åå·®(MAD)
```python
def mad_anomaly_detection(data, window=20, threshold=3):
    """
    åŸºäºç§»åŠ¨ä¸­ä½æ•°ç»å¯¹åå·®çš„å¼‚å¸¸æ£€æµ‹
    """
    def calculate_mad(series):
        median = series.median()
        mad = np.median(np.abs(series - median))
        return mad
    
    rolling_median = data.rolling(window).median()
    rolling_mad = data.rolling(window).apply(calculate_mad)
    
    # è®¡ç®—ä¿®æ­£Z-Score
    modified_z_scores = 0.6745 * (data - rolling_median) / rolling_mad
    
    # è¯†åˆ«å¼‚å¸¸
    anomalies = abs(modified_z_scores) > threshold
    
    return {
        'modified_z_scores': modified_z_scores,
        'anomalies': anomalies,
        'anomaly_values': data[anomalies],
        'rolling_mad': rolling_mad
    }
```

## ä»·æ ¼å¼‚å¸¸æ£€æµ‹

### ä»·æ ¼è·³è·ƒæ£€æµ‹
```python
def price_jump_detection(price_data, threshold_pct=0.05):
    """
    æ£€æµ‹ä»·æ ¼å¼‚å¸¸è·³è·ƒ
    
    Parameters:
    price_data: ä»·æ ¼åºåˆ—
    threshold_pct: è·³è·ƒé˜ˆå€¼ç™¾åˆ†æ¯”
    """
    # è®¡ç®—æ—¥æ”¶ç›Šç‡
    returns = price_data.pct_change()
    
    # æ£€æµ‹è·³è·ƒ
    jumps_up = returns > threshold_pct
    jumps_down = returns < -threshold_pct
    
    all_jumps = jumps_up | jumps_down
    
    jump_analysis = []
    for date in returns.index[all_jumps]:
        jump_size = returns[date]
        jump_type = "å‘ä¸Šè·³è·ƒ" if jump_size > 0 else "å‘ä¸‹è·³è·ƒ"
        
        jump_analysis.append({
            'date': date,
            'jump_size': jump_size,
            'jump_percentage': jump_size * 100,
            'jump_type': jump_type,
            'price_before': price_data[date - pd.Timedelta(days=1)] if date != price_data.index[0] else None,
            'price_after': price_data[date]
        })
    
    return {
        'jumps_up': jumps_up,
        'jumps_down': jumps_down,
        'all_jumps': all_jumps,
        'jump_analysis': jump_analysis,
        'jump_count': all_jumps.sum()
    }
```

### ä»·æ ¼åç¦»æ£€æµ‹
```python
def price_deviation_detection(price_data, benchmark_data, window=20, threshold=0.1):
    """
    æ£€æµ‹ç›¸å¯¹äºåŸºå‡†çš„ä»·æ ¼åç¦»å¼‚å¸¸
    """
    # è®¡ç®—ç›¸å¯¹è¡¨ç°
    price_returns = price_data.pct_change()
    benchmark_returns = benchmark_data.pct_change()
    
    relative_performance = price_returns - benchmark_returns
    
    # ä½¿ç”¨Z-Scoreæ£€æµ‹å¼‚å¸¸åç¦»
    result = zscore_anomaly_detection(relative_performance, window, threshold)
    
    # å¢åŠ åç¦»æ–¹å‘å’Œç¨‹åº¦
    deviation_analysis = []
    for date in result['anomaly_dates']:
        deviation = relative_performance[date]
        deviation_type = "è¶…é¢è¡¨ç°" if deviation > 0 else "è½åè¡¨ç°"
        
        deviation_analysis.append({
            'date': date,
            'deviation': deviation,
            'deviation_percentage': deviation * 100,
            'deviation_type': deviation_type,
            'z_score': result['z_scores'][date]
        })
    
    result['deviation_analysis'] = deviation_analysis
    return result
```

## æˆäº¤é‡å¼‚å¸¸æ£€æµ‹

### æˆäº¤é‡æ¿€å¢æ£€æµ‹
```python
def volume_surge_detection(volume_data, multiplier=3, window=20):
    """
    æ£€æµ‹æˆäº¤é‡å¼‚å¸¸æ¿€å¢
    """
    # è®¡ç®—æ»šåŠ¨å¹³å‡æˆäº¤é‡
    volume_ma = volume_data.rolling(window).mean()
    
    # æ£€æµ‹æ¿€å¢
    volume_surge = volume_data > (volume_ma * multiplier)
    
    surge_analysis = []
    for date in volume_data.index[volume_surge]:
        current_volume = volume_data[date]
        avg_volume = volume_ma[date]
        surge_ratio = current_volume / avg_volume
        
        surge_analysis.append({
            'date': date,
            'current_volume': current_volume,
            'average_volume': avg_volume,
            'surge_ratio': surge_ratio,
            'surge_percentage': (surge_ratio - 1) * 100
        })
    
    return {
        'volume_surge': volume_surge,
        'surge_analysis': surge_analysis,
        'surge_count': volume_surge.sum(),
        'volume_ma': volume_ma
    }
```

### æˆäº¤é‡æ¯ç«­æ£€æµ‹
```python
def volume_drought_detection(volume_data, percentile=10, window=20):
    """
    æ£€æµ‹æˆäº¤é‡å¼‚å¸¸æ¯ç«­
    """
    # è®¡ç®—æ»šåŠ¨åˆ†ä½æ•°
    volume_threshold = volume_data.rolling(window).quantile(percentile/100)
    
    # æ£€æµ‹æ¯ç«­
    volume_drought = volume_data < volume_threshold
    
    drought_analysis = []
    for date in volume_data.index[volume_drought]:
        current_volume = volume_data[date]
        threshold = volume_threshold[date]
        
        drought_analysis.append({
            'date': date,
            'current_volume': current_volume,
            'threshold': threshold,
            'drought_severity': (threshold - current_volume) / threshold
        })
    
    return {
        'volume_drought': volume_drought,
        'drought_analysis': drought_analysis,
        'drought_count': volume_drought.sum()
    }
```

## å¤åˆå¼‚å¸¸æ£€æµ‹

### ä»·é‡èƒŒç¦»å¼‚å¸¸
```python
def price_volume_divergence_detection(price_data, volume_data):
    """
    æ£€æµ‹ä»·é‡èƒŒç¦»å¼‚å¸¸
    """
    price_returns = price_data.pct_change()
    volume_changes = volume_data.pct_change()
    
    # å®šä¹‰èƒŒç¦»æ¡ä»¶
    # ä»·æ¶¨é‡è·ŒèƒŒç¦»
    price_up_volume_down = (price_returns > 0.02) & (volume_changes < -0.1)
    
    # ä»·è·Œé‡æ¶¨èƒŒç¦»  
    price_down_volume_up = (price_returns < -0.02) & (volume_changes > 0.1)
    
    all_divergences = price_up_volume_down | price_down_volume_up
    
    divergence_analysis = []
    for date in price_data.index[all_divergences]:
        price_change = price_returns[date]
        volume_change = volume_changes[date]
        
        if price_up_volume_down[date]:
            divergence_type = "ä»·æ¶¨é‡è·ŒèƒŒç¦»"
        else:
            divergence_type = "ä»·è·Œé‡æ¶¨èƒŒç¦»"
        
        divergence_analysis.append({
            'date': date,
            'divergence_type': divergence_type,
            'price_change': price_change * 100,
            'volume_change': volume_change * 100
        })
    
    return {
        'price_up_volume_down': price_up_volume_down,
        'price_down_volume_up': price_down_volume_up,
        'all_divergences': all_divergences,
        'divergence_analysis': divergence_analysis
    }
```

### å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹
```python
def multi_dimensional_anomaly_detection(price_data, volume_data, window=20):
    """
    å¤šç»´åº¦ç»¼åˆå¼‚å¸¸æ£€æµ‹
    """
    # å„ç§å¼‚å¸¸æ£€æµ‹
    price_anomalies = zscore_anomaly_detection(price_data.pct_change(), window)
    volume_anomalies = zscore_anomaly_detection(volume_data, window)
    jump_anomalies = price_jump_detection(price_data)
    divergence_anomalies = price_volume_divergence_detection(price_data, volume_data)
    
    # ç»¼åˆå¼‚å¸¸è¯„åˆ†
    all_dates = price_data.index
    anomaly_scores = pd.Series(0.0, index=all_dates)
    
    # ä»·æ ¼å¼‚å¸¸ (æƒé‡30%)
    anomaly_scores[price_anomalies['anomalies']] += 0.3
    
    # æˆäº¤é‡å¼‚å¸¸ (æƒé‡20%)
    anomaly_scores[volume_anomalies['anomalies']] += 0.2
    
    # ä»·æ ¼è·³è·ƒ (æƒé‡30%)
    anomaly_scores[jump_anomalies['all_jumps']] += 0.3
    
    # ä»·é‡èƒŒç¦» (æƒé‡20%)
    anomaly_scores[divergence_anomalies['all_divergences']] += 0.2
    
    # è¯†åˆ«é«˜å¼‚å¸¸è¯„åˆ†çš„æ—¥æœŸ
    high_anomaly_days = anomaly_scores[anomaly_scores >= 0.5]
    
    return {
        'anomaly_scores': anomaly_scores,
        'high_anomaly_days': high_anomaly_days,
        'price_anomalies': price_anomalies,
        'volume_anomalies': volume_anomalies,
        'jump_anomalies': jump_anomalies,
        'divergence_anomalies': divergence_anomalies
    }
```

## å®é™…åº”ç”¨ç­–ç•¥

### äº‹ä»¶é©±åŠ¨äº¤æ˜“
```python
def event_driven_trading_signals(anomaly_results, price_data):
    """
    åŸºäºå¼‚å¸¸æ£€æµ‹çš„äº‹ä»¶é©±åŠ¨äº¤æ˜“ä¿¡å·
    """
    signals = []
    
    for date, score in anomaly_results['high_anomaly_days'].items():
        price_change = price_data.pct_change()[date]
        
        # æ ¹æ®å¼‚å¸¸ç±»å‹å’Œä»·æ ¼å˜åŒ–ç”Ÿæˆä¿¡å·
        if score >= 0.7:  # å¼ºå¼‚å¸¸
            if price_change > 0.03:  # å¤§æ¶¨
                signals.append(('SELL_ALERT', date, 'å¼‚å¸¸å¤§æ¶¨ï¼Œè­¦æƒ•å›è°ƒ'))
            elif price_change < -0.03:  # å¤§è·Œ
                signals.append(('BUY_OPPORTUNITY', date, 'å¼‚å¸¸å¤§è·Œï¼ŒæŠ„åº•æœºä¼š'))
        
        elif score >= 0.5:  # ä¸­ç­‰å¼‚å¸¸
            if abs(price_change) > 0.02:
                signals.append(('MONITOR', date, 'ä¸­ç­‰å¼‚å¸¸ï¼Œå¯†åˆ‡å…³æ³¨'))
    
    return signals
```

### é£é™©é¢„è­¦ç³»ç»Ÿ
```python
def risk_warning_system(anomaly_results, threshold=0.6):
    """
    åŸºäºå¼‚å¸¸æ£€æµ‹çš„é£é™©é¢„è­¦ç³»ç»Ÿ
    """
    warnings = []
    
    # è¿‘æœŸå¼‚å¸¸é¢‘ç‡åˆ†æ
    recent_anomalies = anomaly_results['anomaly_scores'].tail(20)
    anomaly_frequency = (recent_anomalies > 0.3).mean()
    
    if anomaly_frequency > 0.3:
        warnings.append({
            'type': 'é«˜é¢‘å¼‚å¸¸è­¦å‘Š',
            'message': f'è¿‘20æ—¥å¼‚å¸¸é¢‘ç‡è¾¾{anomaly_frequency:.1%}ï¼Œå¸‚åœºå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é£é™©',
            'severity': 'HIGH'
        })
    
    # è¿ç»­å¼‚å¸¸æ£€æµ‹
    consecutive_anomalies = 0
    max_consecutive = 0
    
    for score in recent_anomalies:
        if score > threshold:
            consecutive_anomalies += 1
            max_consecutive = max(max_consecutive, consecutive_anomalies)
        else:
            consecutive_anomalies = 0
    
    if max_consecutive >= 3:
        warnings.append({
            'type': 'è¿ç»­å¼‚å¸¸è­¦å‘Š',
            'message': f'æ£€æµ‹åˆ°è¿ç»­{max_consecutive}æ—¥å¼‚å¸¸ï¼Œå»ºè®®é™ä½ä»“ä½',
            'severity': 'MEDIUM'
        })
    
    return warnings
```

## æ³¨æ„äº‹é¡¹

### âš ï¸ æ–¹æ³•å±€é™æ€§
- **å‡é˜³æ€§**: æ­£å¸¸æ³¢åŠ¨å¯èƒ½è¢«è¯¯è¯†åˆ«ä¸ºå¼‚å¸¸
- **æ»åæ€§**: åŸºäºå†å²æ•°æ®ï¼Œæ— æ³•é¢„æµ‹æœªæ¥å¼‚å¸¸
- **å‚æ•°æ•æ„Ÿ**: é˜ˆå€¼è®¾ç½®å½±å“æ£€æµ‹æ•ˆæœ
- **å¸‚åœºç¯å¢ƒ**: ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„å¼‚å¸¸æ ‡å‡†ä¸åŒ

### ğŸ” æœ€ä½³å®è·µ
- **å¤šæ–¹æ³•ç»“åˆ**: ä½¿ç”¨å¤šç§æ£€æµ‹æ–¹æ³•äº¤å‰éªŒè¯
- **å‚æ•°ä¼˜åŒ–**: æ ¹æ®å†å²æ•°æ®ä¼˜åŒ–æ£€æµ‹å‚æ•°
- **äººå·¥éªŒè¯**: å¼‚å¸¸æ£€æµ‹ç»“æœéœ€è¦äººå·¥ç¡®è®¤
- **åŠ¨æ€è°ƒæ•´**: æ ¹æ®å¸‚åœºå˜åŒ–è°ƒæ•´æ£€æµ‹æ ‡å‡†

### ğŸ’¡ å®ç”¨å»ºè®®
- **åˆ†ç±»å¤„ç†**: åŒºåˆ†æŠ€æœ¯æ€§å¼‚å¸¸å’ŒåŸºæœ¬é¢å¼‚å¸¸
- **æ—¶æ•ˆæ€§**: å¼‚å¸¸æ£€æµ‹è¦æ±‚å¿«é€Ÿå“åº”
- **ç»„åˆåˆ†æ**: ç»“åˆå¤šä¸ªETFçš„å¼‚å¸¸æ¨¡å¼åˆ†æ
- **å†å²å›æº¯**: å®šæœŸå›æµ‹å¼‚å¸¸æ£€æµ‹çš„æœ‰æ•ˆæ€§ 