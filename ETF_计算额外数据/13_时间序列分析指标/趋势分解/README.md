# è¶‹åŠ¿åˆ†è§£æŒ‡æ ‡

## ğŸ“Š æŒ‡æ ‡æ¦‚è¿°

è¶‹åŠ¿åˆ†è§£æ˜¯å°†æ—¶é—´åºåˆ—åˆ†è§£ä¸ºè¶‹åŠ¿ã€å­£èŠ‚æ€§ã€å‘¨æœŸæ€§å’Œéšæœºæˆåˆ†çš„æŠ€æœ¯ï¼Œé€šè¿‡åˆ†ç¦»ä¸åŒçš„æˆåˆ†æ¥æ›´å¥½åœ°ç†è§£ETFä»·æ ¼çš„å†…åœ¨ç»“æ„ã€‚è¶‹åŠ¿åˆ†è§£æœ‰åŠ©äºè¯†åˆ«é•¿æœŸæ–¹å‘ã€æ¶ˆé™¤å™ªéŸ³å¹²æ‰°ï¼Œä¸ºæŠ•èµ„å†³ç­–æä¾›æ›´æ¸…æ™°çš„ä¿¡å·ã€‚

## ğŸ”§ æ ¸å¿ƒåˆ†è§£æ–¹æ³•

### 1. çº¿æ€§è¶‹åŠ¿åˆ†è§£
**æœ€ç®€å•çš„è¶‹åŠ¿åˆ†ææ–¹æ³•**
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

def linear_trend_decomposition(price_series):
    """
    çº¿æ€§è¶‹åŠ¿åˆ†è§£
    """
    # å‡†å¤‡æ•°æ®
    x = np.arange(len(price_series)).reshape(-1, 1)
    y = price_series.values
    
    # çº¿æ€§å›å½’æ‹Ÿåˆ
    model = LinearRegression()
    model.fit(x, y)
    
    # è®¡ç®—è¶‹åŠ¿æˆåˆ†
    trend = model.predict(x)
    
    # è®¡ç®—æ®‹å·®ï¼ˆå»è¶‹åŠ¿åçš„åºåˆ—ï¼‰
    detrended = y - trend
    
    # è¶‹åŠ¿å¼ºåº¦
    r_squared = model.score(x, y)
    
    return {
        'trend': pd.Series(trend, index=price_series.index),
        'detrended': pd.Series(detrended, index=price_series.index),
        'slope': model.coef_[0],
        'trend_strength': r_squared
    }
```

### 2. HPæ»¤æ³¢åˆ†è§£ (Hodrick-Prescott Filter)
**ç»å…¸çš„è¶‹åŠ¿-å‘¨æœŸåˆ†è§£æ–¹æ³•**
```python
from statsmodels.tsa.filters.hp_filter import hpfilter

def hp_filter_decomposition(price_series, lamb=1600):
    """
    HPæ»¤æ³¢è¶‹åŠ¿åˆ†è§£
    
    å‚æ•°:
    lamb: å¹³æ»‘å‚æ•°ï¼Œè¶Šå¤§è¶‹åŠ¿è¶Šå¹³æ»‘
    - æ—¥æ•°æ®: 1600
    - æœˆæ•°æ®: 14400
    - å­£åº¦æ•°æ®: 1600
    """
    # HPæ»¤æ³¢
    cycle, trend = hpfilter(price_series, lamb=lamb)
    
    # è®¡ç®—è¶‹åŠ¿å˜åŒ–ç‡
    trend_change = trend.pct_change()
    
    # å‘¨æœŸæ€§æˆåˆ†çš„æ³¢åŠ¨æ€§
    cycle_volatility = cycle.std()
    
    return {
        'trend': trend,
        'cycle': cycle,
        'trend_change': trend_change,
        'cycle_volatility': cycle_volatility
    }
```

### 3. ç§»åŠ¨å¹³å‡è¶‹åŠ¿åˆ†è§£
**çµæ´»çš„è¶‹åŠ¿åˆ†ç¦»æ–¹æ³•**
```python
def moving_average_decomposition(price_series, windows=[5, 20, 60]):
    """
    å¤šé‡ç§»åŠ¨å¹³å‡è¶‹åŠ¿åˆ†è§£
    """
    components = {}
    
    for window in windows:
        ma = price_series.rolling(window=window).mean()
        components[f'MA_{window}'] = ma
    
    # é•¿æœŸè¶‹åŠ¿ (60æ—¥å‡çº¿)
    long_trend = components['MA_60']
    
    # ä¸­æœŸè¶‹åŠ¿ (20æ—¥å‡çº¿ç›¸å¯¹äº60æ—¥å‡çº¿)
    medium_trend = components['MA_20'] - components['MA_60']
    
    # çŸ­æœŸæ³¢åŠ¨ (ä»·æ ¼ç›¸å¯¹äº20æ—¥å‡çº¿)
    short_fluctuation = price_series - components['MA_20']
    
    # å™ªéŸ³ (ä»·æ ¼ç›¸å¯¹äº5æ—¥å‡çº¿)
    noise = price_series - components['MA_5']
    
    return {
        'long_trend': long_trend,
        'medium_trend': medium_trend,
        'short_fluctuation': short_fluctuation,
        'noise': noise,
        'all_ma': components
    }
```

### 4. STLåˆ†è§£ (Seasonal and Trend decomposition using Loess)
**å¤„ç†å­£èŠ‚æ€§çš„é«˜çº§æ–¹æ³•**
```python
from statsmodels.tsa.seasonal import STL

def stl_decomposition(price_series, seasonal=7, robust=True):
    """
    STLå­£èŠ‚æ€§è¶‹åŠ¿åˆ†è§£
    
    å‚æ•°:
    seasonal: å­£èŠ‚æ€§å‘¨æœŸé•¿åº¦
    robust: æ˜¯å¦ä½¿ç”¨é²æ£’ä¼°è®¡
    """
    # STLåˆ†è§£
    stl = STL(price_series, seasonal=seasonal, robust=robust)
    result = stl.fit()
    
    # æå–æˆåˆ†
    trend = result.trend
    seasonal = result.seasonal
    residual = result.resid
    
    # è®¡ç®—å„æˆåˆ†çš„ç›¸å¯¹é‡è¦æ€§
    total_var = price_series.var()
    trend_importance = trend.var() / total_var
    seasonal_importance = seasonal.var() / total_var
    residual_importance = residual.var() / total_var
    
    return {
        'trend': trend,
        'seasonal': seasonal,
        'residual': residual,
        'trend_importance': trend_importance,
        'seasonal_importance': seasonal_importance,
        'residual_importance': residual_importance
    }
```

## ğŸ“ˆ è¶‹åŠ¿å˜åŒ–ç‚¹æ£€æµ‹

### 1. ç»“æ„æ–­ç‚¹æ£€æµ‹
```python
def structural_break_detection(price_series, min_size=30):
    """
    ç»“æ„æ€§æ–­ç‚¹æ£€æµ‹
    """
    breaks = []
    n = len(price_series)
    
    for i in range(min_size, n - min_size):
        # åˆ†å‰²åºåˆ—
        series1 = price_series.iloc[:i]
        series2 = price_series.iloc[i:]
        
        # è®¡ç®—ä¸¤æ®µçš„å‡å€¼å·®å¼‚
        mean1 = series1.mean()
        mean2 = series2.mean()
        
        # è®¡ç®—tç»Ÿè®¡é‡
        var1 = series1.var()
        var2 = series2.var()
        pooled_var = ((len(series1)-1)*var1 + (len(series2)-1)*var2) / (n-2)
        
        t_stat = abs(mean1 - mean2) / np.sqrt(pooled_var * (1/len(series1) + 1/len(series2)))
        
        # å¦‚æœtç»Ÿè®¡é‡è¶³å¤Ÿå¤§ï¼Œè®¤ä¸ºå­˜åœ¨ç»“æ„æ–­ç‚¹
        if t_stat > 2.5:  # å¯è°ƒæ•´é˜ˆå€¼
            breaks.append({
                'index': i,
                'date': price_series.index[i],
                't_statistic': t_stat,
                'mean_before': mean1,
                'mean_after': mean2
            })
    
    return breaks
```

### 2. è¶‹åŠ¿å˜åŒ–å¼ºåº¦åˆ†æ
```python
def trend_change_analysis(price_series, window=20):
    """
    è¶‹åŠ¿å˜åŒ–å¼ºåº¦åˆ†æ
    """
    # è®¡ç®—æ»šåŠ¨æ–œç‡
    rolling_slopes = []
    
    for i in range(window, len(price_series)):
        subset = price_series.iloc[i-window:i]
        x = np.arange(len(subset))
        slope = np.polyfit(x, subset.values, 1)[0]
        rolling_slopes.append(slope)
    
    rolling_slopes = pd.Series(rolling_slopes, 
                              index=price_series.index[window:])
    
    # è¶‹åŠ¿å˜åŒ–ç‚¹
    slope_changes = rolling_slopes.diff().abs()
    
    # è¶‹åŠ¿åŠ é€Ÿ/å‡é€Ÿ
    trend_acceleration = rolling_slopes.diff()
    
    return {
        'rolling_slopes': rolling_slopes,
        'slope_changes': slope_changes,
        'trend_acceleration': trend_acceleration,
        'avg_slope': rolling_slopes.mean(),
        'slope_volatility': rolling_slopes.std()
    }
```

## ğŸ¯ äº¤æ˜“ç­–ç•¥åº”ç”¨

### 1. è¶‹åŠ¿è·Ÿè¸ªç­–ç•¥
```python
def trend_following_strategy(decomposition_result):
    """
    åŸºäºè¶‹åŠ¿åˆ†è§£çš„è·Ÿè¸ªç­–ç•¥
    """
    trend = decomposition_result['trend']
    
    # è¶‹åŠ¿æ–¹å‘
    trend_direction = np.sign(trend.diff())
    
    # è¶‹åŠ¿å¼ºåº¦
    trend_strength = abs(trend.diff()) / trend.abs()
    
    # ä¹°å…¥ä¿¡å·ï¼šè¶‹åŠ¿å‘ä¸Šä¸”å¼ºåº¦è¾ƒå¼º
    buy_signal = (trend_direction > 0) & (trend_strength > trend_strength.quantile(0.7))
    
    # å–å‡ºä¿¡å·ï¼šè¶‹åŠ¿å‘ä¸‹ä¸”å¼ºåº¦è¾ƒå¼º
    sell_signal = (trend_direction < 0) & (trend_strength > trend_strength.quantile(0.7))
    
    # æŒæœ‰ä¿¡å·ï¼šè¶‹åŠ¿ä¸æ˜ç¡®
    hold_signal = trend_strength <= trend_strength.quantile(0.3)
    
    return {
        'buy_signals': buy_signal,
        'sell_signals': sell_signal,
        'hold_signals': hold_signal,
        'trend_strength': trend_strength
    }
```

### 2. å‡å€¼å›å½’ç­–ç•¥
```python
def mean_reversion_strategy(decomposition_result, threshold=2):
    """
    åŸºäºå‘¨æœŸæˆåˆ†çš„å‡å€¼å›å½’ç­–ç•¥
    """
    if 'cycle' in decomposition_result:
        cycle = decomposition_result['cycle']
    else:
        cycle = decomposition_result['short_fluctuation']
    
    # æ ‡å‡†åŒ–å‘¨æœŸæˆåˆ†
    cycle_std = cycle.std()
    cycle_normalized = cycle / cycle_std
    
    # æç«¯åç¦»ä¹°å…¥ä¿¡å·
    buy_signal = cycle_normalized < -threshold
    
    # æç«¯åç¦»å–å‡ºä¿¡å·
    sell_signal = cycle_normalized > threshold
    
    # å›å½’ä¸­æ€§ä¿¡å·
    neutral_signal = abs(cycle_normalized) < 0.5
    
    return {
        'buy_signals': buy_signal,
        'sell_signals': sell_signal,
        'neutral_signals': neutral_signal,
        'cycle_normalized': cycle_normalized
    }
```

## ğŸ“Š å®é™…åº”ç”¨æ¡ˆä¾‹

### ETFè¶‹åŠ¿åˆ†è§£åˆ†æ
```python
def comprehensive_trend_analysis(df, etf_name):
    """
    ETFç»¼åˆè¶‹åŠ¿åˆ†æ
    """
    close = df['close']
    
    # å¤šç§åˆ†è§£æ–¹æ³•
    linear_result = linear_trend_decomposition(close)
    hp_result = hp_filter_decomposition(close)
    ma_result = moving_average_decomposition(close)
    
    # ç»¼åˆåˆ†æ
    analysis = {
        'ETFåç§°': etf_name,
        'çº¿æ€§è¶‹åŠ¿æ–œç‡': round(linear_result['slope'], 4),
        'è¶‹åŠ¿å¼ºåº¦(RÂ²)': round(linear_result['trend_strength'], 3),
        'HPè¶‹åŠ¿æ–¹å‘': 'ä¸Šå‡' if hp_result['trend'].iloc[-1] > hp_result['trend'].iloc[-20] else 'ä¸‹é™',
        'å‘¨æœŸæ³¢åŠ¨ç‡': round(hp_result['cycle_volatility'], 3),
        'å½“å‰è¶‹åŠ¿ä½ç½®': 'ä¸Šè½¨' if ma_result['short_fluctuation'].iloc[-1] > 0 else 'ä¸‹è½¨'
    }
    
    return analysis
```

### è¶‹åŠ¿è´¨é‡è¯„ä¼°
```python
def trend_quality_assessment(decomposition_results):
    """
    è¶‹åŠ¿è´¨é‡è¯„ä¼°
    """
    trend = decomposition_results['trend']
    
    # è¶‹åŠ¿ä¸€è‡´æ€§
    trend_direction_changes = (np.sign(trend.diff()) != np.sign(trend.diff().shift(1))).sum()
    consistency_score = 1 - (trend_direction_changes / len(trend))
    
    # è¶‹åŠ¿å¹³æ»‘åº¦
    trend_volatility = trend.diff().std()
    smoothness_score = 1 / (1 + trend_volatility)
    
    # è¶‹åŠ¿æ˜¾è‘—æ€§
    if 'trend_strength' in decomposition_results:
        significance_score = decomposition_results['trend_strength']
    else:
        significance_score = abs(trend.corr(pd.Series(range(len(trend)))))
    
    # ç»¼åˆè´¨é‡è¯„åˆ†
    quality_score = (consistency_score + smoothness_score + significance_score) / 3
    
    return {
        'consistency_score': consistency_score,
        'smoothness_score': smoothness_score,
        'significance_score': significance_score,
        'overall_quality': quality_score,
        'quality_grade': 'A' if quality_score > 0.8 else 'B' if quality_score > 0.6 else 'C'
    }
```

## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹

### æ–¹æ³•é€‰æ‹©æŒ‡å—
- **çº¿æ€§è¶‹åŠ¿**: é€‚åˆçŸ­æœŸåˆ†æï¼Œè®¡ç®—ç®€å•
- **HPæ»¤æ³¢**: é€‚åˆä¸­é•¿æœŸåˆ†æï¼Œå¤„ç†å‘¨æœŸæ€§è¾ƒå¥½
- **ç§»åŠ¨å¹³å‡**: é€‚åˆå®æ—¶åˆ†æï¼Œæ»åæ€§è¾ƒå°
- **STLåˆ†è§£**: é€‚åˆæœ‰æ˜æ˜¾å­£èŠ‚æ€§çš„æ•°æ®

### å‚æ•°ä¼˜åŒ–
```python
def parameter_optimization(price_series):
    """
    åˆ†è§£å‚æ•°ä¼˜åŒ–å»ºè®®
    """
    data_length = len(price_series)
    
    # HPæ»¤æ³¢å‚æ•°å»ºè®®
    if data_length < 100:
        hp_lambda = 400  # çŸ­æœŸæ•°æ®ç”¨è¾ƒå°å‚æ•°
    elif data_length < 500:
        hp_lambda = 1600  # æ ‡å‡†å‚æ•°
    else:
        hp_lambda = 6400  # é•¿æœŸæ•°æ®ç”¨è¾ƒå¤§å‚æ•°
    
    # ç§»åŠ¨å¹³å‡çª—å£å»ºè®®
    short_window = max(5, data_length // 50)
    medium_window = max(20, data_length // 20)
    long_window = max(60, data_length // 10)
    
    return {
        'hp_lambda': hp_lambda,
        'ma_windows': [short_window, medium_window, long_window],
        'stl_seasonal': 7 if data_length > 50 else max(3, data_length // 10)
    }
```

### ç»“æœéªŒè¯
```python
def decomposition_validation(original_series, decomposed_components):
    """
    åˆ†è§£ç»“æœéªŒè¯
    """
    # é‡æ„åŸåºåˆ—
    reconstructed = sum(decomposed_components.values())
    
    # è®¡ç®—é‡æ„è¯¯å·®
    reconstruction_error = np.mean((original_series - reconstructed) ** 2)
    
    # å„æˆåˆ†æ–¹å·®å æ¯”
    total_variance = original_series.var()
    component_contributions = {
        name: component.var() / total_variance 
        for name, component in decomposed_components.items()
    }
    
    return {
        'reconstruction_error': reconstruction_error,
        'component_contributions': component_contributions,
        'total_explained_variance': sum(component_contributions.values())
    }
```

è¶‹åŠ¿åˆ†è§£æ˜¯ç†è§£ETFä»·æ ¼ç»“æ„çš„é‡è¦å·¥å…·ï¼Œé€šè¿‡åˆ†ç¦»ä¸åŒæˆåˆ†å¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«æŠ•èµ„æœºä¼šå’Œé£é™©ï¼ 