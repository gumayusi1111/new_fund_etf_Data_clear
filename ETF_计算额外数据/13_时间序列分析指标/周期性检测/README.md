# å‘¨æœŸæ€§æ£€æµ‹æŒ‡æ ‡

## ğŸ“Š æŒ‡æ ‡æ¦‚è¿°

å‘¨æœŸæ€§æ£€æµ‹æ˜¯æ—¶é—´åºåˆ—åˆ†æçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡è¯†åˆ«ETFä»·æ ¼æ•°æ®ä¸­çš„å‘¨æœŸæ€§æ¨¡å¼ï¼Œå¸®åŠ©æŠ•èµ„è€…å‘ç°è§„å¾‹æ€§çš„ä»·æ ¼æ³¢åŠ¨å¹¶é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚å‘¨æœŸæ€§åˆ†æèƒ½å¤Ÿæ­ç¤ºå¸‚åœºçš„å†…åœ¨èŠ‚å¾‹ï¼Œä¸ºæ‹©æ—¶æŠ•èµ„æä¾›ç§‘å­¦ä¾æ®ã€‚

## ğŸ” æ ¸å¿ƒæ£€æµ‹æ–¹æ³•

### 1. å‚…é‡Œå¶åˆ†æ (Fourier Analysis)
**åŸç†**: å°†ä»·æ ¼æ—¶é—´åºåˆ—åˆ†è§£ä¸ºä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢ç»„åˆ
```python
import numpy as np
from scipy.fft import fft, fftfreq

def fourier_analysis(price_series, sampling_rate=1):
    """
    å‚…é‡Œå¶é¢‘è°±åˆ†æ
    """
    # å»é™¤è¶‹åŠ¿
    detrended = price_series - price_series.rolling(50).mean()
    detrended = detrended.dropna()
    
    # FFTå˜æ¢
    fft_values = fft(detrended.values)
    frequencies = fftfreq(len(detrended), d=sampling_rate)
    
    # åŠŸç‡è°±
    power_spectrum = np.abs(fft_values) ** 2
    
    # æ‰¾åˆ°ä¸»è¦å‘¨æœŸ
    positive_freq_idx = frequencies > 0
    dominant_freq = frequencies[positive_freq_idx][np.argmax(power_spectrum[positive_freq_idx])]
    dominant_period = 1 / dominant_freq if dominant_freq > 0 else np.inf
    
    return {
        'dominant_period': dominant_period,
        'frequencies': frequencies[positive_freq_idx],
        'power_spectrum': power_spectrum[positive_freq_idx]
    }
```

### 2. è‡ªç›¸å…³åˆ†æ (Autocorrelation Analysis)
**ç”¨é€”**: å‘ç°æ—¶é—´åºåˆ—ä¸­çš„é‡å¤æ¨¡å¼
```python
def autocorrelation_analysis(price_series, max_lags=250):
    """
    è‡ªç›¸å…³å‘¨æœŸæ£€æµ‹
    """
    returns = price_series.pct_change().dropna()
    
    # è®¡ç®—è‡ªç›¸å…³ç³»æ•°
    autocorr = [returns.autocorr(lag=i) for i in range(1, max_lags+1)]
    autocorr_series = pd.Series(autocorr, index=range(1, max_lags+1))
    
    # å¯»æ‰¾æ˜¾è‘—çš„æ­£è‡ªç›¸å…³å³°å€¼
    threshold = 2 / np.sqrt(len(returns))  # 95%ç½®ä¿¡åŒºé—´
    significant_lags = autocorr_series[autocorr_series > threshold]
    
    return {
        'autocorr_values': autocorr_series,
        'significant_lags': significant_lags,
        'potential_cycles': significant_lags.index.tolist()
    }
```

### 3. å°æ³¢åˆ†æ (Wavelet Analysis)
**ä¼˜åŠ¿**: èƒ½åŒæ—¶åˆ†ææ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾
```python
import pywt

def wavelet_analysis(price_series, wavelet='morlet', scales=None):
    """
    å°æ³¢åˆ†ææ£€æµ‹å‘¨æœŸ
    """
    if scales is None:
        scales = np.arange(1, 128)
    
    # å°æ³¢å˜æ¢
    coefficients, frequencies = pywt.cwt(price_series.values, scales, wavelet)
    
    # è®¡ç®—åŠŸç‡
    power = np.abs(coefficients) ** 2
    
    # æ—¶é—´å¹³å‡åŠŸç‡è°±
    global_power = np.mean(power, axis=1)
    
    # æ‰¾åˆ°ä¸»å¯¼å‘¨æœŸ
    dominant_scale_idx = np.argmax(global_power)
    dominant_period = scales[dominant_scale_idx]
    
    return {
        'coefficients': coefficients,
        'scales': scales,
        'global_power': global_power,
        'dominant_period': dominant_period
    }
```

## ğŸ“ˆ å‘¨æœŸè¯†åˆ«ç­–ç•¥

### 1. å¤šå‘¨æœŸç»„åˆåˆ†æ
```python
def multi_cycle_analysis(df):
    """
    å¤šå‘¨æœŸç»¼åˆåˆ†æ
    """
    close = df['close']
    
    # çŸ­æœŸå‘¨æœŸ (5-20å¤©)
    short_cycles = []
    for period in range(5, 21):
        ma_short = close.rolling(period).mean()
        correlation = close.corr(ma_short.shift(period))
        if correlation > 0.7:
            short_cycles.append(period)
    
    # ä¸­æœŸå‘¨æœŸ (20-60å¤©)
    medium_cycles = []
    for period in range(20, 61):
        ma_medium = close.rolling(period).mean()
        correlation = close.corr(ma_medium.shift(period))
        if correlation > 0.6:
            medium_cycles.append(period)
    
    # é•¿æœŸå‘¨æœŸ (60-250å¤©)
    long_cycles = []
    for period in range(60, 251):
        ma_long = close.rolling(period).mean()
        correlation = close.corr(ma_long.shift(period))
        if correlation > 0.5:
            long_cycles.append(period)
    
    return {
        'short_cycles': short_cycles,
        'medium_cycles': medium_cycles,
        'long_cycles': long_cycles
    }
```

### 2. å‘¨æœŸå¼ºåº¦è¯„ä¼°
```python
def cycle_strength_assessment(price_series, detected_cycles):
    """
    è¯„ä¼°æ£€æµ‹åˆ°çš„å‘¨æœŸçš„å¼ºåº¦
    """
    cycle_strengths = {}
    
    for cycle in detected_cycles:
        # è®¡ç®—å‘¨æœŸæ€§ç§»åŠ¨å¹³å‡
        cyclic_ma = price_series.rolling(cycle).mean()
        
        # è®¡ç®—å‘¨æœŸåç¦»åº¦
        deviations = abs(price_series - cyclic_ma) / cyclic_ma
        avg_deviation = deviations.mean()
        
        # è®¡ç®—å‘¨æœŸä¸€è‡´æ€§
        cyclic_returns = price_series.pct_change(cycle)
        consistency = len(cyclic_returns[cyclic_returns > 0]) / len(cyclic_returns.dropna())
        
        # ç»¼åˆå¼ºåº¦è¯„åˆ†
        strength_score = (1 - avg_deviation) * consistency
        
        cycle_strengths[cycle] = {
            'strength_score': strength_score,
            'average_deviation': avg_deviation,
            'consistency': consistency
        }
    
    return cycle_strengths
```

## ğŸ¯ äº¤æ˜“åº”ç”¨

### 1. å‘¨æœŸæ€§æ‹©æ—¶ç­–ç•¥
```python
def cyclical_timing_strategy(df, dominant_cycle):
    """
    åŸºäºä¸»å¯¼å‘¨æœŸçš„æ‹©æ—¶ç­–ç•¥
    """
    close = df['close']
    
    # è®¡ç®—å‘¨æœŸæ€§ä¿¡å·
    cycle_ma = close.rolling(dominant_cycle).mean()
    cycle_position = (close - cycle_ma) / cycle_ma
    
    # å‘¨æœŸåº•éƒ¨ä¹°å…¥ä¿¡å·
    cycle_bottom = (cycle_position < -0.05) & (cycle_position.shift(1) >= -0.05)
    
    # å‘¨æœŸé¡¶éƒ¨å–å‡ºä¿¡å·  
    cycle_top = (cycle_position > 0.05) & (cycle_position.shift(1) <= 0.05)
    
    # å‘¨æœŸä¸­æ€§åŒºåŸŸ
    cycle_neutral = abs(cycle_position) <= 0.02
    
    return {
        'buy_signals': cycle_bottom,
        'sell_signals': cycle_top,
        'neutral_zones': cycle_neutral,
        'cycle_position': cycle_position
    }
```

### 2. å¤šå‘¨æœŸå åŠ ç­–ç•¥
```python
def multi_cycle_overlay_strategy(df, cycles_dict):
    """
    å¤šå‘¨æœŸå åŠ äº¤æ˜“ç­–ç•¥
    """
    close = df['close']
    signals = pd.DataFrame(index=df.index)
    
    # ä¸ºæ¯ä¸ªå‘¨æœŸè®¡ç®—ä¿¡å·
    for cycle_type, cycles in cycles_dict.items():
        if cycles:
            avg_cycle = int(np.mean(cycles))
            cycle_ma = close.rolling(avg_cycle).mean()
            
            # å‘¨æœŸè¶‹åŠ¿ä¿¡å·
            signals[f'{cycle_type}_trend'] = close > cycle_ma
            
            # å‘¨æœŸåŠ¨é‡ä¿¡å·
            signals[f'{cycle_type}_momentum'] = cycle_ma > cycle_ma.shift(5)
    
    # ç»¼åˆä¿¡å·
    long_signal = signals.filter(like='_trend').all(axis=1) & \
                 signals.filter(like='_momentum').all(axis=1)
    
    short_signal = (~signals.filter(like='_trend')).all(axis=1) & \
                  (~signals.filter(like='_momentum')).all(axis=1)
    
    return long_signal, short_signal
```

## ğŸ“Š å®é™…åº”ç”¨æ¡ˆä¾‹

### ETFå‘¨æœŸæ€§ç‰¹å¾åˆ†æ
```python
def etf_cyclical_characteristics(df, etf_name):
    """
    ETFå‘¨æœŸæ€§ç‰¹å¾åˆ†æ
    """
    close = df['close']
    
    # å‚…é‡Œå¶åˆ†æ
    fourier_result = fourier_analysis(close)
    
    # è‡ªç›¸å…³åˆ†æ
    autocorr_result = autocorrelation_analysis(close)
    
    # å¤šå‘¨æœŸåˆ†æ
    cycles = multi_cycle_analysis(df)
    
    analysis = {
        'ETFåç§°': etf_name,
        'ä¸»å¯¼å‘¨æœŸ(å‚…é‡Œå¶)': round(fourier_result['dominant_period'], 1),
        'æ˜¾è‘—è‡ªç›¸å…³å‘¨æœŸ': autocorr_result['potential_cycles'][:3],
        'çŸ­æœŸå‘¨æœŸ': cycles['short_cycles'],
        'ä¸­æœŸå‘¨æœŸ': cycles['medium_cycles'],
        'é•¿æœŸå‘¨æœŸ': cycles['long_cycles']
    }
    
    return analysis
```

## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹

### æ•°æ®è¦æ±‚
- **æ ·æœ¬é‡**: è‡³å°‘éœ€è¦å¾…æ£€æµ‹å‘¨æœŸé•¿åº¦3å€ä»¥ä¸Šçš„æ•°æ®
- **æ•°æ®è´¨é‡**: ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ä¼šä¸¥é‡å½±å“å‘¨æœŸæ£€æµ‹
- **å»è¶‹åŠ¿**: é•¿æœŸè¶‹åŠ¿ä¼šæ©ç›–å‘¨æœŸæ€§ç‰¹å¾

### æ£€æµ‹å±€é™æ€§
- **ä¼ªå‘¨æœŸ**: éšæœºå™ªéŸ³å¯èƒ½äº§ç”Ÿè™šå‡çš„å‘¨æœŸä¿¡å·
- **éå¹³ç¨³æ€§**: å¸‚åœºåˆ¶åº¦å˜åŒ–ä¼šæ”¹å˜å‘¨æœŸç‰¹å¾
- **å¤šé‡å‘¨æœŸ**: å®é™…å¸‚åœºå¾€å¾€å­˜åœ¨å¤šä¸ªé‡å çš„å‘¨æœŸ

### ä¼˜åŒ–å»ºè®®
```python
def cycle_detection_optimization(df):
    """
    å‘¨æœŸæ£€æµ‹ä¼˜åŒ–å»ºè®®
    """
    close = df['close']
    
    # 1. æ•°æ®é¢„å¤„ç†
    # å»é™¤å¼‚å¸¸å€¼
    q1, q3 = close.quantile([0.25, 0.75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    clean_data = close[(close >= lower_bound) & (close <= upper_bound)]
    
    # 2. å¤šæ–¹æ³•éªŒè¯
    methods_results = []
    
    # å‚…é‡Œå¶æ–¹æ³•
    fourier_period = fourier_analysis(clean_data)['dominant_period']
    methods_results.append(fourier_period)
    
    # è‡ªç›¸å…³æ–¹æ³•
    autocorr_cycles = autocorrelation_analysis(clean_data)['potential_cycles']
    if autocorr_cycles:
        methods_results.extend(autocorr_cycles[:2])
    
    # 3. ä¸€è‡´æ€§æ£€éªŒ
    consistent_cycles = []
    for cycle in set(methods_results):
        if methods_results.count(cycle) >= 2 or abs(cycle - np.mean(methods_results)) < 5:
            consistent_cycles.append(cycle)
    
    return consistent_cycles
```

å‘¨æœŸæ€§æ£€æµ‹æ˜¯å‘ç°å¸‚åœºå†…åœ¨è§„å¾‹çš„é‡è¦å·¥å…·ï¼Œç»“åˆå¤šç§åˆ†ææ–¹æ³•èƒ½å¤Ÿæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ 