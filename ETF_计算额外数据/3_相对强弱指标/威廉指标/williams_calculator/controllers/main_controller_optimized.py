"""
å¨å»‰æŒ‡æ ‡ä¼˜åŒ–ä¸»æ§åˆ¶å™¨
é›†æˆæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–å’Œbugä¿®å¤

ä¼˜åŒ–å†…å®¹ï¼š
1. é›†æˆä¼˜åŒ–ç‰ˆæœ¬çš„è®¡ç®—å¼•æ“å’Œæ•°æ®è¯»å–å™¨
2. å¢é‡æ›´æ–°æ”¯æŒ
3. å‘é‡åŒ–è®¡ç®—æå‡
4. ä¿®å¤è®¡ç®—é€»è¾‘bug
5. ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œé”™è¯¯å¤„ç†
"""

import os
import sys
from datetime import datetime
import traceback

# å¸¸é‡å®šä¹‰
PROJECT_ROOT_LEVELS = 6  # å‘ä¸ŠæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•çš„å±‚çº§æ•°
PROGRESS_DISPLAY_INTERVAL = 10  # æ‰¹é‡å¤„ç†æ—¶æ˜¾ç¤ºè¿›åº¦çš„é—´éš”
MAX_FAILED_ETF_DISPLAY = 10  # æ˜¾ç¤ºå¤±è´¥ETFçš„æœ€å¤§æ•°é‡

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file_path = os.path.abspath(__file__)
project_root = current_file_path
for _ in range(PROJECT_ROOT_LEVELS):  # å‘ä¸ŠæŸ¥æ‰¾åˆ°è¾¾data_clearç›®å½•
    project_root = os.path.dirname(project_root)
    if os.path.basename(project_root) == "data_clear":
        break

if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡å—
try:
    # æ·»åŠ å½“å‰åŒ…è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)

    from infrastructure.config import WilliamsConfig
    from infrastructure.data_reader_optimized import WilliamsDataReaderOptimized
    from infrastructure.cache_manager import WilliamsCacheManager
    from engines.williams_engine_optimized import WilliamsEngineOptimized
    from outputs.csv_handler import WilliamsCSVHandler

except ImportError as e:
    print(f"âŒ å¯¼å…¥å¨å»‰æŒ‡æ ‡ä¼˜åŒ–æ¨¡å—å¤±è´¥: {str(e)}")
    raise


class WilliamsMainControllerOptimized:
    """å¨å»‰æŒ‡æ ‡ä¼˜åŒ–ä¸»ä¸šåŠ¡æ§åˆ¶å™¨"""

    def __init__(self, adj_type="å‰å¤æƒ", use_optimized_components=True):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ä¸»æ§åˆ¶å™¨

        Args:
            adj_type: å¤æƒç±»å‹ï¼Œé»˜è®¤ä¸º"å‰å¤æƒ"
            use_optimized_components: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç»„ä»¶
        """
        try:
            # åˆå§‹åŒ–é…ç½®
            self.config = WilliamsConfig(adj_type=adj_type)
            self.use_optimized_components = use_optimized_components

            # éªŒè¯é…ç½®
            validation = self.config.validate_config()
            if not validation["is_valid"]:
                for error in validation["errors"]:
                    print(f"âŒ é…ç½®é”™è¯¯: {error}")
                raise ValueError("é…ç½®éªŒè¯å¤±è´¥")

            # åˆå§‹åŒ–å„æ¨¡å—ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            self._initialize_optimized_modules()

            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            self._initialize_statistics()

            print("âœ… å¨å»‰æŒ‡æ ‡ä¼˜åŒ–ä¸»æ§åˆ¶å™¨åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            print(f"âŒ å¨å»‰æŒ‡æ ‡ä¼˜åŒ–ä¸»æ§åˆ¶å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def _initialize_optimized_modules(self):
        """åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆæœ¬çš„åŠŸèƒ½æ¨¡å—"""
        try:
            if self.use_optimized_components:
                # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„ç»„ä»¶
                self.data_reader = WilliamsDataReaderOptimized(self.config)
                self.williams_engine = WilliamsEngineOptimized(self.config)
                print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ç»„ä»¶")
            else:
                # å›é€€åˆ°åŸç‰ˆæœ¬ç»„ä»¶
                from infrastructure.data_reader import WilliamsDataReader
                from engines.williams_engine import WilliamsEngine

                self.data_reader = WilliamsDataReader(self.config)
                self.williams_engine = WilliamsEngine(self.config)
                print("ğŸ“Š ä½¿ç”¨åŸç‰ˆæœ¬ç»„ä»¶")

            # ç¼“å­˜å’Œè¾“å‡ºç»„ä»¶ä¿æŒä¸å˜
            self.cache_manager = WilliamsCacheManager(self.config)
            self.csv_handler = WilliamsCSVHandler(self.config)

            print("ğŸ”§ å¨å»‰æŒ‡æ ‡ä¼˜åŒ–åŠŸèƒ½æ¨¡å—åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"âŒ ä¼˜åŒ–åŠŸèƒ½æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def _initialize_statistics(self):
        """åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        self.statistics = {
            "processed_etfs": 0,
            "successful_calculations": 0,
            "failed_calculations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "incremental_updates": 0,
            "full_calculations": 0,
            "start_time": None,
            "end_time": None,
            "failed_etf_list": [],
            "optimization_metrics": {
                "total_calculation_time_ms": 0,
                "average_calculation_time_ms": 0,
                "vectorization_benefits": 0,
            },
        }

    def calculate_single_etf_optimized(
        self, etf_code, threshold, save_result=True, use_incremental=True
    ):
        """
        ä¼˜åŒ–çš„å•ä¸ªETFå¨å»‰æŒ‡æ ‡è®¡ç®—

        ä¼˜åŒ–å†…å®¹ï¼š
        1. æ”¯æŒå¢é‡æ›´æ–°
        2. æ€§èƒ½ç›‘æ§
        3. æ™ºèƒ½ç¼“å­˜ç­–ç•¥

        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼('3000ä¸‡é—¨æ§›' or '5000ä¸‡é—¨æ§›')
            save_result: æ˜¯å¦ä¿å­˜ç»“æœï¼Œé»˜è®¤True
            use_incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ›´æ–°ï¼Œé»˜è®¤True

        Returns:
            dict: è®¡ç®—ç»“æœ
        """
        try:
            calculation_start_time = datetime.now()
            print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–è®¡ç®—å¨å»‰æŒ‡æ ‡: {etf_code} ({threshold})")

            self.statistics["processed_etfs"] += 1

            # 1. è¯»å–ETFæ•°æ®
            etf_data = self.data_reader.read_etf_data(etf_code)
            if etf_data is None or etf_data.empty:
                error_msg = f"æ— æ³•è¯»å–ETFæ•°æ®: {etf_code}"
                print(f"âš ï¸ {error_msg}")
                self.statistics["failed_calculations"] += 1
                self.statistics["failed_etf_list"].append(etf_code)
                return {"success": False, "error": error_msg, "etf_code": etf_code}

            # 2. æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
            source_file_path = self.data_reader._find_etf_data_file(
                self.data_reader._clean_etf_code(etf_code)
            )

            cached_data = None

            if source_file_path and self.cache_manager.is_cache_valid_optimized(
                etf_code, threshold, source_file_path
            ):
                # ç¼“å­˜æœ‰æ•ˆï¼Œè€ƒè™‘å¢é‡æ›´æ–°
                cached_data = self.cache_manager.load_etf_cache(etf_code, threshold)
                if not cached_data.empty:
                    if use_incremental:
                        # å°è¯•å¢é‡æ›´æ–°
                        incremental_result = self._try_incremental_update(
                            etf_code, threshold, cached_data, etf_data
                        )
                        if incremental_result is not None:
                            print(f"âš¡ ä½¿ç”¨å¢é‡æ›´æ–°: {etf_code}")
                            self.statistics["cache_hits"] += 1
                            self.statistics["incremental_updates"] += 1
                            self.statistics["successful_calculations"] += 1

                            if save_result:
                                self.csv_handler.save_etf_williams_data(
                                    etf_code, incremental_result, threshold
                                )

                            calculation_time = (
                                datetime.now() - calculation_start_time
                            ).total_seconds() * 1000
                            self._update_performance_metrics(calculation_time)

                            return {
                                "success": True,
                                "etf_code": etf_code,
                                "data_source": "incremental_update",
                                "record_count": len(incremental_result),
                                "data": incremental_result,
                                "calculation_time_ms": calculation_time,
                            }
                    else:
                        # ä½¿ç”¨å®Œæ•´ç¼“å­˜
                        print(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜æ•°æ®: {etf_code}")
                        self.statistics["cache_hits"] += 1
                        self.statistics["successful_calculations"] += 1

                        if save_result:
                            self.csv_handler.save_etf_williams_data(
                                etf_code, cached_data, threshold
                            )

                        calculation_time = (
                            datetime.now() - calculation_start_time
                        ).total_seconds() * 1000
                        self._update_performance_metrics(calculation_time)

                        return {
                            "success": True,
                            "etf_code": etf_code,
                            "data_source": "cache",
                            "record_count": len(cached_data),
                            "data": cached_data,
                            "calculation_time_ms": calculation_time,
                        }

            # 3. ç¼“å­˜æœªå‘½ä¸­æˆ–æ— æ•ˆï¼Œè¿›è¡Œå…¨é‡è®¡ç®—
            self.statistics["cache_misses"] += 1
            self.statistics["full_calculations"] += 1
            print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¼˜åŒ–è®¡ç®—: {etf_code}")

            # ä½¿ç”¨ä¼˜åŒ–å¼•æ“è®¡ç®—å¨å»‰æŒ‡æ ‡
            williams_result = self.williams_engine.calculate_williams_indicators_batch(
                etf_data
            )

            if williams_result.empty:
                error_msg = f"å¨å»‰æŒ‡æ ‡è®¡ç®—å¤±è´¥: {etf_code}"
                print(f"âŒ {error_msg}")
                self.statistics["failed_calculations"] += 1
                self.statistics["failed_etf_list"].append(etf_code)
                return {"success": False, "error": error_msg, "etf_code": etf_code}

            # 4. æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
            formatted_data = self.williams_engine.format_output_data(
                williams_result, etf_code
            )

            if formatted_data.empty:
                error_msg = f"æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {etf_code}"
                print(f"âŒ {error_msg}")
                self.statistics["failed_calculations"] += 1
                self.statistics["failed_etf_list"].append(etf_code)
                return {"success": False, "error": error_msg, "etf_code": etf_code}

            # 5. ä¿å­˜ç¼“å­˜å’Œç»“æœ
            if save_result:
                try:
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self.cache_manager.save_etf_cache(
                        etf_code, formatted_data, threshold
                    )

                    # ä¿å­˜åˆ°æœ€ç»ˆè¾“å‡ºç›®å½•
                    self.csv_handler.save_etf_williams_data(
                        etf_code, formatted_data, threshold
                    )

                except Exception as save_error:
                    print(f"âš ï¸ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {etf_code} - {str(save_error)}")

            # 6. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            calculation_time = (
                datetime.now() - calculation_start_time
            ).total_seconds() * 1000
            self._update_performance_metrics(calculation_time)

            # 7. è¿”å›æˆåŠŸç»“æœ
            self.statistics["successful_calculations"] += 1
            print(
                f"âœ… å¨å»‰æŒ‡æ ‡ä¼˜åŒ–è®¡ç®—å®Œæˆ: {etf_code} (è€—æ—¶: {calculation_time:.2f}ms)"
            )

            return {
                "success": True,
                "etf_code": etf_code,
                "data_source": "optimized_calculation",
                "record_count": len(formatted_data),
                "data": formatted_data,
                "calculation_time_ms": calculation_time,
                "statistics": self.williams_engine.calculate_williams_statistics(
                    williams_result
                )
                if hasattr(self.williams_engine, "calculate_williams_statistics")
                else {},
            }

        except Exception as e:
            error_msg = f"ä¼˜åŒ–è®¡ç®—è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {etf_code} - {str(e)}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

            self.statistics["failed_calculations"] += 1
            self.statistics["failed_etf_list"].append(etf_code)

            return {
                "success": False,
                "error": error_msg,
                "etf_code": etf_code,
                "exception_type": type(e).__name__,
            }

    def _try_incremental_update(self, etf_code, threshold, cached_data, new_etf_data):
        """
        å°è¯•å¢é‡æ›´æ–°

        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            cached_data: ç¼“å­˜æ•°æ®
            new_etf_data: æ–°ETFæ•°æ®

        Returns:
            DataFrame: å¢é‡æ›´æ–°ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
            if "æ—¥æœŸ" not in new_etf_data.columns or "date" not in cached_data.columns:
                return None

            # è·å–ç¼“å­˜æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            cached_latest_date = cached_data["date"].max()

            # è½¬æ¢æ–°æ•°æ®çš„æ—¥æœŸæ ¼å¼è¿›è¡Œæ¯”è¾ƒ
            new_etf_data_copy = new_etf_data.copy()
            new_etf_data_copy["date"] = new_etf_data_copy["æ—¥æœŸ"].dt.strftime(
                "%Y-%m-%d"
            )

            # ç­›é€‰å‡ºæ¯”ç¼“å­˜æ›´æ–°çš„æ•°æ®
            new_data_mask = new_etf_data_copy["date"] > cached_latest_date
            truly_new_data = new_etf_data[new_data_mask]

            if truly_new_data.empty:
                # æ²¡æœ‰æ–°æ•°æ®ï¼Œè¿”å›ç¼“å­˜æ•°æ®
                print(f"ğŸ“Š æ— æ–°æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜: {etf_code}")
                return cached_data

            # ä½¿ç”¨ä¼˜åŒ–å¼•æ“çš„å¢é‡æ›´æ–°åŠŸèƒ½
            if hasattr(self.williams_engine, "calculate_incremental_update"):
                # å‡†å¤‡ç°æœ‰æ•°æ®æ ¼å¼
                existing_data_for_calc = (
                    new_etf_data[~new_data_mask]
                    if (~new_data_mask).any()
                    else new_etf_data.head(0)
                )

                incremental_result = self.williams_engine.calculate_incremental_update(
                    existing_data_for_calc, truly_new_data
                )

                if not incremental_result.empty:
                    # æ ¼å¼åŒ–å¢é‡ç»“æœ
                    formatted_incremental = self.williams_engine.format_output_data(
                        incremental_result, etf_code
                    )

                    # åˆå¹¶ç¼“å­˜æ•°æ®å’Œå¢é‡æ•°æ®
                    combined_result = self.csv_handler.merge_incremental_data(
                        etf_code, threshold, formatted_incremental
                    )

                    return combined_result

            return None

        except Exception as e:
            print(f"âš ï¸ å¢é‡æ›´æ–°å°è¯•å¤±è´¥: {etf_code} - {str(e)}")
            return None

    def _update_performance_metrics(self, calculation_time_ms):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.statistics["optimization_metrics"]["total_calculation_time_ms"] += (
            calculation_time_ms
        )

        if self.statistics["successful_calculations"] > 0:
            self.statistics["optimization_metrics"]["average_calculation_time_ms"] = (
                self.statistics["optimization_metrics"]["total_calculation_time_ms"]
                / self.statistics["successful_calculations"]
            )

    def _calculate_hit_rate(self):
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_cache_operations = (
            self.statistics["cache_hits"] + self.statistics["cache_misses"]
        )
        if total_cache_operations > 0:
            return (self.statistics["cache_hits"] / total_cache_operations) * 100
        return 0.0

    def _calculate_incremental_update_rate(self):
        """è®¡ç®—å¢é‡æ›´æ–°ç‡"""
        if self.statistics["processed_etfs"] > 0:
            return (
                self.statistics["incremental_updates"]
                / self.statistics["processed_etfs"]
            ) * 100
        return 0.0

    def get_optimization_summary(self):
        """è·å–ä¼˜åŒ–æ•ˆæœæ‘˜è¦"""
        try:
            summary = {
                "optimization_status": "enabled"
                if self.use_optimized_components
                else "disabled",
                "performance_metrics": self.statistics["optimization_metrics"],
                "cache_performance": {
                    "hit_rate": self._calculate_hit_rate(),
                    "incremental_update_rate": self._calculate_incremental_update_rate(),
                },
                "calculation_breakdown": {
                    "full_calculations": self.statistics["full_calculations"],
                    "incremental_updates": self.statistics["incremental_updates"],
                    "cache_hits": self.statistics["cache_hits"],
                },
            }

            return summary

        except Exception as e:
            print(f"âš ï¸ è·å–ä¼˜åŒ–æ‘˜è¦å¤±è´¥: {str(e)}")
            return {}

    def print_optimization_summary(self):
        """æ‰“å°ä¼˜åŒ–æ•ˆæœæ‘˜è¦"""
        summary = self.get_optimization_summary()

        if summary:
            print(f"\n{'=' * 60}")
            print("ğŸš€ å¨å»‰æŒ‡æ ‡ç³»ç»Ÿä¼˜åŒ–æ•ˆæœæ‘˜è¦")
            print(f"{'=' * 60}")
            print(
                f"ğŸ”§ ä¼˜åŒ–çŠ¶æ€: {'å·²å¯ç”¨' if summary['optimization_status'] == 'enabled' else 'æœªå¯ç”¨'}"
            )
            print(
                f"â±ï¸ å¹³å‡è®¡ç®—æ—¶é—´: {summary['performance_metrics']['average_calculation_time_ms']:.2f}ms"
            )
            print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {summary['cache_performance']['hit_rate']:.2f}%")
            print(
                f"âš¡ å¢é‡æ›´æ–°ç‡: {summary['cache_performance']['incremental_update_rate']:.2f}%"
            )
            print("ğŸ“Š è®¡ç®—åˆ†å¸ƒ:")
            print(
                f"   â€¢ å…¨é‡è®¡ç®—: {summary['calculation_breakdown']['full_calculations']}æ¬¡"
            )
            print(
                f"   â€¢ å¢é‡æ›´æ–°: {summary['calculation_breakdown']['incremental_updates']}æ¬¡"
            )
            print(f"   â€¢ ç¼“å­˜å‘½ä¸­: {summary['calculation_breakdown']['cache_hits']}æ¬¡")
            print(f"{'=' * 60}")

    # å‘åå…¼å®¹æ¥å£
    def calculate_single_etf(self, etf_code, threshold, save_result=True):
        """å‘åå…¼å®¹æ¥å£"""
        return self.calculate_single_etf_optimized(etf_code, threshold, save_result)

    def calculate_batch_etfs(self, etf_codes, threshold):
        """æ‰¹é‡è®¡ç®—ETFå¨å»‰æŒ‡æ ‡ï¼ˆç»§æ‰¿åŸæœ‰é€»è¾‘ï¼Œä½¿ç”¨ä¼˜åŒ–è®¡ç®—ï¼‰"""
        try:
            print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ‰¹é‡è®¡ç®—å¨å»‰æŒ‡æ ‡: {threshold}")
            print(f"ğŸ“Š å¾…å¤„ç†ETFæ•°é‡: {len(etf_codes)}")

            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self._initialize_statistics()
            self.statistics["start_time"] = datetime.now()

            batch_results = {
                "threshold": threshold,
                "total_count": len(etf_codes),
                "success_count": 0,
                "fail_count": 0,
                "results": {},
                "failed_etfs": [],
                "start_time": self.statistics["start_time"],
                "end_time": None,
                "duration_seconds": 0,
                "optimization_used": self.use_optimized_components,
            }

            # é€ä¸ªå¤„ç†ETF
            for i, etf_code in enumerate(etf_codes, 1):
                try:
                    print(f"ğŸ“ˆ ä¼˜åŒ–å¤„ç†è¿›åº¦: {i}/{len(etf_codes)} - {etf_code}")

                    result = self.calculate_single_etf_optimized(
                        etf_code, threshold, save_result=True
                    )
                    batch_results["results"][etf_code] = result

                    if result["success"]:
                        batch_results["success_count"] += 1
                    else:
                        batch_results["fail_count"] += 1
                        batch_results["failed_etfs"].append(etf_code)

                    # æ¯å¤„ç†æŒ‡å®šæ•°é‡ETFæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦å’Œä¼˜åŒ–æ•ˆæœ
                    if i % PROGRESS_DISPLAY_INTERVAL == 0:
                        success_rate = (batch_results["success_count"] / i) * 100
                        avg_time = self.statistics["optimization_metrics"][
                            "average_calculation_time_ms"
                        ]
                        print(
                            f"ğŸ“Š ä¸­é—´ç»Ÿè®¡: æˆåŠŸç‡ {success_rate:.1f}%, å¹³å‡è€—æ—¶ {avg_time:.2f}ms"
                        )

                except Exception as e:
                    print(f"âŒ å¤„ç†ETFæ—¶å‘ç”Ÿå¼‚å¸¸: {etf_code} - {str(e)}")
                    batch_results["fail_count"] += 1
                    batch_results["failed_etfs"].append(etf_code)
                    batch_results["results"][etf_code] = {
                        "success": False,
                        "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                        "etf_code": etf_code,
                    }

            # å®Œæˆç»Ÿè®¡
            self.statistics["end_time"] = datetime.now()
            batch_results["end_time"] = self.statistics["end_time"]
            batch_results["duration_seconds"] = (
                batch_results["end_time"] - batch_results["start_time"]
            ).total_seconds()

            # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
            self.cache_manager.update_global_cache_stats(threshold)

            # æ‰“å°æ‰¹é‡å¤„ç†ç»“æœå’Œä¼˜åŒ–æ•ˆæœ
            self._print_batch_results_optimized(batch_results)

            return batch_results

        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ‰¹é‡è®¡ç®—è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            raise

    def _print_batch_results_optimized(self, batch_results):
        """æ‰“å°ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†ç»“æœ"""
        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š å¨å»‰æŒ‡æ ‡ä¼˜åŒ–æ‰¹é‡è®¡ç®—ç»“æœ - {batch_results['threshold']}")
        print(f"{'=' * 60}")

        success_rate = (
            (batch_results["success_count"] / batch_results["total_count"] * 100)
            if batch_results["total_count"] > 0
            else 0
        )

        print("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   â€¢ æ€»ETFæ•°é‡: {batch_results['total_count']}")
        print(f"   â€¢ æˆåŠŸæ•°é‡: {batch_results['success_count']}")
        print(f"   â€¢ å¤±è´¥æ•°é‡: {batch_results['fail_count']}")
        print(f"   â€¢ æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"   â€¢ å¤„ç†æ—¶é•¿: {batch_results['duration_seconds']:.1f}ç§’")
        print(
            f"   â€¢ ä¼˜åŒ–çŠ¶æ€: {'âœ… å·²å¯ç”¨' if batch_results['optimization_used'] else 'âŒ æœªå¯ç”¨'}"
        )

        # ä¼˜åŒ–æ•ˆæœç»Ÿè®¡
        avg_time = self.statistics["optimization_metrics"][
            "average_calculation_time_ms"
        ]
        print("ğŸš€ ä¼˜åŒ–æ•ˆæœ:")
        print(f"   â€¢ å¹³å‡è®¡ç®—æ—¶é—´: {avg_time:.2f}ms")
        print(f"   â€¢ å…¨é‡è®¡ç®—: {self.statistics['full_calculations']}æ¬¡")
        print(f"   â€¢ å¢é‡æ›´æ–°: {self.statistics['incremental_updates']}æ¬¡")
        print(f"   â€¢ ç¼“å­˜å‘½ä¸­: {self.statistics['cache_hits']}æ¬¡")

        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.cache_manager.get_cache_summary()
        if cache_stats:
            print("ğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
            print(
                f"   â€¢ ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['performance']['hit_rate_percent']:.2f}%"
            )

        # å¤±è´¥ETFåˆ—è¡¨
        if batch_results["failed_etfs"]:
            print("âŒ å¤±è´¥ETFåˆ—è¡¨:")
            failed_display = batch_results["failed_etfs"][:MAX_FAILED_ETF_DISPLAY]
            print(f"   â€¢ {', '.join(failed_display)}")
            if len(batch_results["failed_etfs"]) > MAX_FAILED_ETF_DISPLAY:
                print(
                    f"   â€¢ ... ä»¥åŠå…¶ä»–{len(batch_results['failed_etfs']) - MAX_FAILED_ETF_DISPLAY}ä¸ª"
                )

        print(f"{'=' * 60}")


if __name__ == "__main__":
    # ä¼˜åŒ–ä¸»æ§åˆ¶å™¨æµ‹è¯•
    try:
        print("ğŸ§ª å¨å»‰æŒ‡æ ‡ä¼˜åŒ–ä¸»æ§åˆ¶å™¨æµ‹è¯•")

        # åˆå§‹åŒ–ä¼˜åŒ–æ§åˆ¶å™¨
        controller = WilliamsMainControllerOptimized(use_optimized_components=True)

        # æ‰“å°ä¼˜åŒ–æ‘˜è¦
        controller.print_optimization_summary()

        print("âœ… ä¼˜åŒ–ä¸»æ§åˆ¶å™¨æµ‹è¯•å®Œæˆ")

    except Exception as e:
        print(f"âŒ ä¼˜åŒ–ä¸»æ§åˆ¶å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
