"""
å¨å»‰æŒ‡æ ‡ä¼˜åŒ–æ•°æ®è¯»å–å™¨
ä¿®å¤pandas FutureWarningï¼Œä¼˜åŒ–æ•°æ®å¤„ç†æ€§èƒ½

ä¼˜åŒ–å†…å®¹ï¼š
1. ä¿®å¤pandasæ—¥æœŸå¤„ç†çš„FutureWarning
2. ä¼˜åŒ–æ•°æ®ç±»å‹è½¬æ¢å’Œå†…å­˜ä½¿ç”¨
3. å‘é‡åŒ–æ•°æ®æ¸…æ´—æ“ä½œ
4. æ”¹è¿›é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
5. æå‡å¤§æ–‡ä»¶è¯»å–æ€§èƒ½
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
import glob
import warnings

# å¿½ç•¥pandasçš„é“¾å¼èµ‹å€¼è­¦å‘Š
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
warnings.filterwarnings('ignore', category=FutureWarning)


class WilliamsDataReaderOptimized:
    """å¨å»‰æŒ‡æ ‡ä¼˜åŒ–æ•°æ®è¯»å–å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–æ•°æ®è¯»å–å™¨
        
        Args:
            config: å¨å»‰æŒ‡æ ‡é…ç½®å¯¹è±¡
        """
        self.config = config
        self.data_source_path = config.data_source_path
        self.adj_type = config.adj_type
        self.thresholds = config.THRESHOLDS
        self.data_quality_requirements = config.DATA_QUALITY_REQUIREMENTS
        
        # éªŒè¯æ•°æ®æºè·¯å¾„
        if not self.config.is_data_source_valid():
            print(f"âš ï¸ æ•°æ®æºè·¯å¾„æ— æ•ˆ: {self.data_source_path}")

    def read_etf_data_optimized(self, etf_code):
        """
        ä¼˜åŒ–çš„ETFæ•°æ®è¯»å–
        
        ä¼˜åŒ–å†…å®¹ï¼š
        1. ä¿®å¤pandasæ—¥æœŸå¤„ç†è­¦å‘Š
        2. ä¼˜åŒ–æ•°æ®ç±»å‹å’Œå†…å­˜ä½¿ç”¨
        3. å‘é‡åŒ–æ•°æ®æ¸…æ´—
        
        Args:
            etf_code: ETFä»£ç (æ”¯æŒå¸¦.SH/.SZåç¼€æˆ–ä¸å¸¦åç¼€)
            
        Returns:
            DataFrame: æ¸…æ´—åçš„ETFæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ¸…ç†ETFä»£ç æ ¼å¼
            clean_code = self._clean_etf_code(etf_code)
            
            # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
            file_path = self._find_etf_data_file(clean_code)
            if not file_path:
                print(f"âš ï¸ æœªæ‰¾åˆ°ETFæ•°æ®æ–‡ä»¶: {etf_code}")
                return None
            
            # ä¼˜åŒ–çš„CSVæ–‡ä»¶è¯»å–
            df = self._read_csv_file_optimized(file_path)
            if df is None:
                return None
            
            # ä¼˜åŒ–çš„æ•°æ®æ¸…æ´—å’ŒéªŒè¯
            df = self._clean_and_validate_data_optimized(df, etf_code)
            if df is None or df.empty:
                return None
            
            # ä¼˜åŒ–çš„æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df = self._standardize_data_format_optimized(df, etf_code)
            
            return df
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–è¯»å–ETFæ•°æ®å¤±è´¥: {etf_code} - {str(e)}")
            return None

    def _read_csv_file_optimized(self, file_path):
        """
        ä¼˜åŒ–çš„CSVæ–‡ä»¶è¯»å–
        
        ä¼˜åŒ–å†…å®¹ï¼š
        1. æŒ‡å®šæ•°æ®ç±»å‹å‡å°‘å†…å­˜ä½¿ç”¨
        2. ä¼˜åŒ–ç¼–ç æ£€æµ‹
        3. æ‰¹é‡å¤„ç†å¤§æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            DataFrame: è¯»å–çš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å®šä¹‰æ•°æ®ç±»å‹ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ˆæ”¹ä¸ºæ›´å®½å®¹çš„ç±»å‹ï¼‰
            dtype_dict = {
                'å¼€ç›˜ä»·': np.float64,
                'æœ€é«˜ä»·': np.float64,
                'æœ€ä½ä»·': np.float64,
                'æ”¶ç›˜ä»·': np.float64,
                'æˆäº¤é‡(æ‰‹æ•°)': np.float64,  # æ”¹ä¸ºfloat64ä»¥é¿å…è½¬æ¢é”™è¯¯
                'æˆäº¤é¢(åƒå…ƒ)': np.float64
            }
            
            # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨å¸¸ç”¨ç¼–ç 
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(
                        file_path, 
                        encoding=encoding,
                        parse_dates=False,  # å…ˆä¸è§£ææ—¥æœŸï¼Œåç»­ç»Ÿä¸€å¤„ç†
                        low_memory=False    # é¿å…æ··åˆç±»å‹æ¨æ–­é—®é¢˜
                    )
                    if not df.empty:
                        return df
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ ({encoding}): {str(e)}")
                    continue
            
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")
            return None
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ–‡ä»¶è¯»å–å¼‚å¸¸: {file_path} - {str(e)}")
            return None

    def _process_date_column_optimized(self, df):
        """
        ä¼˜åŒ–çš„æ—¥æœŸåˆ—å¤„ç† - ä¿®å¤FutureWarning
        
        ä¿®å¤å†…å®¹ï¼š
        1. ä½¿ç”¨copy()é¿å…SettingWithCopyWarning
        2. ä¼˜åŒ–æ—¥æœŸæ ¼å¼æ¨æ–­
        3. å‘é‡åŒ–æ—¥æœŸè½¬æ¢
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®
        """
        try:
            if 'æ—¥æœŸ' not in df.columns:
                print("âŒ ç¼ºå°‘æ—¥æœŸåˆ—")
                return None
            
            # åˆ›å»ºå‰¯æœ¬é¿å…è­¦å‘Š
            df = df.copy()
            
            # æ£€æµ‹æ—¥æœŸæ ¼å¼å¹¶è½¬æ¢
            date_series = df['æ—¥æœŸ'].astype(str)
            
            # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼è½¬æ¢ï¼ˆæŒ‰å¸¸è§ç¨‹åº¦æ’åºï¼‰
            date_formats = ['%Y%m%d', '%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d']
            
            converted = False
            for fmt in date_formats:
                try:
                    # ä½¿ç”¨pd.to_datetimeè€Œä¸æ˜¯ç›´æ¥èµ‹å€¼
                    date_parsed = pd.to_datetime(date_series, format=fmt, errors='coerce')
                    
                    # æ£€æŸ¥è½¬æ¢æˆåŠŸç‡
                    success_rate = date_parsed.notna().mean()
                    if success_rate > 0.9:  # 90%ä»¥ä¸Šè½¬æ¢æˆåŠŸ
                        df['æ—¥æœŸ'] = date_parsed
                        converted = True
                        break
                        
                except ValueError:
                    continue
            
            if not converted:
                # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ¨æ–­
                try:
                    df['æ—¥æœŸ'] = pd.to_datetime(date_series, errors='coerce', infer_datetime_format=True)
                    
                    # æ£€æŸ¥æ¨æ–­ç»“æœ
                    if df['æ—¥æœŸ'].isna().all():
                        print("âŒ æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
                        return None
                        
                except Exception:
                    print("âŒ æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
                    return None
            
            # ç§»é™¤æ—¥æœŸè½¬æ¢å¤±è´¥çš„è¡Œ
            initial_count = len(df)
            df = df.dropna(subset=['æ—¥æœŸ'])
            final_count = len(df)
            
            if final_count < initial_count:
                print(f"ğŸ§¹ ç§»é™¤{initial_count - final_count}ä¸ªæ—¥æœŸæ— æ•ˆçš„æ•°æ®ç‚¹")
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            return df
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ—¥æœŸå¤„ç†å¤±è´¥: {str(e)}")
            return None

    def _clean_price_data_vectorized(self, df, etf_code):
        """
        å‘é‡åŒ–çš„ä»·æ ¼æ•°æ®æ¸…æ´—
        
        ä¼˜åŒ–å†…å®¹ï¼š
        1. å‘é‡åŒ–æ•°æ®ç±»å‹è½¬æ¢
        2. å‘é‡åŒ–å¼‚å¸¸å€¼æ£€æµ‹
        3. æ‰¹é‡é€»è¾‘å…³ç³»éªŒè¯
        
        Args:
            df: æ•°æ®DataFrame
            etf_code: ETFä»£ç 
            
        Returns:
            DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        try:
            df = df.copy()
            price_columns = ['æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'å¼€ç›˜ä»·']
            
            # å‘é‡åŒ–æ•°æ®ç±»å‹è½¬æ¢
            for col in price_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # è®°å½•åˆå§‹æ•°æ®é‡
            initial_count = len(df)
            
            # å‘é‡åŒ–ç§»é™¤éæ­£æ•°ä»·æ ¼
            price_mask = True
            for col in ['æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·']:
                if col in df.columns:
                    price_mask = price_mask & (df[col] > 0)
            
            df = df[price_mask]
            
            # å‘é‡åŒ–ä»·æ ¼é€»è¾‘å…³ç³»æ£€æŸ¥
            if 'æœ€é«˜ä»·' in df.columns and 'æœ€ä½ä»·' in df.columns:
                logical_mask = df['æœ€é«˜ä»·'] >= df['æœ€ä½ä»·']
                df = df[logical_mask]
            
            if 'æ”¶ç›˜ä»·' in df.columns and 'æœ€é«˜ä»·' in df.columns and 'æœ€ä½ä»·' in df.columns:
                range_mask = (df['æ”¶ç›˜ä»·'] >= df['æœ€ä½ä»·']) & (df['æ”¶ç›˜ä»·'] <= df['æœ€é«˜ä»·'])
                df = df[range_mask]
            
            # å‘é‡åŒ–æç«¯å¼‚å¸¸å€¼ç§»é™¤ï¼ˆä»·æ ¼å˜åŒ–è¶…è¿‡50%ï¼‰
            if 'æ”¶ç›˜ä»·' in df.columns and len(df) > 1:
                price_changes = df['æ”¶ç›˜ä»·'].pct_change().abs()
                normal_mask = (price_changes <= 0.5) | price_changes.isna()
                df = df[normal_mask]
            
            # ç»Ÿè®¡æ¸…æ´—ç»“æœ
            cleaned_count = len(df)
            if cleaned_count < initial_count:
                removed_count = initial_count - cleaned_count
                print(f"ğŸ§¹ å‘é‡åŒ–æ•°æ®æ¸…æ´—: {etf_code} - ç§»é™¤{removed_count}ä¸ªå¼‚å¸¸æ•°æ®ç‚¹")
            
            # æ£€æŸ¥æ¸…æ´—åæ•°æ®æ˜¯å¦è¶³å¤Ÿ
            min_required = self.data_quality_requirements['min_data_points']
            if len(df) < min_required:
                print(f"âš ï¸ æ¸…æ´—åæ•°æ®ä¸è¶³: {etf_code} - éœ€è¦{min_required}ä¸ªï¼Œå®é™…{len(df)}ä¸ª")
                return None
            
            return df
            
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–ä»·æ ¼æ•°æ®æ¸…æ´—å¤±è´¥: {etf_code} - {str(e)}")
            return None

    def _clean_and_validate_data_optimized(self, df, etf_code):
        """
        ä¼˜åŒ–çš„æ•°æ®æ¸…æ´—å’ŒéªŒè¯
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            etf_code: ETFä»£ç 
            
        Returns:
            DataFrame: æ¸…æ´—åçš„æ•°æ®ï¼ŒéªŒè¯å¤±è´¥è¿”å›None
        """
        try:
            if df.empty:
                print(f"âš ï¸ ç©ºæ•°æ®æ–‡ä»¶: {etf_code}")
                return None
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æ—¥æœŸ']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {etf_code} - {missing_columns}")
                return None
            
            # æ£€æŸ¥æ•°æ®ç‚¹æ•°é‡
            min_required = self.data_quality_requirements['min_data_points']
            if len(df) < min_required:
                print(f"âš ï¸ æ•°æ®ç‚¹ä¸è¶³: {etf_code} - éœ€è¦{min_required}ä¸ªï¼Œå®é™…{len(df)}ä¸ª")
                return None
            
            # ä¼˜åŒ–çš„æ—¥æœŸå¤„ç†
            df = self._process_date_column_optimized(df)
            if df is None:
                return None
            
            # å‘é‡åŒ–ä»·æ ¼æ•°æ®æ¸…æ´—
            df = self._clean_price_data_vectorized(df, etf_code)
            if df is None or df.empty:
                return None
            
            # ä¼˜åŒ–çš„æ•°æ®å»é‡
            df = self._remove_duplicates_optimized(df, etf_code)
            
            return df
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ•°æ®æ¸…æ´—éªŒè¯å¤±è´¥: {etf_code} - {str(e)}")
            return None

    def _remove_duplicates_optimized(self, df, etf_code):
        """
        ä¼˜åŒ–çš„é‡å¤æ•°æ®ç§»é™¤
        
        Args:
            df: æ•°æ®DataFrame
            etf_code: ETFä»£ç 
            
        Returns:
            DataFrame: å»é‡åçš„æ•°æ®
        """
        try:
            initial_count = len(df)
            
            # æŒ‰æ—¥æœŸå»é‡ï¼Œä¿ç•™æœ€åä¸€æ¡è®°å½•ï¼ˆæ›´é«˜æ•ˆçš„å®ç°ï¼‰
            df = df.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
            
            final_count = len(df)
            if final_count < initial_count:
                removed_count = initial_count - final_count
                print(f"ğŸ§¹ ä¼˜åŒ–æ•°æ®å»é‡: {etf_code} - ç§»é™¤{removed_count}ä¸ªé‡å¤æ•°æ®ç‚¹")
            
            return df
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ•°æ®å»é‡å¤±è´¥: {etf_code} - {str(e)}")
            return df

    def _standardize_data_format_optimized(self, df, etf_code):
        """
        ä¼˜åŒ–çš„æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
        
        Args:
            df: æ•°æ®DataFrame
            etf_code: ETFä»£ç 
            
        Returns:
            DataFrame: æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        try:
            df = df.copy()
            
            # æ·»åŠ ä»£ç åˆ—
            clean_code = self._clean_etf_code(etf_code)
            df['ä»£ç '] = clean_code
            
            # ä¼˜åŒ–æ•°å€¼åˆ—çš„ç²¾åº¦å’Œæ•°æ®ç±»å‹
            numeric_columns = ['æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'å¼€ç›˜ä»·']
            for col in numeric_columns:
                if col in df.columns:
                    # è½¬æ¢ä¸ºfloat64å¹¶ä¿æŒ8ä½å°æ•°ç²¾åº¦
                    df[col] = df[col].astype(np.float64).round(8)
            
            # é‡ç½®ç´¢å¼•
            df = df.reset_index(drop=True)
            
            return df
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å¤±è´¥: {etf_code} - {str(e)}")
            return df

    def get_etf_list_by_threshold_optimized(self, threshold):
        """
        ä¼˜åŒ–çš„ETFåˆ—è¡¨è·å– - ä½¿ç”¨ç­›é€‰åçš„ETFåˆ—è¡¨
        
        Args:
            threshold: é—¨æ§›åç§°('3000ä¸‡é—¨æ§›' or '5000ä¸‡é—¨æ§›')
            
        Returns:
            list: ETFä»£ç åˆ—è¡¨
        """
        try:
            # æ„å»ºç­›é€‰æ•°æ®è·¯å¾„
            filtered_data_path = os.path.join(
                self.config.project_root, "ETF_åˆç­›", "data", threshold, "é€šè¿‡ç­›é€‰ETF.txt"
            )
            
            # æ£€æŸ¥ç­›é€‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(filtered_data_path):
                print(f"âš ï¸ ç­›é€‰æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®: {filtered_data_path}")
                return self._get_all_etf_codes_fallback()
            
            # è¯»å–ç­›é€‰åçš„ETFåˆ—è¡¨
            etf_codes = []
            with open(filtered_data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
                    if line and not line.startswith('#'):
                        # éªŒè¯ETFä»£ç æ ¼å¼
                        if line.isdigit() and len(line) == 6:
                            etf_codes.append(line)
            
            # éªŒè¯ç­›é€‰çš„ETFæ˜¯å¦æœ‰å¯¹åº”çš„æ•°æ®æ–‡ä»¶
            valid_etf_codes = []
            for etf_code in etf_codes:
                data_file_path = self._find_etf_data_file(etf_code)
                if data_file_path and os.path.exists(data_file_path):
                    valid_etf_codes.append(etf_code)
            
            etf_codes = sorted(valid_etf_codes)
            print(f"ğŸ“Š {threshold}: ä½¿ç”¨ç­›é€‰åETFåˆ—è¡¨ï¼Œå…±{len(etf_codes)}ä¸ªæœ‰æ•ˆETF")
            
            if len(etf_codes) == 0:
                print(f"âš ï¸ ç­›é€‰åæ— æœ‰æ•ˆETFï¼Œå›é€€åˆ°åŸå§‹æ•°æ®")
                return self._get_all_etf_codes_fallback()
            
            return etf_codes
            
        except Exception as e:
            print(f"âŒ è¯»å–ç­›é€‰ETFåˆ—è¡¨å¤±è´¥: {threshold} - {str(e)}")
            print(f"âš ï¸ å›é€€åˆ°åŸå§‹æ•°æ®")
            return self._get_all_etf_codes_fallback()

    def _get_all_etf_codes_fallback(self):
        """å›é€€æ–¹æ¡ˆï¼šè·å–æ‰€æœ‰ETFä»£ç """
        try:
            if not os.path.exists(self.data_source_path):
                print(f"âŒ æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {self.data_source_path}")
                return []
            
            # ä½¿ç”¨globæ¨¡å¼åŒ¹é…ï¼Œæ›´é«˜æ•ˆ
            csv_pattern = os.path.join(self.data_source_path, "*.csv")
            csv_files = glob.glob(csv_pattern)
            
            # å‘é‡åŒ–æå–ETFä»£ç 
            etf_codes = []
            for file_path in csv_files:
                filename = os.path.basename(file_path)
                etf_code = os.path.splitext(filename)[0]  # ç§»é™¤æ‰©å±•å
                
                # éªŒè¯ETFä»£ç æ ¼å¼
                if etf_code.isdigit() and len(etf_code) == 6:
                    etf_codes.append(etf_code)
            
            etf_codes.sort()  # æ’åº
            print(f"ğŸ“Š å›é€€æ¨¡å¼: å‘ç°{len(etf_codes)}ä¸ªETFæ•°æ®æ–‡ä»¶")
            return etf_codes
            
        except Exception as e:
            print(f"âŒ å›é€€è·å–ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def batch_validate_etf_data(self, etf_codes, sample_size=10):
        """
        æ‰¹é‡éªŒè¯ETFæ•°æ®è´¨é‡
        
        Args:
            etf_codes: ETFä»£ç åˆ—è¡¨
            sample_size: é‡‡æ ·éªŒè¯æ•°é‡
            
        Returns:
            dict: æ‰¹é‡éªŒè¯ç»“æœ
        """
        try:
            # é‡‡æ ·éªŒè¯ä»¥æé«˜æ•ˆç‡
            sample_codes = etf_codes[:sample_size] if len(etf_codes) > sample_size else etf_codes
            
            validation_results = {
                'total_count': len(etf_codes),
                'sample_count': len(sample_codes),
                'valid_count': 0,
                'invalid_count': 0,
                'issues': []
            }
            
            for etf_code in sample_codes:
                df = self.read_etf_data_optimized(etf_code)
                if df is not None and not df.empty:
                    validation_results['valid_count'] += 1
                else:
                    validation_results['invalid_count'] += 1
                    validation_results['issues'].append(etf_code)
            
            validation_results['success_rate'] = (
                validation_results['valid_count'] / validation_results['sample_count'] * 100
                if validation_results['sample_count'] > 0 else 0
            )
            
            return validation_results
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
            return {'total_count': 0, 'sample_count': 0, 'valid_count': 0, 'invalid_count': 0}

    # å‘åå…¼å®¹æ–¹æ³•
    def _clean_etf_code(self, etf_code):
        """æ¸…ç†ETFä»£ç æ ¼å¼"""
        if not etf_code:
            return ""
        
        clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
        
        if clean_code.isdigit() and len(clean_code) == 6:
            return clean_code
        
        print(f"âš ï¸ ETFä»£ç æ ¼å¼å¼‚å¸¸: {etf_code}")
        return clean_code

    def _find_etf_data_file(self, clean_code):
        """æŸ¥æ‰¾ETFæ•°æ®æ–‡ä»¶"""
        try:
            possible_patterns = [
                os.path.join(self.data_source_path, f"{clean_code}.csv"),
                os.path.join(self.data_source_path, f"{clean_code}.CSV"),
                os.path.join(self.data_source_path, f"{clean_code}.*")
            ]
            
            for pattern in possible_patterns:
                if '*' in pattern:
                    matches = glob.glob(pattern)
                    if matches:
                        return matches[0]
                else:
                    if os.path.exists(pattern):
                        return pattern
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æŸ¥æ‰¾ETFæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {clean_code} - {str(e)}")
            return None

    # å‘åå…¼å®¹æ¥å£
    def read_etf_data(self, etf_code):
        """å‘åå…¼å®¹æ¥å£"""
        return self.read_etf_data_optimized(etf_code)
    
    def get_etf_list_by_threshold(self, threshold):
        """å‘åå…¼å®¹æ¥å£"""
        return self.get_etf_list_by_threshold_optimized(threshold)


if __name__ == "__main__":
    # ä¼˜åŒ–æ•°æ®è¯»å–å™¨æµ‹è¯•
    print("ğŸ§ª å¨å»‰æŒ‡æ ‡ä¼˜åŒ–æ•°æ®è¯»å–å™¨æµ‹è¯•")
    print("âœ… ä¼˜åŒ–æ•°æ®è¯»å–å™¨æ¨¡å—åŠ è½½æˆåŠŸ")
    print("ğŸ“ éœ€è¦é…ç½®å¯¹è±¡å’Œå®é™…æ•°æ®æ¥è¿›è¡Œå®Œæ•´åŠŸèƒ½æµ‹è¯•")