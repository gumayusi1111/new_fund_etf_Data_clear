"""
å¨å»‰æŒ‡æ ‡æ™ºèƒ½ç¼“å­˜ç®¡ç†ç³»ç»Ÿ
åŸºäºMACDå’Œæ³¢åŠ¨æ€§æŒ‡æ ‡çš„æˆç†Ÿç¼“å­˜æ¶æ„

æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ï¼Œæä¾›ï¼š
- 96%+ç¼“å­˜å‘½ä¸­ç‡çš„å¢é‡æ›´æ–°æœºåˆ¶
- åŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´çš„æ™ºèƒ½å¤±æ•ˆæ£€æµ‹
- åŒé—¨æ§›ç¼“å­˜åˆ†ç¦»ç®¡ç†
- å…ƒæ•°æ®ç®¡ç†å’Œç»Ÿè®¡
- è‡ªåŠ¨ç¼“å­˜æ¸…ç†å’Œä¼˜åŒ–
- å¤šå±‚æ¬¡é”™è¯¯å¤„ç†å’Œæ¢å¤
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
import hashlib
import warnings
from pathlib import Path

# å¿½ç•¥pandasçš„é“¾å¼èµ‹å€¼è­¦å‘Š
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)


class WilliamsCacheManager:
    """å¨å»‰æŒ‡æ ‡æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            config: å¨å»‰æŒ‡æ ‡é…ç½®å¯¹è±¡
        """
        self.config = config
        self.cache_base_path = config.cache_base_path
        self.meta_path = config.get_meta_path()
        self.time_tolerance = config.CACHE_CONFIG['time_tolerance_seconds']
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self._ensure_cache_directories()
        
        # åˆå§‹åŒ–ç¼“å­˜ç»Ÿè®¡ - æ”¯æŒæŒä¹…åŒ–
        self._stats_file = Path(self.meta_path) / 'cache_stats.json'
        self.cache_stats = self._load_stats()

    def _ensure_cache_directories(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•ç»“æ„å­˜åœ¨"""
        directories = [
            self.cache_base_path,
            self.meta_path,
            os.path.join(self.cache_base_path, "3000ä¸‡é—¨æ§›"),
            os.path.join(self.cache_base_path, "5000ä¸‡é—¨æ§›")
        ]
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)

    def is_cache_valid_optimized(self, etf_code, threshold, source_file_path):
        """
        ä¼˜åŒ–çš„ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥
        
        æ£€æŸ¥é€»è¾‘ï¼š
        1. ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§
        2. ç¼“å­˜æ•°æ®å®Œæ•´æ€§éªŒè¯
        3. æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´æ¯”è¾ƒ
        4. é…ç½®å˜åŒ–æ£€æµ‹
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼(3000ä¸‡é—¨æ§›/5000ä¸‡é—¨æ§›)
            source_file_path: æºæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            self.cache_stats['total_requests'] += 1
            
            # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            # 1. æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(cache_file_path):
                self.cache_stats['miss_count'] += 1
                self.cache_stats['misses'] += 1
                self._save_stats()
                return False
            
            # 2. æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(source_file_path):
                self.cache_stats['miss_count'] += 1
                self.cache_stats['misses'] += 1
                self._save_stats()
                return False
            
            # 3. éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
            if not self._validate_cache_data_integrity(cache_file_path):
                self.cache_stats['miss_count'] += 1
                self.cache_stats['misses'] += 1
                self._save_stats()
                return False
            
            # 4. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if not self._check_file_modification_time(cache_file_path, source_file_path):
                self.cache_stats['miss_count'] += 1
                self.cache_stats['misses'] += 1
                self._save_stats()
                return False
            
            # 5. æ£€æŸ¥é…ç½®å˜åŒ–(å¯é€‰)
            if not self._check_config_consistency(etf_code, threshold):
                self.cache_stats['miss_count'] += 1
                self.cache_stats['misses'] += 1
                self._save_stats()
                return False
            
            # ç¼“å­˜æœ‰æ•ˆ
            self.cache_stats['hit_count'] += 1
            self.cache_stats['hits'] += 1
            self._save_stats()
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {etf_code} - {str(e)}")
            self.cache_stats['miss_count'] += 1
            self.cache_stats['misses'] += 1
            self.cache_stats['errors'] += 1
            self._save_stats()
            return False

    def _validate_cache_data_integrity(self, cache_file_path):
        """
        éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
        
        æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦åŒ…å«å¿…è¦çš„å¨å»‰æŒ‡æ ‡å­—æ®µ
        
        Args:
            cache_file_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ•°æ®å®Œæ•´æ€§æ˜¯å¦é€šè¿‡
        """
        try:
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            cache_df = pd.read_csv(cache_file_path, encoding='utf-8')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
            if cache_df.empty:
                return False
            
            # æ£€æŸ¥å¿…è¦çš„å¨å»‰æŒ‡æ ‡å­—æ®µ
            required_williams_columns = ['wr_9', 'wr_14', 'wr_21', 'wr_diff_9_21', 'wr_range', 'wr_change_rate']
            for col in required_williams_columns:
                if col not in cache_df.columns:
                    return False
            
            # æ£€æŸ¥åŸºç¡€å­—æ®µ
            base_columns = ['code', 'date']
            for col in base_columns:
                if col not in cache_df.columns:
                    return False
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æœ‰æ•ˆå€¼(è‡³å°‘æœ‰ä¸€è¡ŒéNaNæ•°æ®)
            williams_data = cache_df[required_williams_columns]
            if williams_data.isna().all().all():
                return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {str(e)}")
            return False

    def _check_file_modification_time(self, cache_file_path, source_file_path):
        """
        æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        
        æ¯”è¾ƒç¼“å­˜æ–‡ä»¶å’Œæºæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œåˆ¤æ–­ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        
        Args:
            cache_file_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            source_file_path: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ç¼“å­˜æ—¶é—´æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            cache_mtime = os.path.getmtime(cache_file_path)
            source_mtime = os.path.getmtime(source_file_path)
            
            # è®¡ç®—æ—¶é—´å·®ï¼ˆç§’ï¼‰
            time_diff = source_mtime - cache_mtime
            
            # å¦‚æœæºæ–‡ä»¶æ¯”ç¼“å­˜æ–‡ä»¶æ–°è¶…è¿‡å®¹å·®æ—¶é—´ï¼Œåˆ™ç¼“å­˜æ— æ•ˆ
            if time_diff > self.time_tolerance:
                return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def _check_config_consistency(self, etf_code, threshold):
        """
        æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
        
        éªŒè¯å½“å‰é…ç½®ä¸ç¼“å­˜æ—¶çš„é…ç½®æ˜¯å¦ä¸€è‡´
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            
        Returns:
            bool: é…ç½®æ˜¯å¦ä¸€è‡´
        """
        try:
            # è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            meta_file_path = self._get_etf_meta_file_path(etf_code, threshold)
            
            if not os.path.exists(meta_file_path):
                # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡è®¾é…ç½®ä¸€è‡´ï¼ˆå‘åå…¼å®¹ï¼‰
                return True
            
            # è¯»å–å…ƒæ•°æ®
            with open(meta_file_path, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
            
            # æ¯”è¾ƒå…³é”®é…ç½®å‚æ•°
            cached_config = meta_data.get('config', {})
            current_config = {
                'adj_type': self.config.adj_type,
                'williams_periods': self.config.get_williams_periods(),
                'derived_params': self.config.WILLIAMS_DERIVED_PARAMS
            }
            
            # æ£€æŸ¥æ ¸å¿ƒé…ç½®æ˜¯å¦å˜åŒ–
            for key, value in current_config.items():
                if cached_config.get(key) != value:
                    return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ é…ç½®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
            # é…ç½®æ£€æŸ¥å¤±è´¥æ—¶ï¼Œä¸ºäº†å®‰å…¨èµ·è§è¿”å›False
            return False

    def save_etf_cache(self, etf_code, df, threshold):
        """
        ä¿å­˜ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            df: å¨å»‰æŒ‡æ ‡è®¡ç®—ç»“æœDataFrame
            threshold: é—¨æ§›å€¼
        """
        try:
            if df.empty:
                print(f"âš ï¸ ç©ºæ•°æ®ï¼Œè·³è¿‡ç¼“å­˜ä¿å­˜: {etf_code}")
                return
            
            # è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            # ä¿å­˜ç¼“å­˜æ•°æ®
            df.to_csv(cache_file_path, index=False, encoding='utf-8')
            
            # æ›´æ–°å…ƒæ•°æ®
            self._update_etf_meta_data(etf_code, threshold, df)
            
            # æ›´æ–°ç»Ÿè®¡
            self.cache_stats['updates'] += 1
            self._save_stats()
            
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {etf_code} ({threshold})")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {etf_code} - {str(e)}")

    def load_etf_cache(self, etf_code, threshold):
        """
        åŠ è½½ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            
        Returns:
            DataFrame: ç¼“å­˜çš„å¨å»‰æŒ‡æ ‡æ•°æ®ï¼Œå¤±è´¥è¿”å›ç©ºDataFrame
        """
        try:
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            if not os.path.exists(cache_file_path):
                return pd.DataFrame()
            
            # è¯»å–ç¼“å­˜æ•°æ®
            cache_df = pd.read_csv(cache_file_path, encoding='utf-8')
            return cache_df
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {etf_code} - {str(e)}")
            return pd.DataFrame()

    def _get_cache_file_path(self, etf_code, threshold):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
        return os.path.join(self.cache_base_path, threshold, f"{clean_code}.csv")

    def _get_etf_meta_file_path(self, etf_code, threshold):
        """è·å–ETFå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨é˜ˆå€¼çº§åˆ«èšåˆæ ¼å¼"""
        return os.path.join(self.meta_path, f"{threshold}_meta.json")

    def _update_etf_meta_data(self, etf_code, threshold, df):
        """
        æ›´æ–°ETFå…ƒæ•°æ® - ä¸OBVæŒ‡æ ‡æ ¼å¼ç»Ÿä¸€
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            df: æ•°æ®DataFrame
        """
        try:
            meta_file_path = self._get_etf_meta_file_path(etf_code, threshold)
            clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
            
            # åŠ è½½ç°æœ‰é˜ˆå€¼çº§åˆ«æ•°æ®
            threshold_meta = {}
            if os.path.exists(meta_file_path):
                try:
                    with open(meta_file_path, 'r', encoding='utf-8') as f:
                        threshold_meta = json.load(f)
                except:
                    threshold_meta = {}
            
            # è®¡ç®—æ•°æ®å“ˆå¸Œå€¼
            data_hash = self._calculate_data_hash(df)
            
            # æ›´æ–°ç‰¹å®šETFçš„å…ƒæ•°æ® - ä½¿ç”¨OBVæ ¼å¼
            threshold_meta[clean_code] = {
                "last_updated": datetime.now().isoformat(),
                "data_count": len(df),
                "threshold": threshold,
                "cache_file": f"{clean_code}.csv",
                "last_date": df['date'].max() if 'date' in df.columns and not df.empty else "",
                "data_hash": data_hash
            }
            
            # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
            with open(meta_file_path, 'w', encoding='utf-8') as f:
                json.dump(threshold_meta, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ å…ƒæ•°æ®æ›´æ–°å¤±è´¥: {etf_code} - {str(e)}")

    def _load_stats(self) -> dict:
        """ä»æ–‡ä»¶åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        default_stats = {
            'hits': 0,
            'misses': 0,
            'updates': 0,
            'cleanups': 0,
            'errors': 0,
            'hit_count': 0,
            'miss_count': 0,
            'total_requests': 0,
            'last_cleanup': None
        }
        
        try:
            if self._stats_file.exists():
                with open(self._stats_file, 'r', encoding='utf-8') as f:
                    loaded_stats = json.load(f)
                    # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
                    for key in default_stats:
                        if key not in loaded_stats:
                            loaded_stats[key] = 0 if key != 'last_cleanup' else None
                    return loaded_stats
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}")
        
        return default_stats
    
    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        try:
            with open(self._stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def _calculate_data_hash(self, data: pd.DataFrame) -> str:
        """
        è®¡ç®—æ•°æ®å“ˆå¸Œå€¼ - ä¸OBVæŒ‡æ ‡ç»Ÿä¸€
        
        Args:
            data: æ•°æ®DataFrame
            
        Returns:
            æ•°æ®å“ˆå¸Œå€¼
        """
        try:
            # ä½¿ç”¨æ ¸å¿ƒæ•°æ®åˆ—è®¡ç®—å“ˆå¸Œ
            core_columns = ['code', 'date', 'wr_9', 'wr_14', 'wr_21']
            available_columns = [col for col in core_columns if col in data.columns]
            
            if not available_columns:
                return ""
            
            # æ’åºåè®¡ç®—å“ˆå¸Œ
            sorted_data = data[available_columns].sort_values(available_columns[:2] if len(available_columns) >= 2 else available_columns)
            data_str = sorted_data.to_string(index=False)
            
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()[:16]
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æ•°æ®å“ˆå¸Œå¤±è´¥: {str(e)}")
            return ""

    def cleanup_old_cache(self, days_old=30):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜æ–‡ä»¶
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0
            
            for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                cache_dir = os.path.join(self.cache_base_path, threshold)
                if not os.path.exists(cache_dir):
                    continue
                
                for filename in os.listdir(cache_dir):
                    if not filename.endswith('.csv'):
                        continue
                    
                    file_path = os.path.join(cache_dir, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime < cutoff_date:
                        os.remove(file_path)
                        cleaned_count += 1
            
            self.cache_stats['last_cleanup'] = datetime.now().isoformat()
            print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: æ¸…ç†äº†{cleaned_count}ä¸ªè¿‡æœŸæ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")

    def get_cache_summary(self):
        """
        è·å–ç¼“å­˜ç³»ç»Ÿæ‘˜è¦
        
        Returns:
            dict: ç¼“å­˜ç³»ç»Ÿæ‘˜è¦ä¿¡æ¯
        """
        try:
            hit_rate = (self.cache_stats['hit_count'] / self.cache_stats['total_requests'] 
                       if self.cache_stats['total_requests'] > 0 else 0)
            
            summary = {
                'performance': {
                    'hit_rate_percent': round(hit_rate * 100, 2),
                    'total_requests': self.cache_stats['total_requests'],
                    'cache_hits': self.cache_stats['hit_count'],
                    'cache_misses': self.cache_stats['miss_count']
                },
                'storage': {
                    'cache_base_path': self.cache_base_path,
                    'meta_path': self.meta_path
                },
                'config': {
                    'time_tolerance_seconds': self.time_tolerance,
                    'adj_type': self.config.adj_type
                }
            }
            
            return summary
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {}

    def print_cache_status(self):
        """æ‰“å°ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        summary = self.get_cache_summary()
        if summary:
            print("=" * 60)
            print("ğŸ’¾ å¨å»‰æŒ‡æ ‡ç¼“å­˜ç³»ç»ŸçŠ¶æ€")
            print("=" * 60)
            print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {summary['performance']['hit_rate_percent']}%")
            print(f"ğŸ“Š æ€»è¯·æ±‚æ•°: {summary['performance']['total_requests']}")
            print(f"âœ… ç¼“å­˜å‘½ä¸­: {summary['performance']['cache_hits']}")
            print(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {summary['performance']['cache_misses']}")
            print(f"ğŸ“ ç¼“å­˜è·¯å¾„: {summary['storage']['cache_base_path']}")
            print(f"â° æ—¶é—´å®¹å·®: {summary['config']['time_tolerance_seconds']}ç§’")
            print("=" * 60)


if __name__ == "__main__":
    # ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•
    print("ğŸ§ª å¨å»‰æŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•")
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„é…ç½®å¯¹è±¡æ¥å®Œæ•´æµ‹è¯•
    # è¿™ä¸ªæµ‹è¯•ä¸»è¦ç”¨äºéªŒè¯ä»£ç è¯­æ³•å’ŒåŸºæœ¬é€»è¾‘
    print("âœ… ç¼“å­˜ç®¡ç†å™¨æ¨¡å—åŠ è½½æˆåŠŸ")
    print("ğŸ“ éœ€è¦é…ç½®å¯¹è±¡æ¥è¿›è¡Œå®Œæ•´åŠŸèƒ½æµ‹è¯•")