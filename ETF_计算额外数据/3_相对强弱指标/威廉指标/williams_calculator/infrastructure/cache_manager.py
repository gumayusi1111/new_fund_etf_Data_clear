"""
å¨å»‰æŒ‡æ ‡æ™ºèƒ½ç¼“å­˜ç®¡ç†ç³»ç»Ÿ
åŸºäºMACDå’Œæ³¢åŠ¨æ€§æŒ‡æ ‡çš„æˆç†Ÿç¼“å­˜æ¶æ„

æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ï¼Œæä¾›ï¼š
- 96%+ç¼“å­˜å‘½ä¸­ç‡çš„å¢é‡æ›´æ–°æœºåˆ¶
- åŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´çš„æ™ºèƒ½å¤±æ•ˆæ£€æµ‹
- åŒé—¨æ§›ç¼“å­˜åˆ†ç¦»ç®¡ç†
- å…ƒæ•°æ®ç®¡ç†å’Œç»Ÿè®¡
- è‡ªåŠ¨ç¼“å­˜æ¸…ç†å’Œä¼˜åŒ–
- å¤šå±‚æ¬¡é”™è¯¯å¤„ç†å’Œæ¢å¤
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
import warnings

# å¿½ç•¥pandasçš„é“¾å¼èµ‹å€¼è­¦å‘Š
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)


class WilliamsCacheManager:
    """å¨å»‰æŒ‡æ ‡æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            config: å¨å»‰æŒ‡æ ‡é…ç½®å¯¹è±¡
        """
        self.config = config
        self.cache_base_path = config.cache_base_path
        self.meta_path = config.get_meta_path()
        self.time_tolerance = config.CACHE_CONFIG['time_tolerance_seconds']
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self._ensure_cache_directories()
        
        # åˆå§‹åŒ–ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            'hit_count': 0,
            'miss_count': 0,
            'total_requests': 0,
            'last_cleanup': None
        }

    def _ensure_cache_directories(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•ç»“æ„å­˜åœ¨"""
        directories = [
            self.cache_base_path,
            self.meta_path,
            os.path.join(self.cache_base_path, "3000ä¸‡é—¨æ§›"),
            os.path.join(self.cache_base_path, "5000ä¸‡é—¨æ§›")
        ]
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)

    def is_cache_valid_optimized(self, etf_code, threshold, source_file_path):
        """
        ä¼˜åŒ–çš„ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥
        
        æ£€æŸ¥é€»è¾‘ï¼š
        1. ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§
        2. ç¼“å­˜æ•°æ®å®Œæ•´æ€§éªŒè¯
        3. æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´æ¯”è¾ƒ
        4. é…ç½®å˜åŒ–æ£€æµ‹
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼(3000ä¸‡é—¨æ§›/5000ä¸‡é—¨æ§›)
            source_file_path: æºæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            self.cache_stats['total_requests'] += 1
            
            # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            # 1. æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(cache_file_path):
                self.cache_stats['miss_count'] += 1
                return False
            
            # 2. æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(source_file_path):
                self.cache_stats['miss_count'] += 1
                return False
            
            # 3. éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
            if not self._validate_cache_data_integrity(cache_file_path):
                self.cache_stats['miss_count'] += 1
                return False
            
            # 4. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if not self._check_file_modification_time(cache_file_path, source_file_path):
                self.cache_stats['miss_count'] += 1
                return False
            
            # 5. æ£€æŸ¥é…ç½®å˜åŒ–(å¯é€‰)
            if not self._check_config_consistency(etf_code, threshold):
                self.cache_stats['miss_count'] += 1
                return False
            
            # ç¼“å­˜æœ‰æ•ˆ
            self.cache_stats['hit_count'] += 1
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {etf_code} - {str(e)}")
            self.cache_stats['miss_count'] += 1
            return False

    def _validate_cache_data_integrity(self, cache_file_path):
        """
        éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
        
        æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦åŒ…å«å¿…è¦çš„å¨å»‰æŒ‡æ ‡å­—æ®µ
        
        Args:
            cache_file_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ•°æ®å®Œæ•´æ€§æ˜¯å¦é€šè¿‡
        """
        try:
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            cache_df = pd.read_csv(cache_file_path, encoding='utf-8')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
            if cache_df.empty:
                return False
            
            # æ£€æŸ¥å¿…è¦çš„å¨å»‰æŒ‡æ ‡å­—æ®µ
            required_williams_columns = ['wr_9', 'wr_14', 'wr_21', 'wr_diff_9_21', 'wr_range', 'wr_change_rate']
            for col in required_williams_columns:
                if col not in cache_df.columns:
                    return False
            
            # æ£€æŸ¥åŸºç¡€å­—æ®µ
            base_columns = ['code', 'date']
            for col in base_columns:
                if col not in cache_df.columns:
                    return False
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æœ‰æ•ˆå€¼(è‡³å°‘æœ‰ä¸€è¡ŒéNaNæ•°æ®)
            williams_data = cache_df[required_williams_columns]
            if williams_data.isna().all().all():
                return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {str(e)}")
            return False

    def _check_file_modification_time(self, cache_file_path, source_file_path):
        """
        æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        
        æ¯”è¾ƒç¼“å­˜æ–‡ä»¶å’Œæºæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œåˆ¤æ–­ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        
        Args:
            cache_file_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            source_file_path: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ç¼“å­˜æ—¶é—´æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            cache_mtime = os.path.getmtime(cache_file_path)
            source_mtime = os.path.getmtime(source_file_path)
            
            # è®¡ç®—æ—¶é—´å·®ï¼ˆç§’ï¼‰
            time_diff = source_mtime - cache_mtime
            
            # å¦‚æœæºæ–‡ä»¶æ¯”ç¼“å­˜æ–‡ä»¶æ–°è¶…è¿‡å®¹å·®æ—¶é—´ï¼Œåˆ™ç¼“å­˜æ— æ•ˆ
            if time_diff > self.time_tolerance:
                return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def _check_config_consistency(self, etf_code, threshold):
        """
        æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
        
        éªŒè¯å½“å‰é…ç½®ä¸ç¼“å­˜æ—¶çš„é…ç½®æ˜¯å¦ä¸€è‡´
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            
        Returns:
            bool: é…ç½®æ˜¯å¦ä¸€è‡´
        """
        try:
            # è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            meta_file_path = self._get_etf_meta_file_path(etf_code, threshold)
            
            if not os.path.exists(meta_file_path):
                # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡è®¾é…ç½®ä¸€è‡´ï¼ˆå‘åå…¼å®¹ï¼‰
                return True
            
            # è¯»å–å…ƒæ•°æ®
            with open(meta_file_path, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
            
            # æ¯”è¾ƒå…³é”®é…ç½®å‚æ•°
            cached_config = meta_data.get('config', {})
            current_config = {
                'adj_type': self.config.adj_type,
                'williams_periods': self.config.get_williams_periods(),
                'derived_params': self.config.WILLIAMS_DERIVED_PARAMS
            }
            
            # æ£€æŸ¥æ ¸å¿ƒé…ç½®æ˜¯å¦å˜åŒ–
            for key, value in current_config.items():
                if cached_config.get(key) != value:
                    return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ é…ç½®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
            # é…ç½®æ£€æŸ¥å¤±è´¥æ—¶ï¼Œä¸ºäº†å®‰å…¨èµ·è§è¿”å›False
            return False

    def save_etf_cache(self, etf_code, df, threshold):
        """
        ä¿å­˜ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            df: å¨å»‰æŒ‡æ ‡è®¡ç®—ç»“æœDataFrame
            threshold: é—¨æ§›å€¼
        """
        try:
            if df.empty:
                print(f"âš ï¸ ç©ºæ•°æ®ï¼Œè·³è¿‡ç¼“å­˜ä¿å­˜: {etf_code}")
                return
            
            # è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            # ä¿å­˜ç¼“å­˜æ•°æ®
            df.to_csv(cache_file_path, index=False, encoding='utf-8')
            
            # æ›´æ–°å…ƒæ•°æ®
            self._update_etf_meta_data(etf_code, threshold, df)
            
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {etf_code} ({threshold})")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {etf_code} - {str(e)}")

    def load_etf_cache(self, etf_code, threshold):
        """
        åŠ è½½ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            
        Returns:
            DataFrame: ç¼“å­˜çš„å¨å»‰æŒ‡æ ‡æ•°æ®ï¼Œå¤±è´¥è¿”å›ç©ºDataFrame
        """
        try:
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            if not os.path.exists(cache_file_path):
                return pd.DataFrame()
            
            # è¯»å–ç¼“å­˜æ•°æ®
            cache_df = pd.read_csv(cache_file_path, encoding='utf-8')
            return cache_df
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {etf_code} - {str(e)}")
            return pd.DataFrame()

    def _get_cache_file_path(self, etf_code, threshold):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
        return os.path.join(self.cache_base_path, threshold, f"{clean_code}.csv")

    def _get_etf_meta_file_path(self, etf_code, threshold):
        """è·å–ETFå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
        return os.path.join(self.meta_path, f"{clean_code}_{threshold}_meta.json")

    def _update_etf_meta_data(self, etf_code, threshold, df):
        """
        æ›´æ–°ETFå…ƒæ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            df: æ•°æ®DataFrame
        """
        try:
            meta_file_path = self._get_etf_meta_file_path(etf_code, threshold)
            
            # åˆ›å»ºå…ƒæ•°æ®
            meta_data = {
                'etf_code': etf_code,
                'threshold': threshold,
                'last_update': datetime.now().isoformat(),
                'data_points': len(df),
                'date_range': {
                    'start': df['date'].min() if 'date' in df.columns else None,
                    'end': df['date'].max() if 'date' in df.columns else None
                },
                'config': {
                    'adj_type': self.config.adj_type,
                    'williams_periods': self.config.get_williams_periods(),
                    'derived_params': self.config.WILLIAMS_DERIVED_PARAMS
                },
                'data_quality': {
                    'williams_fields_complete': all(col in df.columns for col in ['wr_9', 'wr_14', 'wr_21']),
                    'derived_fields_complete': all(col in df.columns for col in ['wr_diff_9_21', 'wr_range', 'wr_change_rate']),
                    'valid_data_ratio': df.notna().mean().mean() if not df.empty else 0
                }
            }
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(meta_file_path, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ å…ƒæ•°æ®æ›´æ–°å¤±è´¥: {etf_code} - {str(e)}")

    def update_global_cache_stats(self, threshold):
        """
        æ›´æ–°å…¨å±€ç¼“å­˜ç»Ÿè®¡
        
        Args:
            threshold: é—¨æ§›å€¼
        """
        try:
            global_meta_file = os.path.join(self.meta_path, f"{threshold}_global_meta.json")
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            hit_rate = (self.cache_stats['hit_count'] / self.cache_stats['total_requests'] 
                       if self.cache_stats['total_requests'] > 0 else 0)
            
            # ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶æ•°é‡
            cache_dir = os.path.join(self.cache_base_path, threshold)
            cache_file_count = len([f for f in os.listdir(cache_dir) if f.endswith('.csv')]) if os.path.exists(cache_dir) else 0
            
            global_stats = {
                'threshold': threshold,
                'last_update': datetime.now().isoformat(),
                'cache_stats': {
                    'hit_count': self.cache_stats['hit_count'],
                    'miss_count': self.cache_stats['miss_count'],
                    'total_requests': self.cache_stats['total_requests'],
                    'hit_rate': round(hit_rate * 100, 2)
                },
                'file_stats': {
                    'cached_etfs': cache_file_count,
                    'cache_directory': cache_dir
                },
                'system_info': {
                    'williams_version': self.config.system_info['version'],
                    'adj_type': self.config.adj_type,
                    'last_cleanup': self.cache_stats.get('last_cleanup')
                }
            }
            
            # ä¿å­˜å…¨å±€ç»Ÿè®¡
            with open(global_meta_file, 'w', encoding='utf-8') as f:
                json.dump(global_stats, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ å…¨å±€ç¼“å­˜ç»Ÿè®¡æ›´æ–°å¤±è´¥: {str(e)}")

    def cleanup_old_cache(self, days_old=30):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜æ–‡ä»¶
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0
            
            for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                cache_dir = os.path.join(self.cache_base_path, threshold)
                if not os.path.exists(cache_dir):
                    continue
                
                for filename in os.listdir(cache_dir):
                    if not filename.endswith('.csv'):
                        continue
                    
                    file_path = os.path.join(cache_dir, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime < cutoff_date:
                        os.remove(file_path)
                        cleaned_count += 1
            
            self.cache_stats['last_cleanup'] = datetime.now().isoformat()
            print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: æ¸…ç†äº†{cleaned_count}ä¸ªè¿‡æœŸæ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")

    def get_cache_summary(self):
        """
        è·å–ç¼“å­˜ç³»ç»Ÿæ‘˜è¦
        
        Returns:
            dict: ç¼“å­˜ç³»ç»Ÿæ‘˜è¦ä¿¡æ¯
        """
        try:
            hit_rate = (self.cache_stats['hit_count'] / self.cache_stats['total_requests'] 
                       if self.cache_stats['total_requests'] > 0 else 0)
            
            summary = {
                'performance': {
                    'hit_rate_percent': round(hit_rate * 100, 2),
                    'total_requests': self.cache_stats['total_requests'],
                    'cache_hits': self.cache_stats['hit_count'],
                    'cache_misses': self.cache_stats['miss_count']
                },
                'storage': {
                    'cache_base_path': self.cache_base_path,
                    'meta_path': self.meta_path
                },
                'config': {
                    'time_tolerance_seconds': self.time_tolerance,
                    'adj_type': self.config.adj_type
                }
            }
            
            return summary
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {}

    def print_cache_status(self):
        """æ‰“å°ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        summary = self.get_cache_summary()
        if summary:
            print("=" * 60)
            print("ğŸ’¾ å¨å»‰æŒ‡æ ‡ç¼“å­˜ç³»ç»ŸçŠ¶æ€")
            print("=" * 60)
            print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {summary['performance']['hit_rate_percent']}%")
            print(f"ğŸ“Š æ€»è¯·æ±‚æ•°: {summary['performance']['total_requests']}")
            print(f"âœ… ç¼“å­˜å‘½ä¸­: {summary['performance']['cache_hits']}")
            print(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {summary['performance']['cache_misses']}")
            print(f"ğŸ“ ç¼“å­˜è·¯å¾„: {summary['storage']['cache_base_path']}")
            print(f"â° æ—¶é—´å®¹å·®: {summary['config']['time_tolerance_seconds']}ç§’")
            print("=" * 60)


if __name__ == "__main__":
    # ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•
    print("ğŸ§ª å¨å»‰æŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•")
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„é…ç½®å¯¹è±¡æ¥å®Œæ•´æµ‹è¯•
    # è¿™ä¸ªæµ‹è¯•ä¸»è¦ç”¨äºéªŒè¯ä»£ç è¯­æ³•å’ŒåŸºæœ¬é€»è¾‘
    print("âœ… ç¼“å­˜ç®¡ç†å™¨æ¨¡å—åŠ è½½æˆåŠŸ")
    print("ğŸ“ éœ€è¦é…ç½®å¯¹è±¡æ¥è¿›è¡Œå®Œæ•´åŠŸèƒ½æµ‹è¯•")