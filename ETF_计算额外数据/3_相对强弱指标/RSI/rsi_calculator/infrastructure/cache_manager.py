"""
RSIæŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨
åŸºäºå¨å»‰æŒ‡æ ‡çš„ç¼“å­˜ç®¡ç†æ¶æ„

åŠŸèƒ½ç‰¹æ€§ï¼š
1. æ™ºèƒ½ç¼“å­˜éªŒè¯å’Œç®¡ç†
2. æ”¯æŒå¢é‡æ›´æ–°çš„ç¼“å­˜ç­–ç•¥
3. å¤šé—¨æ§›ç¼“å­˜ç»„ç»‡ç»“æ„
4. è‡ªåŠ¨ç¼“å­˜æ¸…ç†å’Œä¼˜åŒ–
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
import traceback


class RSICacheManager:
    """RSIæŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–RSIç¼“å­˜ç®¡ç†å™¨
        
        Args:
            config: RSIé…ç½®å¯¹è±¡
        """
        self.config = config
        self.cache_base_path = config.cache_base_path
        self.meta_path = config.get_meta_path()
        
        # ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'cache_saves': 0,
            'cache_validation_errors': 0
        }
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self._ensure_cache_directories()
        
        print("âœ… RSIç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ’¾ ç¼“å­˜è·¯å¾„: {self.cache_base_path}")

    def _ensure_cache_directories(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•ç»“æ„å­˜åœ¨"""
        try:
            directories = [
                self.cache_base_path,
                self.meta_path,
                self.config.get_cache_path("3000ä¸‡é—¨æ§›"),
                self.config.get_cache_path("5000ä¸‡é—¨æ§›")
            ]
            
            for directory in directories:
                if not os.path.exists(directory):
                    os.makedirs(directory, exist_ok=True)
                    print(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {directory}")
                    
        except Exception as e:
            print(f"âŒ åˆ›å»ºç¼“å­˜ç›®å½•å¤±è´¥: {str(e)}")

    def is_cache_valid_optimized(self, etf_code, threshold, source_file_path):
        """
        ä¼˜åŒ–çš„ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            source_file_path: æºæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            if not os.path.exists(cache_file_path):
                self.cache_stats['cache_misses'] += 1
                return False
            
            # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(source_file_path):
                print(f"âš ï¸ æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file_path}")
                self.cache_stats['cache_misses'] += 1
                return False
            
            # æ¯”è¾ƒæ–‡ä»¶ä¿®æ”¹æ—¶é—´
            cache_mtime = os.path.getmtime(cache_file_path)
            source_mtime = os.path.getmtime(source_file_path)
            
            # å¦‚æœæºæ–‡ä»¶æ›´æ–°ï¼Œç¼“å­˜æ— æ•ˆ
            if source_mtime > cache_mtime:
                print(f"ğŸ“ æºæ–‡ä»¶å·²æ›´æ–°ï¼Œç¼“å­˜æ— æ•ˆ: {etf_code}")
                self.cache_stats['cache_misses'] += 1
                return False
            
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å®Œæ•´æ€§
            try:
                cache_df = pd.read_csv(cache_file_path)
                if cache_df.empty:
                    print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸ºç©º: {etf_code}")
                    self.cache_stats['cache_misses'] += 1
                    return False
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['code', 'date', 'rsi_6', 'rsi_12', 'rsi_24']
                missing_fields = [field for field in required_fields if field not in cache_df.columns]
                
                if missing_fields:
                    print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ç¼ºå°‘å­—æ®µ: {etf_code} - {missing_fields}")
                    self.cache_stats['cache_misses'] += 1
                    return False
                
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {etf_code} - {str(e)}")
                self.cache_stats['cache_misses'] += 1
                return False
            
            self.cache_stats['cache_hits'] += 1
            return True
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜éªŒè¯å¤±è´¥: {etf_code} - {str(e)}")
            self.cache_stats['cache_validation_errors'] += 1
            return False

    def load_etf_cache(self, etf_code, threshold):
        """
        åŠ è½½ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            
        Returns:
            DataFrame: ç¼“å­˜çš„RSIæ•°æ®
        """
        try:
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            if not os.path.exists(cache_file_path):
                print(f"ğŸ“ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {etf_code}")
                return pd.DataFrame()
            
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            cache_df = pd.read_csv(cache_file_path)
            
            if cache_df.empty:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸ºç©º: {etf_code}")
                return pd.DataFrame()
            
            print(f"ğŸ’¾ æˆåŠŸåŠ è½½ç¼“å­˜: {etf_code} ({len(cache_df)}è¡Œæ•°æ®)")
            return cache_df
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {etf_code} - {str(e)}")
            return pd.DataFrame()

    def save_etf_cache(self, etf_code, rsi_data, threshold):
        """
        ä¿å­˜ETFç¼“å­˜æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            rsi_data: RSIè®¡ç®—ç»“æœ
            threshold: é—¨æ§›å€¼
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if rsi_data is None or rsi_data.empty:
                print(f"âš ï¸ RSIæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç¼“å­˜ä¿å­˜: {etf_code}")
                return False
            
            # è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file_path = self._get_cache_file_path(etf_code, threshold)
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            cache_dir = os.path.dirname(cache_file_path)
            if not os.path.exists(cache_dir):
                os.makedirs(cache_dir, exist_ok=True)
            
            # ä¿å­˜ç¼“å­˜æ–‡ä»¶
            rsi_data.to_csv(cache_file_path, index=False, encoding='utf-8')
            
            # æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
            self._update_cache_metadata(etf_code, threshold, len(rsi_data))
            
            self.cache_stats['cache_saves'] += 1
            print(f"ğŸ’¾ ç¼“å­˜ä¿å­˜æˆåŠŸ: {etf_code} ({len(rsi_data)}è¡Œæ•°æ®)")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {etf_code} - {str(e)}")
            print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def _get_cache_file_path(self, etf_code, threshold):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_dir = self.config.get_cache_path(threshold)
        return os.path.join(cache_dir, f"{etf_code}.csv")

    def _update_cache_metadata(self, etf_code, threshold, data_count):
        """
        æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›å€¼
            data_count: æ•°æ®è¡Œæ•°
        """
        try:
            meta_file = os.path.join(self.meta_path, f"{threshold}_meta.json")
            
            # è¯»å–ç°æœ‰å…ƒæ•°æ®
            if os.path.exists(meta_file):
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
            else:
                meta_data = {}
            
            # æ›´æ–°ETFå…ƒæ•°æ®
            meta_data[etf_code] = {
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'data_count': data_count,
                'threshold': threshold,
                'cache_file': f"{etf_code}.csv"
            }
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {etf_code} - {str(e)}")

    def cleanup_invalid_cache(self, days_old=30):
        """
        æ¸…ç†æ— æ•ˆçš„ç¼“å­˜æ–‡ä»¶
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜
            
        Returns:
            dict: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            cleanup_stats = {
                'total_checked': 0,
                'deleted_files': 0,
                'deleted_size_mb': 0,
                'errors': 0
            }
            
            cutoff_time = datetime.now() - timedelta(days=days_old)
            cutoff_timestamp = cutoff_time.timestamp()
            
            # æ£€æŸ¥æ‰€æœ‰é—¨æ§›çš„ç¼“å­˜ç›®å½•
            for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                cache_dir = self.config.get_cache_path(threshold)
                
                if not os.path.exists(cache_dir):
                    continue
                
                # éå†ç¼“å­˜æ–‡ä»¶
                for filename in os.listdir(cache_dir):
                    if not filename.endswith('.csv'):
                        continue
                    
                    file_path = os.path.join(cache_dir, filename)
                    cleanup_stats['total_checked'] += 1
                    
                    try:
                        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                        file_mtime = os.path.getmtime(file_path)
                        
                        if file_mtime < cutoff_timestamp:
                            # è·å–æ–‡ä»¶å¤§å°
                            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                            
                            # åˆ é™¤æ–‡ä»¶
                            os.remove(file_path)
                            
                            cleanup_stats['deleted_files'] += 1
                            cleanup_stats['deleted_size_mb'] += file_size
                            
                            print(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜: {filename}")
                            
                    except Exception as e:
                        print(f"âš ï¸ æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename} - {str(e)}")
                        cleanup_stats['errors'] += 1
            
            # æ¸…ç†å…ƒæ•°æ®ä¸­å·²åˆ é™¤æ–‡ä»¶çš„è®°å½•
            self._cleanup_metadata()
            
            print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ:")
            print(f"   æ£€æŸ¥æ–‡ä»¶: {cleanup_stats['total_checked']}")
            print(f"   åˆ é™¤æ–‡ä»¶: {cleanup_stats['deleted_files']}")
            print(f"   é‡Šæ”¾ç©ºé—´: {cleanup_stats['deleted_size_mb']:.2f}MB")
            print(f"   é”™è¯¯æ•°é‡: {cleanup_stats['errors']}")
            
            return cleanup_stats
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
            return {'error': str(e)}

    def _cleanup_metadata(self):
        """æ¸…ç†å…ƒæ•°æ®ä¸­æ— æ•ˆçš„è®°å½•"""
        try:
            for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                meta_file = os.path.join(self.meta_path, f"{threshold}_meta.json")
                
                if not os.path.exists(meta_file):
                    continue
                
                # è¯»å–å…ƒæ•°æ®
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                
                # æ£€æŸ¥æ¯ä¸ªETFçš„ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                cache_dir = self.config.get_cache_path(threshold)
                etfs_to_remove = []
                
                for etf_code, etf_meta in meta_data.items():
                    cache_file = os.path.join(cache_dir, etf_meta.get('cache_file', f"{etf_code}.csv"))
                    if not os.path.exists(cache_file):
                        etfs_to_remove.append(etf_code)
                
                # ç§»é™¤æ— æ•ˆè®°å½•
                for etf_code in etfs_to_remove:
                    del meta_data[etf_code]
                
                # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
                if etfs_to_remove:
                    with open(meta_file, 'w', encoding='utf-8') as f:
                        json.dump(meta_data, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ§¹ æ¸…ç†å…ƒæ•°æ®è®°å½•: {len(etfs_to_remove)}ä¸ªæ— æ•ˆè®°å½•")
                    
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å…ƒæ•°æ®å¤±è´¥: {str(e)}")

    def update_global_cache_stats(self, threshold):
        """
        æ›´æ–°å…¨å±€ç¼“å­˜ç»Ÿè®¡
        
        Args:
            threshold: é—¨æ§›å€¼
        """
        try:
            global_meta_file = os.path.join(self.meta_path, "cache_global_meta.json")
            
            # è¯»å–ç°æœ‰å…¨å±€å…ƒæ•°æ®
            if os.path.exists(global_meta_file):
                with open(global_meta_file, 'r', encoding='utf-8') as f:
                    global_meta = json.load(f)
            else:
                global_meta = {}
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            global_meta[threshold] = {
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'cache_stats': self.cache_stats.copy(),
                'total_cache_files': self._count_cache_files(threshold)
            }
            
            # ä¿å­˜å…¨å±€å…ƒæ•°æ®
            with open(global_meta_file, 'w', encoding='utf-8') as f:
                json.dump(global_meta, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°å…¨å±€ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")

    def _count_cache_files(self, threshold):
        """ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶æ•°é‡"""
        try:
            cache_dir = self.config.get_cache_path(threshold)
            if not os.path.exists(cache_dir):
                return 0
            
            csv_files = [f for f in os.listdir(cache_dir) if f.endswith('.csv')]
            return len(csv_files)
            
        except Exception as e:
            print(f"âš ï¸ ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return 0

    def get_cache_summary(self):
        """
        è·å–ç¼“å­˜æ‘˜è¦ä¿¡æ¯
        
        Returns:
            dict: ç¼“å­˜æ‘˜è¦
        """
        try:
            summary = {
                'cache_directories': {},
                'performance': {},
                'metadata': {}
            }
            
            # ç»Ÿè®¡å„é—¨æ§›çš„ç¼“å­˜æ–‡ä»¶
            for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                cache_dir = self.config.get_cache_path(threshold)
                file_count = self._count_cache_files(threshold)
                
                summary['cache_directories'][threshold] = {
                    'file_count': file_count,
                    'directory': cache_dir,
                    'exists': os.path.exists(cache_dir)
                }
            
            # æ€§èƒ½ç»Ÿè®¡
            total_operations = self.cache_stats['cache_hits'] + self.cache_stats['cache_misses']
            if total_operations > 0:
                hit_rate = self.cache_stats['cache_hits'] / total_operations * 100
            else:
                hit_rate = 0
            
            summary['performance'] = {
                'hit_rate_percent': round(hit_rate, 2),
                'cache_hits': self.cache_stats['cache_hits'],
                'cache_misses': self.cache_stats['cache_misses'],
                'cache_saves': self.cache_stats['cache_saves'],
                'validation_errors': self.cache_stats['cache_validation_errors']
            }
            
            # å…ƒæ•°æ®ç»Ÿè®¡
            summary['metadata'] = {
                'meta_path': self.meta_path,
                'global_meta_exists': os.path.exists(os.path.join(self.meta_path, "cache_global_meta.json"))
            }
            
            return summary
            
        except Exception as e:
            print(f"âŒ è·å–ç¼“å­˜æ‘˜è¦å¤±è´¥: {str(e)}")
            return {}

    def print_cache_summary(self):
        """æ‰“å°ç¼“å­˜æ‘˜è¦"""
        summary = self.get_cache_summary()
        
        if not summary:
            print("âŒ æ— æ³•è·å–ç¼“å­˜æ‘˜è¦")
            return
        
        print(f"\n{'=' * 60}")
        print("ğŸ’¾ RSIç¼“å­˜ç®¡ç†å™¨æ‘˜è¦")
        print(f"{'=' * 60}")
        
        # ç¼“å­˜ç›®å½•ç»Ÿè®¡
        print("ğŸ“ ç¼“å­˜ç›®å½•:")
        for threshold, info in summary['cache_directories'].items():
            status = "âœ…" if info['exists'] else "âŒ"
            print(f"   {status} {threshold}: {info['file_count']}ä¸ªæ–‡ä»¶")
        
        # æ€§èƒ½ç»Ÿè®¡
        perf = summary['performance']
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {perf['hit_rate_percent']:.2f}%")
        print(f"   ç¼“å­˜å‘½ä¸­: {perf['cache_hits']}æ¬¡")
        print(f"   ç¼“å­˜æœªå‘½ä¸­: {perf['cache_misses']}æ¬¡")
        print(f"   ç¼“å­˜ä¿å­˜: {perf['cache_saves']}æ¬¡")
        print(f"   éªŒè¯é”™è¯¯: {perf['validation_errors']}æ¬¡")
        
        print(f"{'=' * 60}")


if __name__ == "__main__":
    # ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•
    try:
        from config import RSIConfig
        
        print("ğŸ§ª RSIç¼“å­˜ç®¡ç†å™¨æµ‹è¯•")
        config = RSIConfig()
        cache_manager = RSICacheManager(config)
        
        # æ‰“å°ç¼“å­˜æ‘˜è¦
        cache_manager.print_cache_summary()
        
        print("âœ… RSIç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ RSIç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")