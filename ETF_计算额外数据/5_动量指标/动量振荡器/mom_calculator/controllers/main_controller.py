"""
ä¸»æ§åˆ¶å™¨æ¨¡å—
============

åè°ƒå„ä¸ªç»„ä»¶ï¼Œå®ç°åŠ¨é‡æŒ¯è¡å™¨çš„å®Œæ•´ä¸šåŠ¡æµç¨‹
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime

from ..infrastructure.config import MomentumConfig
from ..infrastructure.data_reader import MomentumDataReader
from ..infrastructure.cache_manager import MomentumCacheManager
from ..engines.momentum_engine import MomentumEngine
from ..outputs.csv_handler import MomentumCSVHandler
from ..outputs.display_formatter import MomentumDisplayFormatter

class MomentumController:
    """åŠ¨é‡æŒ¯è¡å™¨ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, base_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ä¸»æ§åˆ¶å™¨
        
        Args:
            base_path: åŸºç¡€è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        """
        # è®¾ç½®åŸºç¡€è·¯å¾„
        if base_path:
            MomentumConfig.BASE_DIR = Path(base_path).parent
            MomentumConfig.init_directories()
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.data_reader = MomentumDataReader()
        self.cache_manager = MomentumCacheManager()
        self.engine = MomentumEngine()
        self.csv_handler = MomentumCSVHandler()
        self.formatter = MomentumDisplayFormatter()
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {"success": 0, "failed": 0, "skipped": 0}
        
        self.logger.info("ğŸš€ åŠ¨é‡æŒ¯è¡å™¨ä¸»æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_config = MomentumConfig.LOGGING_CONFIG
        log_file = MomentumConfig.LOGS_DIR / log_config['file_name']
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        MomentumConfig.LOGS_DIR.mkdir(parents=True, exist_ok=True)
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨é¿å…é‡å¤
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        logging.basicConfig(
            level=getattr(logging, log_config['level']),
            format=log_config['format'],
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8', mode='w'),  # æ¯æ¬¡è¿è¡Œæ¸…ç©ºæ—¥å¿—æ–‡ä»¶
                logging.StreamHandler() if log_config.get('console_output', True) else logging.NullHandler()
            ],
            force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
        )
    
    def process_single_etf(self, etf_code: str, force_recalculate: bool = False, threshold_override: Optional[str] = None) -> bool:
        """
        å¤„ç†å•ä¸ªETFçš„åŠ¨é‡æŒ‡æ ‡è®¡ç®—
        
        Args:
            etf_code: ETFä»£ç 
            force_recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—
            threshold_override: å¼ºåˆ¶æŒ‡å®šé—¨æ§›ç±»å‹
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # è¯»å–åŸå§‹æ•°æ®
            raw_data = self.data_reader.read_etf_data(etf_code)
            if raw_data is None or raw_data.empty:
                self.logger.warning(f"âš ï¸ {etf_code}: æ— æ³•è¯»å–æ•°æ®æˆ–æ•°æ®ä¸ºç©º")
                return False
            
            # åˆ¤æ–­æˆäº¤é¢é—¨æ§›
            threshold_category = threshold_override or self._determine_threshold(raw_data, etf_code)
            if not threshold_category:
                self.logger.info(f"â­ï¸ {etf_code}: æˆäº¤é¢ä¸è¶³é—¨æ§›ï¼Œè·³è¿‡å¤„ç†")
                return False
            
            # æ£€æŸ¥ç¼“å­˜
            if not force_recalculate:
                cached_data = self.cache_manager.load_cache(etf_code, threshold_category)
                if cached_data is not None:
                    self.logger.info(f"ğŸ’¾ {etf_code}: ä½¿ç”¨ç¼“å­˜æ•°æ®")
                    # ä¿å­˜ç¼“å­˜æ•°æ®åˆ°è¾“å‡ºç›®å½•
                    self.csv_handler.save_to_csv(cached_data, etf_code, threshold_category)
                    return True
            
            # è®¡ç®—åŠ¨é‡æŒ‡æ ‡
            calculation_result = self.engine.calculate_momentum_indicators(raw_data, etf_code)
            
            if not calculation_result['success']:
                self.logger.error(f"âŒ {etf_code}: è®¡ç®—å¤±è´¥ - {calculation_result.get('error', 'Unknown')}")
                return False
            
            result_data = calculation_result['data']
            
            # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
            if not self.csv_handler.save_to_csv(result_data, etf_code, threshold_category):
                self.logger.error(f"âŒ {etf_code}: ä¿å­˜å¤±è´¥")
                return False
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache_manager.save_cache(etf_code, threshold_category, result_data)
            
            self.logger.info(f"âœ… {etf_code}: åŠ¨é‡æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œ{len(result_data)}æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {etf_code}: å¤„ç†å¼‚å¸¸ - {str(e)}")
            return False
    
    def batch_process_etfs(self, 
                          source_data_path: Optional[str] = None,
                          etf_codes: Optional[List[str]] = None,
                          use_filtered_list: bool = True,
                          force_recalculate: bool = False,
                          threshold_override: Optional[str] = None) -> Dict[str, int]:
        """
        æ‰¹é‡å¤„ç†ETFåŠ¨é‡æŒ‡æ ‡è®¡ç®—
        
        Args:
            source_data_path: æºæ•°æ®è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
            etf_codes: æŒ‡å®šå¤„ç†çš„ETFä»£ç åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨åˆç­›åˆ—è¡¨
            use_filtered_list: æ˜¯å¦ä½¿ç”¨åˆç­›åˆ—è¡¨
            force_recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—
            threshold_override: å¼ºåˆ¶æŒ‡å®šé—¨æ§›ç±»å‹
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # é‡ç½®ç»Ÿè®¡
            self.stats = {"success": 0, "failed": 0, "skipped": 0}
            
            # ç¡®å®šè¦å¤„ç†çš„ETFåˆ—è¡¨
            target_etfs = self._get_target_etfs(etf_codes, use_filtered_list)
            
            if not target_etfs:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ETFæ–‡ä»¶")
                return self.stats
            
            self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(target_etfs)} ä¸ªETFçš„åŠ¨é‡æŒ‡æ ‡")
            
            # é€ä¸ªå¤„ç†ETF
            for etf_code in target_etfs:
                try:
                    success = self.process_single_etf(etf_code, force_recalculate, threshold_override)
                    
                    if success:
                        self.stats["success"] += 1
                    else:
                        self.stats["skipped"] += 1
                        
                except Exception as e:
                    self.logger.error(f"âŒ {etf_code}: æ‰¹é‡å¤„ç†å¼‚å¸¸ - {str(e)}")
                    self.stats["failed"] += 1
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_processing_metadata(threshold_override)
            
            self.logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ{self.stats['success']}, å¤±è´¥{self.stats['failed']}, è·³è¿‡{self.stats['skipped']}")
            return self.stats
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
            return self.stats
    
    def _determine_threshold(self, data: pd.DataFrame, etf_code: str) -> Optional[str]:
        """ç¡®å®šæˆäº¤é¢é—¨æ§›ç±»åˆ« - æ ¹æ®ETFæ‰€å±çš„åˆç­›åˆ—è¡¨ç¡®å®šé—¨æ§›"""
        # æ£€æŸ¥ETFå±äºå“ªä¸ªé—¨æ§›åˆ—è¡¨
        for threshold in ["5000ä¸‡é—¨æ§›", "3000ä¸‡é—¨æ§›"]:  # ä¼˜å…ˆæ£€æŸ¥5000ä¸‡
            filtered_etfs = self._load_filtered_etf_list(threshold)
            if etf_code in filtered_etfs:
                return threshold
        
        # å¦‚æœéƒ½ä¸åœ¨ï¼Œè¿”å›é»˜è®¤é—¨æ§›
        return "3000ä¸‡é—¨æ§›"
    
    def _get_target_etfs(self, etf_codes: Optional[List[str]], use_filtered_list: bool) -> List[str]:
        """è·å–ç›®æ ‡ETFåˆ—è¡¨"""
        try:
            if etf_codes:
                return etf_codes
            
            if use_filtered_list:
                # åŠ è½½åˆç­›ETFåˆ—è¡¨
                all_etfs = []
                for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                    filtered_etfs = self._load_filtered_etf_list(threshold)
                    all_etfs.extend(filtered_etfs)
                return list(set(all_etfs))  # å»é‡
            else:
                # å‘ç°æ‰€æœ‰å¯ç”¨ETF
                return self.data_reader._discover_available_etfs()
                
        except Exception as e:
            self.logger.error(f"è·å–ç›®æ ‡ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _load_filtered_etf_list(self, threshold_category: str) -> List[str]:
        """åŠ è½½é€šè¿‡åˆç­›çš„ETFåˆ—è¡¨"""
        try:
            filter_file = MomentumConfig.get_filter_file_path(threshold_category)
            
            if not filter_file.exists():
                return []
            
            filtered_etfs = []
            with open(filter_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        etf_code = line.split('.')[0]
                        if etf_code:
                            filtered_etfs.append(etf_code)
            
            return filtered_etfs
            
        except Exception as e:
            self.logger.error(f"åŠ è½½åˆç­›ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _save_processing_metadata(self, target_threshold: Optional[str] = None):
        """ä¿å­˜å¤„ç†å…ƒæ•°æ®"""
        try:
            metadata = {
                "system_info": MomentumConfig.get_system_info(),
                "calculation_params": {
                    "precision": MomentumConfig.MOMENTUM_CONFIG['precision'],
                    "momentum_periods": MomentumConfig.MOMENTUM_CONFIG['momentum_periods'],
                    "roc_periods": MomentumConfig.MOMENTUM_CONFIG['roc_periods'],
                    "pmo_params": MomentumConfig.MOMENTUM_CONFIG['pmo_config'],
                    "williams_period": MomentumConfig.MOMENTUM_CONFIG['williams_period']
                },
                "fields_description": MomentumConfig.FIELD_DESCRIPTIONS,
                "processing_stats": self.stats,
                "last_update": datetime.now().isoformat()
            }
            
            # æ ¹æ®target_thresholdå†³å®šä¿å­˜èŒƒå›´
            if target_threshold:
                # å•é—¨æ§›å¤„ç†ï¼Œåªä¿å­˜æŒ‡å®šé—¨æ§›çš„å…ƒæ•°æ®
                self.csv_handler.batch_save_metadata(metadata, target_threshold)
            else:
                # åŒé—¨æ§›å¤„ç†ï¼Œä¿å­˜åˆ°ä¸¤ä¸ªé—¨æ§›çš„metaç›®å½•
                for threshold in ["3000ä¸‡é—¨æ§›", "5000ä¸‡é—¨æ§›"]:
                    self.csv_handler.batch_save_metadata(metadata, threshold)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤„ç†å…ƒæ•°æ®å¤±è´¥: {str(e)}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            cache_stats = self.cache_manager.get_cache_statistics()
            
            return {
                'system_info': MomentumConfig.get_system_info(),
                'cache_status': cache_stats,
                'last_processing_stats': self.stats,
                'directories': {
                    'data': str(MomentumConfig.DATA_DIR),
                    'cache': str(MomentumConfig.CACHE_DIR),
                    'logs': str(MomentumConfig.LOGS_DIR)
                }
            }
            
        except Exception as e:
            self.logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")
            return {'error': str(e)}