#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³¢åŠ¨ç‡æ‰¹å¤„ç†å™¨
=============

å¤„ç†æ‰¹é‡ETFçš„æ³¢åŠ¨ç‡è®¡ç®—ï¼Œé›†æˆç¼“å­˜ç®¡ç†å’Œå¢é‡æ›´æ–°
"""

import os
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Any
from ..infrastructure.config import VolatilityConfig
from ..infrastructure.cache_manager import VolatilityCacheManager
from .etf_processor import VolatilityETFProcessor


class VolatilityBatchProcessor:
    """æ³¢åŠ¨ç‡æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, etf_processor: VolatilityETFProcessor, 
                 cache_manager: Optional[VolatilityCacheManager],
                 config: VolatilityConfig, enable_cache: bool = True):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨
        
        Args:
            etf_processor: ETFå¤„ç†å™¨
            cache_manager: ç¼“å­˜ç®¡ç†å™¨
            config: é…ç½®å¯¹è±¡
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.etf_processor = etf_processor
        self.cache_manager = cache_manager
        self.config = config
        self.enable_cache = enable_cache
        
        if not config.performance_mode:
            print("ğŸ“Š æ³¢åŠ¨ç‡æ‰¹å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"   ğŸ—‚ï¸ ç¼“å­˜: {'å¯ç”¨' if enable_cache else 'ç¦ç”¨'}")
    
    def process_etf_list(self, etf_codes: List[str], threshold: Optional[str] = None,
                        include_advanced_analysis: bool = False) -> List[Dict]:
        """
        å¤„ç†ETFåˆ—è¡¨
        
        Args:
            etf_codes: ETFä»£ç åˆ—è¡¨
            threshold: é—¨æ§›ç±»å‹
            include_advanced_analysis: æ˜¯å¦åŒ…å«é«˜çº§åˆ†æ
            
        Returns:
            List[Dict]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ“Š å¼€å§‹æ‰¹é‡å¤„ç† {len(etf_codes)} ä¸ªETF...")
        if threshold:
            print(f"   ğŸ¯ é—¨æ§›: {threshold}")
        
        results = []
        cache_hits = 0
        fresh_calculations = 0
        incremental_updates = 0
        
        for i, etf_code in enumerate(etf_codes, 1):
            try:
                if not self.config.performance_mode:
                    print(f"[{i}/{len(etf_codes)}] å¤„ç†: {etf_code}")
                
                # å°è¯•ä»ç¼“å­˜åŠ è½½
                result = None
                if self.enable_cache and self.cache_manager:
                    result = self._try_load_from_cache(etf_code, threshold)
                    if result:
                        result['data_source'] = 'cache'
                        cache_hits += 1
                        if not self.config.performance_mode:
                            print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­")
                
                # å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡Œè®¡ç®—
                if result is None:
                    result = self.etf_processor.process_single_etf(etf_code, include_advanced_analysis)
                    if result:
                        result['data_source'] = 'fresh_calculation'
                        fresh_calculations += 1
                        
                        # ä¿å­˜åˆ°ç¼“å­˜
                        if self.enable_cache and self.cache_manager:
                            self._save_to_cache(result, threshold)
                
                if result:
                    results.append(result)
                    if not self.config.performance_mode:
                        print(f"   âœ… å¤„ç†æˆåŠŸ")
                else:
                    if not self.config.performance_mode:
                        print(f"   âŒ å¤„ç†å¤±è´¥")
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¼‚å¸¸ {etf_code}: {str(e)}")
                continue
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_processed = len(results)
        success_rate = (total_processed / len(etf_codes)) * 100
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ:")
        print(f"   âœ… æˆåŠŸ: {total_processed}/{len(etf_codes)} ({success_rate:.1f}%)")
        print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hits}")
        print(f"   ğŸ”„ å…¨æ–°è®¡ç®—: {fresh_calculations}")
        print(f"   âš¡ å¢é‡æ›´æ–°: {incremental_updates}")
        
        return results
    
    def process_screening_results(self, threshold: str) -> List[Dict]:
        """
        å¤„ç†ç­›é€‰ç»“æœçš„ETFï¼Œæ”¯æŒå¢é‡æ›´æ–°
        
        Args:
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            List[Dict]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ“Š å¤„ç†ç­›é€‰ç»“æœ: {threshold}")
        
        # è·å–ç­›é€‰çš„ETFåˆ—è¡¨
        etf_codes = self.etf_processor.data_reader.get_screening_etf_codes(threshold)
        
        if not etf_codes:
            print(f"   âŒ æœªæ‰¾åˆ° {threshold} çš„ç­›é€‰ç»“æœ")
            return []
        
        print(f"   ğŸ“ˆ æ‰¾åˆ° {len(etf_codes)} ä¸ªETF")
        
        # æ‰¹é‡å¤„ç†ï¼ˆæ”¯æŒç¼“å­˜å’Œå¢é‡æ›´æ–°ï¼‰
        return self.process_etf_list(etf_codes, threshold, include_advanced_analysis=False)
    
    def _try_load_from_cache(self, etf_code: str, threshold: Optional[str] = None) -> Optional[Dict]:
        """
        å°è¯•ä»ç¼“å­˜åŠ è½½ç»“æœ
        
        Args:
            etf_code: ETFä»£ç 
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Dict: ç¼“å­˜ç»“æœæˆ–None
        """
        try:
            if not self.cache_manager:
                return None
            
            # è·å–æºæ–‡ä»¶è·¯å¾„
            source_file_path = self.etf_processor.data_reader.get_etf_file_path(etf_code)
            if not source_file_path:
                return None
            
            # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
            is_valid, meta_data = self.cache_manager.check_cache_validity(
                etf_code, source_file_path, threshold
            )
            
            if not is_valid:
                return None
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            cached_df = self.cache_manager.load_cache(etf_code, threshold)
            if cached_df is None:
                return None
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
            if self._needs_incremental_update(cached_df, source_file_path):
                return self._perform_incremental_update(etf_code, cached_df, threshold)
            
            # ä»ç¼“å­˜å…ƒæ•°æ®æ„å»ºç»“æœ
            if meta_data and 'volatility_values' in meta_data:
                return {
                    'etf_code': etf_code,
                    'volatility_values': meta_data['volatility_values'],
                    'metadata': meta_data.get('metadata', {}),
                    'calculation_date': meta_data.get('cache_created_time'),
                    'data_source': 'cache'
                }
            
            return None
            
        except Exception as e:
            if not self.config.performance_mode:
                print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¼‚å¸¸ {etf_code}: {str(e)}")
            return None
    
    def _needs_incremental_update(self, cached_df: pd.DataFrame, source_file_path: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
        
        Args:
            cached_df: ç¼“å­˜æ•°æ®
            source_file_path: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
        """
        try:
            # è¯»å–æºæ–‡ä»¶çš„æœ€æ–°æ•°æ®
            source_df = pd.read_csv(source_file_path, encoding='utf-8')
            
            if source_df.empty:
                return False
            
            # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼ä¸€è‡´
            if 'date' in cached_df.columns:
                cached_df['date'] = pd.to_datetime(cached_df['date'])
            if 'æ—¥æœŸ' in source_df.columns:
                source_df['æ—¥æœŸ'] = pd.to_datetime(source_df['æ—¥æœŸ'])
            
            # æ¯”è¾ƒæœ€æ–°æ—¥æœŸ
            cached_latest = cached_df['date'].max() if 'date' in cached_df.columns else pd.Timestamp.min
            source_latest = source_df['æ—¥æœŸ'].max() if 'æ—¥æœŸ' in source_df.columns else pd.Timestamp.min
            
            return source_latest > cached_latest
            
        except Exception:
            return False
    
    def _perform_incremental_update(self, etf_code: str, cached_df: pd.DataFrame,
                                   threshold: Optional[str] = None) -> Optional[Dict]:
        """
        æ‰§è¡Œå¢é‡æ›´æ–°
        
        Args:
            etf_code: ETFä»£ç 
            cached_df: ç¼“å­˜æ•°æ®
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Dict: æ›´æ–°åçš„ç»“æœæˆ–None
        """
        try:
            # è¯»å–æ–°æ•°æ®
            data_result = self.etf_processor.data_reader.read_etf_data(etf_code)
            if data_result is None:
                return None
            
            new_df, metadata = data_result
            
            # æ‰§è¡Œå¢é‡æ›´æ–°è®¡ç®—
            from ..engines.historical_calculator import VolatilityHistoricalCalculator
            historical_calculator = VolatilityHistoricalCalculator(self.config)
            
            updated_df = historical_calculator.calculate_incremental_update(
                cached_df, new_df, etf_code
            )
            
            if updated_df is None:
                return None
            
            # è·å–æœ€æ–°çš„æ³¢åŠ¨ç‡å€¼
            latest_volatility_values = self._extract_latest_volatility_values(updated_df)
            
            # æ„å»ºç»“æœ
            result = {
                'etf_code': etf_code,
                'volatility_values': latest_volatility_values,
                'metadata': metadata,
                'calculation_date': datetime.now().isoformat(),
                'data_source': 'incremental_update'
            }
            
            # æ›´æ–°ç¼“å­˜
            if self.cache_manager:
                source_file_path = self.etf_processor.data_reader.get_etf_file_path(etf_code)
                additional_meta = {'volatility_values': latest_volatility_values}
                
                self.cache_manager.save_cache(
                    etf_code, updated_df, source_file_path, threshold, additional_meta
                )
            
            return result
            
        except Exception as e:
            if not self.config.performance_mode:
                print(f"   âŒ å¢é‡æ›´æ–°å¼‚å¸¸ {etf_code}: {str(e)}")
            return None
    
    def _extract_latest_volatility_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ä»å†å²æ•°æ®ä¸­æå–æœ€æ–°çš„æ³¢åŠ¨ç‡å€¼
        
        Args:
            df: å†å²æ•°æ®
            
        Returns:
            Dict: æœ€æ–°æ³¢åŠ¨ç‡å€¼
        """
        try:
            if df.empty:
                return {}
            
            latest_row = df.iloc[0]  # æœ€æ–°æ•°æ®åœ¨ç¬¬ä¸€è¡Œ
            volatility_values = {}
            
            # æå–æ³¢åŠ¨ç‡æŒ‡æ ‡
            volatility_columns = [col for col in df.columns if any(
                keyword in col for keyword in ['Volatility_', 'Rolling_Vol_', 'Price_Range', 'Vol_']
            )]
            
            for col in volatility_columns:
                if col in latest_row and pd.notna(latest_row[col]):
                    volatility_values[col] = float(latest_row[col])
            
            return volatility_values
            
        except Exception as e:
            print(f"   âš ï¸ æå–æ³¢åŠ¨ç‡å€¼å¼‚å¸¸: {str(e)}")
            return {}
    
    def _save_to_cache(self, result: Dict, threshold: Optional[str] = None) -> bool:
        """
        ä¿å­˜ç»“æœåˆ°ç¼“å­˜
        
        Args:
            result: è®¡ç®—ç»“æœ
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            if not self.cache_manager:
                return False
            
            etf_code = result.get('etf_code')
            volatility_values = result.get('volatility_values', {})
            
            # åˆ›å»ºç®€åŒ–çš„ç¼“å­˜æ•°æ®ï¼ˆåªä¿å­˜å¿…è¦ä¿¡æ¯ï¼‰
            cache_data = {
                'etf_code': etf_code,
                'calculation_date': result.get('calculation_date'),
                'volatility_values': volatility_values
            }
            
            cache_df = pd.DataFrame([cache_data])
            
            # è·å–æºæ–‡ä»¶è·¯å¾„
            source_file_path = self.etf_processor.data_reader.get_etf_file_path(etf_code)
            
            # ä¿å­˜ç¼“å­˜
            additional_meta = {
                'volatility_values': volatility_values,
                'metadata': result.get('metadata', {})
            }
            
            return self.cache_manager.save_cache(
                etf_code, cache_df, source_file_path, threshold, additional_meta
            )
            
        except Exception as e:
            if not self.config.performance_mode:
                print(f"   âš ï¸ ç¼“å­˜ä¿å­˜å¼‚å¸¸: {str(e)}")
            return False
    
    def save_results_to_files(self, results: List[Dict], output_dir: str,
                            threshold: Optional[str] = None) -> Dict[str, Any]:
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: ç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            threshold: é—¨æ§›ç±»å‹
            
        Returns:
            Dict: ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not results:
                return {'files_saved': 0, 'error': 'æ— å¯ä¿å­˜çš„ç»“æœ'}
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            if threshold:
                final_output_dir = os.path.join(output_dir, threshold)
            else:
                final_output_dir = output_dir
            
            os.makedirs(final_output_dir, exist_ok=True)
            
            saved_count = 0
            total_size = 0
            
            for result in results:
                etf_code = result.get('etf_code', '')
                clean_code = etf_code.replace('.SH', '').replace('.SZ', '')
                
                output_file = os.path.join(final_output_dir, f"{clean_code}.csv")
                
                # å‡†å¤‡ä¿å­˜æ•°æ®
                save_data = {
                    'ETF_Code': etf_code,
                    'Calculation_Date': result.get('calculation_date', ''),
                    'Data_Source': result.get('data_source', ''),
                    **result.get('volatility_values', {})
                }
                
                # ä¿å­˜æ–‡ä»¶
                save_df = pd.DataFrame([save_data])
                save_df.to_csv(output_file, index=False, encoding='utf-8')
                
                # ç»Ÿè®¡ä¿¡æ¯
                file_size = os.path.getsize(output_file)
                total_size += file_size
                saved_count += 1
            
            return {
                'files_saved': saved_count,
                'total_files': len(results),
                'success_rate': (saved_count / len(results)) * 100,
                'total_size_kb': total_size / 1024,
                'output_directory': final_output_dir
            }
            
        except Exception as e:
            return {'files_saved': 0, 'error': f'ä¿å­˜å¼‚å¸¸: {str(e)}'}