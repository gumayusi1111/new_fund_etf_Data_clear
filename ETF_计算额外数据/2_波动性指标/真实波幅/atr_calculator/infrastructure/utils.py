#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ATRå·¥å…·å‡½æ•°æ¨¡å—
==============

æä¾›ATRè®¡ç®—å™¨çš„é€šç”¨å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®éªŒè¯å·¥å…·
- æ–‡ä»¶æ“ä½œå·¥å…·
- æ•°å­¦è®¡ç®—å·¥å…·
- æ ¼å¼åŒ–å·¥å…·
- æ€§èƒ½ç›‘æ§å·¥å…·
"""

import os
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import logging


def setup_logger(name: str, level: str = "INFO") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(name)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    logger.setLevel(getattr(logging, level.upper()))
    return logger


def timer(func):
    """æ€§èƒ½è®¡æ—¶è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        logger = logging.getLogger(__name__)
        logger.debug(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {end_time - start_time:.4f}ç§’")
        
        return result
    return wrapper


def format_number(value: Union[int, float], decimal_places: int = 2) -> str:
    """æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º"""
    if pd.isna(value) or value is None:
        return "N/A"
    
    if isinstance(value, (int, float)):
        if abs(value) >= 1e9:
            return f"{value/1e9:.{decimal_places}f}B"
        elif abs(value) >= 1e6:
            return f"{value/1e6:.{decimal_places}f}M"
        elif abs(value) >= 1e3:
            return f"{value/1e3:.{decimal_places}f}K"
        else:
            return f"{value:.{decimal_places}f}"
    
    return str(value)


def format_percentage(value: Union[int, float], decimal_places: int = 2) -> str:
    """æ ¼å¼åŒ–ç™¾åˆ†æ¯”æ˜¾ç¤º"""
    if pd.isna(value) or value is None:
        return "N/A"
    
    return f"{value:.{decimal_places}f}%"


def safe_divide(numerator: Union[int, float], denominator: Union[int, float], 
               default_value: float = np.nan) -> float:
    """å®‰å…¨é™¤æ³•ï¼Œé¿å…é™¤é›¶é”™è¯¯"""
    try:
        if pd.isna(numerator) or pd.isna(denominator) or denominator == 0:
            return default_value
        return numerator / denominator
    except (TypeError, ZeroDivisionError):
        return default_value


def calculate_statistics(series: pd.Series) -> Dict[str, Any]:
    """è®¡ç®—åºåˆ—ç»Ÿè®¡ä¿¡æ¯"""
    try:
        clean_series = series.dropna()
        
        if len(clean_series) == 0:
            return {
                'count': 0,
                'mean': np.nan,
                'std': np.nan,
                'min': np.nan,
                'max': np.nan,
                'median': np.nan,
                'q25': np.nan,
                'q75': np.nan
            }
        
        return {
            'count': len(clean_series),
            'mean': clean_series.mean(),
            'std': clean_series.std(),
            'min': clean_series.min(),
            'max': clean_series.max(),
            'median': clean_series.median(),
            'q25': clean_series.quantile(0.25),
            'q75': clean_series.quantile(0.75)
        }
    
    except Exception:
        return {'count': 0, 'mean': np.nan, 'std': np.nan, 'min': np.nan, 
                'max': np.nan, 'median': np.nan, 'q25': np.nan, 'q75': np.nan}


def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> Tuple[bool, str]:
    """éªŒè¯DataFrameæ ¼å¼å’Œå†…å®¹"""
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if df is None or df.empty:
            return False, "DataFrameä¸ºç©º"
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}"
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        for col in required_columns:
            if col == 'æ—¥æœŸ':
                continue  # æ—¥æœŸåˆ—å•ç‹¬å¤„ç†
            
            if not pd.api.types.is_numeric_dtype(df[col]):
                try:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                except:
                    return False, f"åˆ— {col} æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        for col in required_columns:
            if col != 'æ—¥æœŸ' and df[col].isna().all():
                return False, f"åˆ— {col} å…¨éƒ¨ä¸ºç©ºå€¼"
        
        return True, "éªŒè¯é€šè¿‡"
    
    except Exception as e:
        return False, f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"


def clean_etf_code(etf_code: str) -> str:
    """æ¸…ç†å’Œæ ‡å‡†åŒ–ETFä»£ç """
    if not etf_code:
        return ""
    
    # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    etf_code = etf_code.strip().upper()
    
    # å¦‚æœåªæœ‰6ä½æ•°å­—ï¼Œæ·»åŠ äº¤æ˜“æ‰€åç¼€
    if len(etf_code) == 6 and etf_code.isdigit():
        if etf_code.startswith(('50', '51', '52', '56', '58')):
            etf_code += '.SH'
        elif etf_code.startswith(('15', '16', '18')):
            etf_code += '.SZ'
        else:
            etf_code += '.SH'  # é»˜è®¤ä¸Šæµ·
    
    return etf_code


def get_file_size(file_path: Union[str, Path]) -> int:
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
    try:
        return Path(file_path).stat().st_size
    except (OSError, FileNotFoundError):
        return 0


def get_file_modification_time(file_path: Union[str, Path]) -> datetime:
    """è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´"""
    try:
        timestamp = Path(file_path).stat().st_mtime
        return datetime.fromtimestamp(timestamp)
    except (OSError, FileNotFoundError):
        return datetime.min


def ensure_directory_exists(directory: Union[str, Path]) -> bool:
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    try:
        Path(directory).mkdir(parents=True, exist_ok=True)
        return True
    except Exception:
        return False


def calculate_atr_metrics_summary(atr_data: pd.DataFrame) -> Dict[str, Any]:
    """è®¡ç®—ATRæŒ‡æ ‡æ±‡æ€»ç»Ÿè®¡"""
    try:
        summary = {
            'data_overview': {
                'total_records': len(atr_data),
                'date_range': None,
                'valid_atr_records': 0
            },
            'atr_statistics': {},
            'volatility_analysis': {},
            'risk_metrics': {}
        }
        
        # æ•°æ®æ¦‚è§ˆ
        if 'æ—¥æœŸ' in atr_data.columns and not atr_data['æ—¥æœŸ'].isna().all():
            dates = pd.to_datetime(atr_data['æ—¥æœŸ']).dropna()
            if len(dates) > 0:
                summary['data_overview']['date_range'] = {
                    'start': dates.min().strftime('%Y-%m-%d'),
                    'end': dates.max().strftime('%Y-%m-%d'),
                    'trading_days': len(dates)
                }
        
        # ATRç»Ÿè®¡
        atr_fields = ['tr', 'atr_10', 'atr_percent', 'atr_change_rate', 'atr_ratio_hl']
        
        for field in atr_fields:
            if field in atr_data.columns:
                summary['atr_statistics'][field] = calculate_statistics(atr_data[field])
        
        # æ³¢åŠ¨æ€§åˆ†æ
        if 'atr_percent' in atr_data.columns:
            atr_percent = atr_data['atr_percent'].dropna()
            if len(atr_percent) > 0:
                summary['volatility_analysis'] = {
                    'average_volatility': atr_percent.mean(),
                    'volatility_stability': atr_percent.std(),
                    'high_volatility_days': (atr_percent > 3.0).sum(),
                    'low_volatility_days': (atr_percent < 1.5).sum(),
                    'extreme_volatility_days': (atr_percent > 5.0).sum()
                }
        
        # é£é™©æŒ‡æ ‡
        if 'stop_loss' in atr_data.columns and 'æ”¶ç›˜ä»·' in atr_data.columns:
            stop_loss = atr_data['stop_loss'].dropna()
            close_price = atr_data['æ”¶ç›˜ä»·'].dropna()
            
            if len(stop_loss) > 0 and len(close_price) > 0:
                # è®¡ç®—æ­¢æŸè·ç¦»
                stop_distance = ((close_price - stop_loss) / close_price * 100).dropna()
                summary['risk_metrics'] = {
                    'average_stop_distance_pct': stop_distance.mean(),
                    'max_stop_distance_pct': stop_distance.max(),
                    'min_stop_distance_pct': stop_distance.min()
                }
        
        # æ³¢åŠ¨æ€§åˆ†çº§ç»Ÿè®¡
        if 'volatility_level' in atr_data.columns:
            vol_level_counts = atr_data['volatility_level'].value_counts()
            summary['volatility_level_distribution'] = vol_level_counts.to_dict()
        
        return summary
    
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"è®¡ç®—ATRæŒ‡æ ‡æ±‡æ€»å¤±è´¥: {e}")
        return {
            'error': str(e),
            'data_overview': {'total_records': 0},
            'atr_statistics': {},
            'volatility_analysis': {},
            'risk_metrics': {}
        }


def format_processing_results(results: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å¤„ç†ç»“æœä¸ºå¯è¯»æ–‡æœ¬"""
    try:
        output_lines = []
        
        # åŸºæœ¬ä¿¡æ¯
        if 'total_etfs_processed' in results:
            output_lines.append(f"ğŸ“Š æ€»å¤„ç†ETFæ•°é‡: {results['total_etfs_processed']}")
        
        # é—¨æ§›ç»Ÿè®¡
        if 'threshold_statistics' in results:
            for threshold, stats in results['threshold_statistics'].items():
                if stats:
                    success_count = stats.get('successful_etfs', 0)
                    total_count = stats.get('total_etfs', 0)
                    success_rate = stats.get('success_rate', 0)
                    
                    output_lines.append(f"\nğŸ“ˆ {threshold}:")
                    output_lines.append(f"   âœ… æˆåŠŸ: {success_count}/{total_count} ({success_rate:.1f}%)")
                    
                    if 'file_size_mb' in stats:
                        output_lines.append(f"   ğŸ’¾ æ–‡ä»¶å¤§å°: {stats['file_size_mb']:.2f} MB")
        
        # å¤„ç†æ—¶é—´
        if 'processing_time' in results:
            duration = results['processing_time']
            if duration > 60:
                output_lines.append(f"\nâ±ï¸ å¤„ç†æ—¶é—´: {duration/60:.1f}åˆ†é’Ÿ")
            else:
                output_lines.append(f"\nâ±ï¸ å¤„ç†æ—¶é—´: {duration:.1f}ç§’")
        
        return "\n".join(output_lines)
    
    except Exception:
        return "ç»“æœæ ¼å¼åŒ–å¤±è´¥"


def batch_process_with_progress(items: List[Any], process_func, 
                              batch_size: int = 10, 
                              show_progress: bool = True) -> List[Any]:
    """æ‰¹é‡å¤„ç†ï¼Œæ˜¾ç¤ºè¿›åº¦"""
    results = []
    total_items = len(items)
    
    logger = logging.getLogger(__name__)
    
    for i in range(0, total_items, batch_size):
        batch = items[i:i + batch_size]
        batch_results = []
        
        for item in batch:
            try:
                result = process_func(item)
                batch_results.append(result)
            except Exception as e:
                logger.warning(f"å¤„ç†é¡¹ç›®å¤±è´¥ {item}: {e}")
                batch_results.append(None)
        
        results.extend(batch_results)
        
        if show_progress:
            processed = min(i + batch_size, total_items)
            progress = processed / total_items * 100
            logger.info(f"å¤„ç†è¿›åº¦: {processed}/{total_items} ({progress:.1f}%)")
    
    return results


def get_etf_screening_file_path(threshold: str) -> str:
    """è·å–ETFç­›é€‰æ–‡ä»¶è·¯å¾„"""
    # æ ¹æ®é¡¹ç›®ç»“æ„å®šä½ç­›é€‰æ–‡ä»¶
    current_dir = os.getcwd()
    
    if "ETF_è®¡ç®—é¢å¤–æ•°æ®" in current_dir:
        project_root = current_dir.split("ETF_è®¡ç®—é¢å¤–æ•°æ®")[0]
    else:
        project_root = current_dir
    
    screening_file = os.path.join(
        project_root, "ETF_åˆç­›", "data", threshold, "é€šè¿‡ç­›é€‰ETF.txt"
    )
    
    return screening_file


def read_etf_screening_list(threshold: str) -> List[str]:
    """è¯»å–ETFç­›é€‰åˆ—è¡¨"""
    screening_file = get_etf_screening_file_path(threshold)
    
    if not os.path.exists(screening_file):
        return []
    
    try:
        with open(screening_file, 'r', encoding='utf-8') as f:
            etf_list = [line.strip() for line in f.readlines() 
                       if line.strip() and not line.strip().startswith('#')]
        return etf_list
    except Exception:
        return []