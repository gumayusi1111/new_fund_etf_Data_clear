#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFå‘¨æ›´æ•°æ®å¯¼å…¥å™¨
ä¸“é—¨å¤„ç†å‘¨æ›´æ–°æ•°æ®çš„æ•°æ®åº“å¯¼å…¥åŠŸèƒ½
"""

import os
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥db_connection
sys.path.insert(0, str(Path(__file__).parent.parent))
from db_connection import ETFDatabaseManager

# å¯¼å…¥hashç®¡ç†å™¨
from config.hash_manager import HashManager

class WeeklyDataImporter:
    """ETFå‘¨æ›´æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯¼å…¥å™¨"""
        self.db_manager = ETFDatabaseManager()
        self.schema_map = {
            'å‰å¤æƒ': 'basic_info_weekly',
            'åå¤æƒ': 'basic_info_weekly', 
            'é™¤æƒ': 'basic_info_weekly'
        }
        self.table_map = {
            'å‰å¤æƒ': 'forward_adjusted',
            'åå¤æƒ': 'backward_adjusted',
            'é™¤æƒ': 'ex_rights'
        }
        # åˆå§‹åŒ–hashç®¡ç†å™¨
        self.hash_manager = HashManager()
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        return self.db_manager.connect()
    
    def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        self.db_manager.disconnect()
    
    @property 
    def cursor(self):
        """è·å–æ•°æ®åº“æ¸¸æ ‡"""
        return self.db_manager.cursor
    
    def import_weekly_directories(self, base_dir: str) -> Dict[str, bool]:
        """å¯¼å…¥å‘¨æ›´æ–°ç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®"""
        if not os.path.exists(base_dir):
            print(f"âŒ åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return {}
        
        print(f"ğŸš€ å¼€å§‹å¯¼å…¥å‘¨æ›´æ•°æ®: {base_dir}")
        
        # å‘¨æ›´æ•°æ®ç›®å½•æ˜ å°„
        directories = {
            'å‰å¤æƒ': '0_ETFæ—¥K(å‰å¤æƒ)',
            'åå¤æƒ': '0_ETFæ—¥K(åå¤æƒ)',
            'é™¤æƒ': '0_ETFæ—¥K(é™¤æƒ)'
        }
        
        results = {}
        
        try:
            if not self.connect():
                return results
            
            # ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
            self._ensure_tables_exist()
            
            for adj_type, dir_name in directories.items():
                dir_path = os.path.join(base_dir, dir_name)
                if os.path.exists(dir_path):
                    print(f"\nğŸ“‚ å¯¼å…¥{adj_type}æ•°æ®: {dir_name}")
                    success = self._import_directory(dir_path, adj_type)
                    results[adj_type] = success
                else:
                    print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {dir_name}")
                    results[adj_type] = False
            
            # æäº¤æ‰€æœ‰æ›´æ”¹
            self.db_manager.connection.commit()
            print("\nğŸ‰ å‘¨æ›´æ•°æ®å¯¼å…¥å®Œæˆ!")
            
            # æ˜¾ç¤ºå¯¼å…¥æ±‡æ€»
            self._show_import_summary()
            
        except Exception as e:
            print(f"âŒ å‘¨æ›´æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            if hasattr(self, 'db_manager') and self.db_manager.connection:
                self.db_manager.connection.rollback()
        finally:
            self.disconnect()
        
        return results
    
    def import_latest_weekly_data(self, base_dir: str, weeks_back: int = 1) -> Dict[str, bool]:
        """åªå¯¼å…¥æœ€è¿‘å‡ å‘¨çš„æ–°æ•°æ®ï¼ˆå¢é‡å¯¼å…¥ï¼‰"""
        print(f"ğŸ”„ æ‰§è¡Œå‘¨æ›´å¢é‡å¯¼å…¥ï¼šæœ€è¿‘{weeks_back}å‘¨çš„æ•°æ®")
        
        if not os.path.exists(base_dir):
            print(f"âŒ åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return {}
        
        directories = {
            'å‰å¤æƒ': '0_ETFæ—¥K(å‰å¤æƒ)',
            'åå¤æƒ': '0_ETFæ—¥K(åå¤æƒ)', 
            'é™¤æƒ': '0_ETFæ—¥K(é™¤æƒ)'
        }
        
        results = {}
        
        try:
            if not self.connect():
                return results
            
            # ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
            self._ensure_tables_exist()
            
            # è·å–éœ€è¦æ›´æ–°çš„æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘å‡ å‘¨ï¼‰
            target_dates = self._get_recent_week_dates(weeks_back)
            print(f"ğŸ“… ç›®æ ‡æ—¥æœŸèŒƒå›´: {len(target_dates)} å¤©")
            
            for adj_type, dir_name in directories.items():
                dir_path = os.path.join(base_dir, dir_name)
                if os.path.exists(dir_path):
                    print(f"\nğŸ“‚ å‘¨æ›´å¢é‡å¯¼å…¥{adj_type}æ•°æ®")
                    success = self._import_recent_data(dir_path, adj_type, target_dates)
                    results[adj_type] = success
                else:
                    print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {dir_name}")
                    results[adj_type] = False
            
            # æäº¤æ›´æ”¹
            self.db_manager.connection.commit()
            print("\nâœ… å‘¨æ›´å¢é‡å¯¼å…¥å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ å‘¨æ›´å¢é‡å¯¼å…¥å¤±è´¥: {e}")
            if hasattr(self, 'db_manager') and self.db_manager.connection:
                self.db_manager.connection.rollback()
        finally:
            self.disconnect()
        
        return results
    
    def _import_recent_data(self, dir_path: str, adj_type: str, target_dates: List[str]) -> bool:
        """å¯¼å…¥æœ€è¿‘æ•°æ®ï¼ˆå¢é‡ï¼‰"""
        try:
            schema = self.schema_map[adj_type]
            table = self.table_map[adj_type]
            table_name = f"{schema}.{table}"
            
            # è·å–æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = list(Path(dir_path).glob("*.csv"))
            print(f"ğŸ“„ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
            
            updated_count = 0
            
            for i, csv_file in enumerate(csv_files, 1):
                if i % 100 == 0:
                    print(f"  ğŸ“ˆ è¿›åº¦: {i}/{len(csv_files)}")
                
                # è¯»å–CSVå¹¶è¿‡æ»¤æœ€è¿‘çš„æ•°æ®
                if self._import_recent_records_from_csv(str(csv_file), table_name, target_dates):
                    updated_count += 1
            
            print(f"âœ… {adj_type}å‘¨æ›´å¢é‡æ›´æ–°: {updated_count}ä¸ªæ–‡ä»¶æœ‰æ›´æ–°")
            return updated_count > 0
            
        except Exception as e:
            print(f"âŒ {adj_type}å‘¨æ›´å¢é‡å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def _ensure_tables_exist(self):
        """ç¡®ä¿å‘¨æ›´æ•°æ®è¡¨å­˜åœ¨"""
        try:
            # åˆ›å»ºschema
            self.cursor.execute("CREATE SCHEMA IF NOT EXISTS basic_info_weekly")
            
            # åˆ›å»ºè¡¨çš„SQLæ¨¡æ¿
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS basic_info_weekly.{table_name} (
                id SERIAL PRIMARY KEY,
                etf_code VARCHAR(10) NOT NULL,
                trade_date DATE NOT NULL,
                open_price NUMERIC,
                high_price NUMERIC,
                low_price NUMERIC,
                close_price NUMERIC,
                volume NUMERIC,
                amount NUMERIC,
                prev_close NUMERIC,
                change_amount NUMERIC,
                change_percent NUMERIC,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(etf_code, trade_date)
            )
            """
            
            # åˆ›å»ºä¸‰ä¸ªå¤æƒè¡¨
            for table_name in ['forward_adjusted', 'backward_adjusted', 'ex_rights']:
                self.cursor.execute(create_table_sql.format(table_name=table_name))
            
            self.db_manager.connection.commit()
            print("âœ… å‘¨æ›´æ•°æ®è¡¨ç»“æ„æ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¡¨ç»“æ„å¤±è´¥: {e}")
            raise e
    
    def _import_directory(self, dir_path: str, adj_type: str) -> bool:
        """å¯¼å…¥æŒ‡å®šç›®å½•çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        try:
            csv_files = list(Path(dir_path).glob("*.csv"))
            print(f"ğŸ“„ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
            
            schema = self.schema_map[adj_type]
            table = self.table_map[adj_type]
            table_name = f"{schema}.{table}"
            
            success_count = 0
            
            for i, csv_file in enumerate(csv_files, 1):
                if i % 100 == 0:
                    print(f"  ğŸ“ˆ è¿›åº¦: {i}/{len(csv_files)}")
                
                if self._import_csv_file(str(csv_file), table_name):
                    success_count += 1
                
                # æ¯50ä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
                if i % 50 == 0:
                    self.db_manager.connection.commit()
            
            print(f"âœ… {adj_type}æ•°æ®å¯¼å…¥å®Œæˆ: {success_count}/{len(csv_files)}")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥{adj_type}æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _import_csv_file(self, csv_file_path: str, table_name: str) -> bool:
        """å¯¼å…¥å•ä¸ªCSVæ–‡ä»¶"""
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8')
            if df.empty:
                return True
            
            insert_sql = f"""
                INSERT INTO {table_name} 
                (etf_code, trade_date, open_price, high_price, low_price, close_price,
                 volume, amount, prev_close, change_amount, change_percent)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (etf_code, trade_date) 
                DO UPDATE SET
                    open_price = EXCLUDED.open_price,
                    high_price = EXCLUDED.high_price,
                    low_price = EXCLUDED.low_price,
                    close_price = EXCLUDED.close_price,
                    volume = EXCLUDED.volume,
                    amount = EXCLUDED.amount,
                    prev_close = EXCLUDED.prev_close,
                    change_amount = EXCLUDED.change_amount,
                    change_percent = EXCLUDED.change_percent,
                    created_at = CURRENT_TIMESTAMP
            """
            
            for _, row in df.iterrows():
                try:
                    trade_date = datetime.strptime(str(row['æ—¥æœŸ']), '%Y%m%d').date()
                    
                    self.cursor.execute(insert_sql, (
                        row['ä»£ç '],
                        trade_date,
                        float(row['å¼€ç›˜ä»·']) if pd.notnull(row['å¼€ç›˜ä»·']) else None,
                        float(row['æœ€é«˜ä»·']) if pd.notnull(row['æœ€é«˜ä»·']) else None,
                        float(row['æœ€ä½ä»·']) if pd.notnull(row['æœ€ä½ä»·']) else None,
                        float(row['æ”¶ç›˜ä»·']) if pd.notnull(row['æ”¶ç›˜ä»·']) else None,
                        float(row['æˆäº¤é‡(æ‰‹æ•°)']) if pd.notnull(row['æˆäº¤é‡(æ‰‹æ•°)']) else None,
                        float(row['æˆäº¤é¢(åƒå…ƒ)']) if pd.notnull(row['æˆäº¤é¢(åƒå…ƒ)']) else None,
                        float(row['ä¸Šæ—¥æ”¶ç›˜']) if pd.notnull(row['ä¸Šæ—¥æ”¶ç›˜']) else None,
                        float(row['æ¶¨è·Œ']) if pd.notnull(row['æ¶¨è·Œ']) else None,
                        float(row['æ¶¨å¹…%']) if pd.notnull(row['æ¶¨å¹…%']) else None
                    ))
                except Exception as e:
                    continue  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
            
            return True
            
        except Exception as e:
            return False
    
    def _import_recent_records_from_csv(self, csv_file_path: str, table_name: str, target_dates: List[str]) -> bool:
        """ä»CSVæ–‡ä»¶å¯¼å…¥æœ€è¿‘çš„è®°å½•"""
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8')
            if df.empty:
                return False
            
            # è¿‡æ»¤æœ€è¿‘çš„æ•°æ®
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str)
            recent_df = df[df['æ—¥æœŸ'].isin(target_dates)]
            
            if recent_df.empty:
                return False
            
            # å¦‚æœæœ‰æ–°æ•°æ®ï¼Œåˆ™é‡æ–°å¯¼å…¥æ•´ä¸ªæ–‡ä»¶
            return self._import_csv_file(csv_file_path, table_name)
            
        except Exception as e:
            return False
    
    def _get_recent_week_dates(self, weeks_back: int) -> List[str]:
        """è·å–æœ€è¿‘å‡ å‘¨çš„æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨"""
        from datetime import datetime, timedelta
        
        dates = []
        today = datetime.now()
        
        # è·å–æœ€è¿‘weeks_backå‘¨çš„æ‰€æœ‰æ—¥æœŸ
        for week in range(weeks_back):
            for day in range(7):
                date = today - timedelta(weeks=week, days=day)
                dates.append(date.strftime('%Y%m%d'))
        
        return dates
    
    def _show_import_summary(self):
        """æ˜¾ç¤ºå¯¼å…¥æ±‡æ€»ä¿¡æ¯"""
        try:
            print(f"\nğŸ“Š å‘¨æ›´æ•°æ®å¯¼å…¥æ±‡æ€»:")
            
            for adj_type, table in self.table_map.items():
                table_name = f"basic_info_weekly.{table}"
                
                # è·å–è®°å½•æ•°
                self.cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                count = self.cursor.fetchone()[0]
                
                # è·å–æœ€æ–°æ—¥æœŸ
                self.cursor.execute(f"SELECT MAX(trade_date) FROM {table_name}")
                latest_date = self.cursor.fetchone()[0]
                
                # è·å–ETFæ•°é‡
                self.cursor.execute(f"SELECT COUNT(DISTINCT etf_code) FROM {table_name}")
                etf_count = self.cursor.fetchone()[0]
                
                print(f"  ğŸ“ˆ {adj_type}: {count:,} æ¡è®°å½•ï¼Œ{etf_count} ä¸ªETFï¼Œæœ€æ–°æ—¥æœŸ: {latest_date}")
                
        except Exception as e:
            print(f"âŒ æ±‡æ€»ä¿¡æ¯è·å–å¤±è´¥: {e}")

    def _execute_batch_insert_weekly(self, table_name: str, batch_data: List[tuple]) -> None:
        """æ‰§è¡Œæ‰¹é‡æ’å…¥ï¼ˆå‘¨æ›´ç‰ˆæœ¬ï¼‰"""
        if not batch_data:
            return
            
        insert_sql = f"""
            INSERT INTO {table_name} 
            (etf_code, trade_date, open_price, high_price, low_price, close_price,
             volume, amount, prev_close, change_amount, change_percent)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (etf_code, trade_date) 
            DO UPDATE SET
                open_price = EXCLUDED.open_price,
                high_price = EXCLUDED.high_price,
                low_price = EXCLUDED.low_price,
                close_price = EXCLUDED.close_price,
                volume = EXCLUDED.volume,
                amount = EXCLUDED.amount,
                prev_close = EXCLUDED.prev_close,
                change_amount = EXCLUDED.change_amount,
                change_percent = EXCLUDED.change_percent,
                updated_at = CURRENT_TIMESTAMP
        """
        
        try:
            # ä½¿ç”¨executemanyè¿›è¡Œæ‰¹é‡æ’å…¥
            self.cursor.executemany(insert_sql, batch_data)
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            raise e

    def _import_recent_data_optimized(self, dir_path: str, adj_type: str, target_dates: List[str]) -> bool:
        """ä¼˜åŒ–ç‰ˆæœ¬çš„å‘¨æ›´å¢é‡å¯¼å…¥ - åªå¤„ç†æœ‰å˜åŒ–çš„æ–‡ä»¶ï¼Œæ‰¹é‡å¯¼å…¥"""
        try:
            schema = self.schema_map[adj_type]
            table = self.table_map[adj_type]
            table_name = f"{schema}.{table}"
            
            # è·å–æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = list(Path(dir_path).glob("*.csv"))
            print(f"ğŸ“„ æ‰«æ {len(csv_files)} ä¸ªCSVæ–‡ä»¶...")
            
            # æ£€æŸ¥æœ‰å˜åŒ–çš„æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            changed_files = []
            unchanged_count = 0
            
            # æ‰¹é‡æ£€æŸ¥hashï¼Œæ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            for i, csv_file in enumerate(csv_files):
                if i % 100 == 0 and i > 0:
                    print(f"  ğŸ” æ£€æŸ¥è¿›åº¦: {i}/{len(csv_files)}")
                
                file_path = str(csv_file)
                filename = csv_file.name
                
                # è®¡ç®—å½“å‰æ–‡ä»¶hash
                current_hash = self.hash_manager.calculate_file_hash(file_path)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
                stored_hash_key = f"weekly_{adj_type}_{filename}"
                
                if stored_hash_key in self.hash_manager.hash_data:
                    stored_hash = self.hash_manager.hash_data[stored_hash_key]
                    if current_hash == stored_hash:
                        unchanged_count += 1
                        continue  # æ–‡ä»¶æ²¡æœ‰å˜åŒ–ï¼Œè·³è¿‡
                
                # æ–‡ä»¶æœ‰å˜åŒ–æˆ–æ˜¯æ–°æ–‡ä»¶ï¼ŒåŠ å…¥å¤„ç†åˆ—è¡¨
                changed_files.append((csv_file, current_hash, stored_hash_key))
            
            print(f"ğŸ“Š æ–‡ä»¶å˜åŒ–ç»Ÿè®¡: {len(changed_files)} ä¸ªæœ‰å˜åŒ–ï¼Œ{unchanged_count} ä¸ªæ— å˜åŒ–")
            
            if not changed_files:
                print(f"âœ… {adj_type}æ•°æ®æ— å˜åŒ–ï¼Œè·³è¿‡å¯¼å…¥")
                return False  # æ²¡æœ‰å˜åŒ–å°±è¿”å›False
            
            # æ‰¹é‡å¤„ç†æœ‰å˜åŒ–çš„æ–‡ä»¶
            print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å…¥ {len(changed_files)} ä¸ªå˜åŒ–çš„æ–‡ä»¶...")
            updated_count = 0
            
            for i, (csv_file, current_hash, hash_key) in enumerate(changed_files, 1):
                if i % 20 == 0:  # æ¯20ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"  ğŸ“ˆ å¯¼å…¥è¿›åº¦: {i}/{len(changed_files)} ({i/len(changed_files)*100:.1f}%)")
                
                # ä½¿ç”¨æ‰¹é‡å¯¼å…¥
                if self._import_recent_records_from_csv_batch(str(csv_file), table_name, target_dates):
                    updated_count += 1
                    # æ›´æ–°hashè®°å½•
                    self.hash_manager.hash_data[hash_key] = current_hash
            
            # ä¿å­˜hashæ–‡ä»¶
            if updated_count > 0:
                self.hash_manager._save_hash_file()
                print(f"âœ… {adj_type}æ‰¹é‡å¯¼å…¥å®Œæˆ: {updated_count}ä¸ªæ–‡ä»¶æ›´æ–°")
                return True
            else:
                print(f"â„¹ï¸ {adj_type}: æ–‡ä»¶æœ‰å˜åŒ–ï¼Œä½†æ²¡æœ‰ç›®æ ‡æ—¥æœŸèŒƒå›´å†…çš„æ–°æ•°æ®")
                return False
                
        except Exception as e:
            print(f"âŒ {adj_type}ä¼˜åŒ–å¯¼å…¥å¤±è´¥: {e}")
            return False

    def _import_recent_records_from_csv_batch(self, csv_file_path: str, table_name: str, target_dates: List[str]) -> bool:
        """æ‰¹é‡å¯¼å…¥CSVæ–‡ä»¶ä¸­çš„æœ€è¿‘è®°å½•ï¼ˆå‘¨æ›´ç‰ˆæœ¬ï¼‰"""
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8')
            if df.empty:
                return False
            
            # è¿‡æ»¤æœ€è¿‘çš„æ•°æ®
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str)
            recent_df = df[df['æ—¥æœŸ'].isin(target_dates)]
            
            if recent_df.empty:
                return False
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
            batch_data = []
            
            for _, row in recent_df.iterrows():
                try:
                    trade_date = datetime.strptime(str(row['æ—¥æœŸ']), '%Y%m%d').date()
                    
                    record = (
                        row['ä»£ç '],
                        trade_date,
                        float(row['å¼€ç›˜ä»·']) if pd.notnull(row['å¼€ç›˜ä»·']) else None,
                        float(row['æœ€é«˜ä»·']) if pd.notnull(row['æœ€é«˜ä»·']) else None,
                        float(row['æœ€ä½ä»·']) if pd.notnull(row['æœ€ä½ä»·']) else None,
                        float(row['æ”¶ç›˜ä»·']) if pd.notnull(row['æ”¶ç›˜ä»·']) else None,
                        float(row['æˆäº¤é‡(æ‰‹æ•°)']) if pd.notnull(row['æˆäº¤é‡(æ‰‹æ•°)']) else None,
                        float(row['æˆäº¤é¢(åƒå…ƒ)']) if pd.notnull(row['æˆäº¤é¢(åƒå…ƒ)']) else None,
                        float(row['ä¸Šæ—¥æ”¶ç›˜']) if pd.notnull(row['ä¸Šæ—¥æ”¶ç›˜']) else None,
                        float(row['æ¶¨è·Œ']) if pd.notnull(row['æ¶¨è·Œ']) else None,
                        float(row['æ¶¨å¹…%']) if pd.notnull(row['æ¶¨å¹…%']) else None
                    )
                    batch_data.append(record)
                        
                except Exception as e:
                    continue  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
            
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            if batch_data:
                self._execute_batch_insert_weekly(table_name, batch_data)
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¯¼å…¥æœ€è¿‘è®°å½•å¤±è´¥: {e}")
            return False

    def import_latest_weekly_data_optimized(self, base_dir: str, weeks_back: int = 1) -> Dict[str, bool]:
        """ä¼˜åŒ–ç‰ˆæœ¬çš„å‘¨æ›´å¢é‡å¯¼å…¥ - é«˜æ€§èƒ½æ‰¹é‡å¤„ç†"""
        print(f"ğŸš€ æ‰§è¡Œé«˜æ€§èƒ½å‘¨æ›´å¢é‡å¯¼å…¥ï¼šæœ€è¿‘{weeks_back}å‘¨çš„æ•°æ®")
        
        if not os.path.exists(base_dir):
            print(f"âŒ åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return {}
        
        directories = {
            'å‰å¤æƒ': '0_ETFæ—¥K(å‰å¤æƒ)',
            'åå¤æƒ': '0_ETFæ—¥K(åå¤æƒ)', 
            'é™¤æƒ': '0_ETFæ—¥K(é™¤æƒ)'
        }
        
        results = {}
        
        try:
            if not self.connect():
                return results
            
            # ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
            self._ensure_tables_exist()
            
            # è·å–éœ€è¦æ›´æ–°çš„æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘å‡ å‘¨ï¼‰
            target_dates = self._get_recent_week_dates(weeks_back)
            print(f"ğŸ“… ç›®æ ‡æ—¥æœŸèŒƒå›´: {len(target_dates)} å¤©")
            
            total_start_time = datetime.now()
            
            for adj_type, dir_name in directories.items():
                dir_path = os.path.join(base_dir, dir_name)
                if os.path.exists(dir_path):
                    print(f"\nğŸ“‚ é«˜æ€§èƒ½å¯¼å…¥{adj_type}æ•°æ®...")
                    start_time = datetime.now()
                    
                    success = self._import_recent_data_optimized(dir_path, adj_type, target_dates)
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    print(f"â±ï¸ {adj_type}å¯¼å…¥è€—æ—¶: {duration:.2f}ç§’")
                    
                    results[adj_type] = success
                else:
                    print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {dir_name}")
                    results[adj_type] = False
            
            # æäº¤æ›´æ”¹
            self.db_manager.connection.commit()
            
            total_end_time = datetime.now()
            total_duration = (total_end_time - total_start_time).total_seconds()
            print(f"\nâœ… é«˜æ€§èƒ½å‘¨æ›´å¢é‡å¯¼å…¥å®Œæˆ! æ€»è€—æ—¶: {total_duration:.2f}ç§’")
            
        except Exception as e:
            print(f"âŒ é«˜æ€§èƒ½å‘¨æ›´å¢é‡å¯¼å…¥å¤±è´¥: {e}")
            if hasattr(self, 'db_manager') and self.db_manager.connection:
                self.db_manager.connection.rollback()
        finally:
            self.disconnect()
        
        return results


def main():
    """ç‹¬ç«‹è¿è¡Œæµ‹è¯•"""
    print("ğŸš€ ETFå‘¨æ›´æ•°æ®å¯¼å…¥å™¨æµ‹è¯•")
    
    # é»˜è®¤ç›®å½•è·¯å¾„
    base_dir = "../../ETFå‘¨æ›´"
    
    importer = WeeklyDataImporter()
    
    # æµ‹è¯•å¢é‡å¯¼å…¥ï¼ˆæœ€è¿‘1å‘¨ï¼‰
    results = importer.import_latest_weekly_data(base_dir, weeks_back=1)
    
    print(f"\nğŸ“Š å¯¼å…¥ç»“æœ: {results}")
    
    if any(results.values()):
        print("âœ… æµ‹è¯•æˆåŠŸ!")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥!")
    
    return any(results.values())


if __name__ == "__main__":
    main() 