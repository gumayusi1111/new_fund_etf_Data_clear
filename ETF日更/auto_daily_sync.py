#!/usr/bin/env python3
"""
ETF æ—¥æ›´æ–°è‡ªåŠ¨åŒæ­¥è„šæœ¬ (ä¸´æ—¶å¤„ç†ç‰ˆæœ¬)
1. åªæ£€æŸ¥ä»Šå¤©æ—¥æœŸçš„æ–‡ä»¶
2. ä¸´æ—¶ä¸‹è½½åˆ°å†…å­˜/ä¸´æ—¶ç›®å½•å¤„ç†
3. å¤„ç†å®Œæˆåç«‹å³åˆ é™¤ä¸´æ—¶æ–‡ä»¶
4. å®ç°çœŸæ­£çš„å¢é‡æ›´æ–°ï¼Œä¸å ç”¨å­˜å‚¨ç©ºé—´
"""

import os
import sys
import subprocess
import tempfile
import shutil
from datetime import datetime, timedelta
from typing import List, Set, Dict

# æ·»åŠ configç›®å½•åˆ°è·¯å¾„
config_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config')
sys.path.insert(0, config_dir)

try:
    import sys
    import importlib.util
    # å¯¼å…¥å“ˆå¸Œç®¡ç†å™¨
    hash_manager_path = os.path.join(config_dir, 'hash_manager.py')
    spec = importlib.util.spec_from_file_location("hash_manager", hash_manager_path)
    hash_manager_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(hash_manager_module)
    HashManager = hash_manager_module.HashManager
    
    # å¯¼å…¥æ—¥å¿—é…ç½®
    logger_config_path = os.path.join(config_dir, 'logger_config.py')
    spec = importlib.util.spec_from_file_location("logger_config", logger_config_path)
    logger_config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(logger_config_module)
    setup_daily_logger = logger_config_module.setup_daily_logger
    
    # è®¾ç½®æ—¥æ›´ä¸“ç”¨æ—¥å¿—
    logger = setup_daily_logger()
except ImportError as e:
    print(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥é…ç½®æ¨¡å—: {e}")
    HashManager = None
    logger = None

try:
    from bypy import ByPy
except ImportError:
    print("é”™è¯¯ï¼šæœªå®‰è£… bypyï¼Œè¯·è¿è¡Œ: pip install bypy")
    sys.exit(1)

# é…ç½®é¡¹
BAIDU_REMOTE_BASE = "/ETF_æŒ‰æ—¥æœŸ"  # ç™¾åº¦ç½‘ç›˜ä¸­æŒ‰æ—¥æœŸæ•°æ®æ ¹ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))  # å½“å‰è„šæœ¬ç›®å½•
PROCESSOR_SCRIPT = "daily_etf_processor.py"  # æ•°æ®å¤„ç†è„šæœ¬


def get_today_filename() -> str:
    """è·å–ä»Šå¤©çš„æ–‡ä»¶å"""
    return datetime.now().strftime('%Y%m%d.csv')


def get_recent_filenames(days_back=15) -> List[str]:
    """è·å–æœ€è¿‘Nå¤©çš„æ–‡ä»¶ååˆ—è¡¨"""
    filenames = []
    today = datetime.now()
    
    for i in range(days_back):
        date = today - timedelta(days=i)
        filename = date.strftime('%Y%m%d.csv')
        filenames.append(filename)
    
    return filenames


def batch_check_remote_files(bp: ByPy, filenames: List[str]) -> List[str]:
    """æ‰¹é‡æ£€æŸ¥ç™¾åº¦ç½‘ç›˜ä¸­å“ªäº›æ–‡ä»¶å­˜åœ¨"""
    print(f"ğŸ” æ£€æŸ¥ç™¾åº¦ç½‘ç›˜ä¸­æœ€è¿‘{len(filenames)}å¤©çš„æ–‡ä»¶...")
    
    try:
        import io
        from contextlib import redirect_stdout
        
        # è·å–è¿œç¨‹æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸€æ¬¡æ€§è·å–ï¼‰
        f = io.StringIO()
        with redirect_stdout(f):
            bp.list(BAIDU_REMOTE_BASE)
        
        output = f.getvalue()
        
        # è§£æå‡ºæ‰€æœ‰è¿œç¨‹æ–‡ä»¶å
        remote_files = set()
        for line in output.split('\n'):
            line = line.strip()
            if line.startswith('F '):
                parts = line.split(' ', 3)
                if len(parts) >= 2 and parts[1].endswith('.csv'):
                    remote_files.add(parts[1])
        
        # æ£€æŸ¥å“ªäº›ç›®æ ‡æ–‡ä»¶å­˜åœ¨
        existing_files = []
        for filename in filenames:
            if filename in remote_files:
                existing_files.append(filename)
        
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        existing_files.sort(reverse=True)
        
        print(f"â˜ï¸ è¿œç¨‹å­˜åœ¨ {len(existing_files)}/{len(filenames)} ä¸ªæ–‡ä»¶")
        return existing_files
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡æ£€æŸ¥è¿œç¨‹æ–‡ä»¶å¤±è´¥: {e}")
        return []


def check_local_data_exists(date_str: str) -> bool:
    """æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æœ¬åœ°æ•°æ®æ˜¯å¦å­˜åœ¨"""
    test_etfs = ["159001", "159003", "159005"]
    output_dirs = ["0_ETFæ—¥K(å‰å¤æƒ)", "0_ETFæ—¥K(åå¤æƒ)", "0_ETFæ—¥K(é™¤æƒ)"]
    
    for etf_code in test_etfs:
        for output_dir in output_dirs:
            etf_file = os.path.join(CURRENT_DIR, output_dir, f"{etf_code}.csv")
            
            if os.path.exists(etf_file):
                try:
                    # å¿«é€Ÿæ£€æŸ¥ï¼šåªè¯»å‰å‡ è¡Œ
                    with open(etf_file, 'r', encoding='utf-8') as f:
                        f.readline()  # è·³è¿‡è¡¨å¤´
                        for _ in range(5):  # åªæ£€æŸ¥å‰5è¡Œ
                            line = f.readline().strip()
                            if line and date_str in line:
                                return True
                except Exception:
                    continue
    
    return False


def check_local_processed_status(filenames: List[str], hash_manager) -> Dict[str, bool]:
    """æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¤„ç†çŠ¶æ€"""
    print(f"ğŸ“Š æ£€æŸ¥æœ¬åœ°å¤„ç†çŠ¶æ€...")
    
    status = {}
    
    for filename in filenames:
        date_str = filename[:8]  # YYYYMMDD
        
        # æ–¹æ³•1ï¼šæ£€æŸ¥hashè®°å½•
        has_hash_record = hash_manager and hash_manager.is_file_downloaded(filename)
        
        # æ–¹æ³•2ï¼šæ£€æŸ¥æœ¬åœ°æ•°æ®å®Œæ•´æ€§
        has_local_data = check_local_data_exists(date_str)
        
        # ä»»ä¸€æ–¹æ³•ç¡®è®¤å­˜åœ¨å°±è®¤ä¸ºå·²å¤„ç†
        status[filename] = has_hash_record or has_local_data
    
    processed_count = sum(status.values())
    print(f"ğŸ’¾ æœ¬åœ°å·²å¤„ç† {processed_count}/{len(filenames)} ä¸ªæ–‡ä»¶")
    
    return status


def check_remote_file_exists(bp: ByPy, filename: str) -> bool:
    """æ£€æŸ¥ç™¾åº¦ç½‘ç›˜ä¸­æŒ‡å®šæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    try:
        import io
        from contextlib import redirect_stdout
        
        # æ•è· list å‘½ä»¤çš„è¾“å‡º
        f = io.StringIO()
        with redirect_stdout(f):
            bp.list(BAIDU_REMOTE_BASE)
        
        output = f.getvalue()
        
        # è§£æè¾“å‡ºï¼ŒæŸ¥æ‰¾æŒ‡å®šæ–‡ä»¶
        for line in output.split('\n'):
            line = line.strip()
            if line.startswith('F '):
                # æ ¼å¼: F æ–‡ä»¶å å¤§å° æ—¥æœŸæ—¶é—´ å“ˆå¸Œ
                parts = line.split(' ', 3)
                if len(parts) >= 2 and parts[1] == filename:
                    return True
        
        return False
    except Exception as e:
        print(f"æ£€æŸ¥è¿œç¨‹æ–‡ä»¶å¤±è´¥: {e}")
        return False


def download_to_temp(bp: ByPy, filename: str, temp_dir: str, hash_manager=None) -> str:
    """ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
    try:
        remote_path = f"{BAIDU_REMOTE_BASE}/{filename}"
        local_temp_path = os.path.join(temp_dir, filename)
        
        print(f"ğŸ“¥ ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•: {filename}...")
        bp.downfile(remote_path, local_temp_path)
        
        # æ›´æ–°å“ˆå¸Œè®°å½•
        if hash_manager:
            hash_manager.update_file_hash(filename, local_temp_path)
        
        print(f"âœ“ ä¸´æ—¶ä¸‹è½½å®Œæˆ: {filename}")
        return local_temp_path
    except Exception as e:
        print(f"âœ— ä¸‹è½½å¤±è´¥ {filename}: {e}")
        return None


def should_update_data(filename: str, hash_manager) -> tuple[bool, str]:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®"""
    if not hash_manager:
        return True, "æ— å“ˆå¸ŒéªŒè¯"
    
    if not hash_manager.is_file_downloaded(filename):
        return True, "æ–°æ–‡ä»¶æˆ–å·²æ›´æ–°"
    
    # æ£€æŸ¥æœ¬åœ°æ•°æ®å®Œæ•´æ€§ï¼šä¸‰ä¸ªå¤æƒç›®å½•éƒ½å¿…é¡»æœ‰ä»Šå¤©çš„æ•°æ®
    today_date = filename[:8]  # YYYYMMDD
    output_dirs = [
        "0_ETFæ—¥K(å‰å¤æƒ)",
        "0_ETFæ—¥K(åå¤æƒ)", 
        "0_ETFæ—¥K(é™¤æƒ)"
    ]
    
    # æ£€æŸ¥å‡ ä¸ªä»£è¡¨æ€§ETFæ˜¯å¦æœ‰ä»Šå¤©çš„æ•°æ®
    test_etfs = ["159001", "159003", "159005"]
    
    for etf_code in test_etfs:
        for output_dir in output_dirs:
            etf_file = os.path.join(CURRENT_DIR, output_dir, f"{etf_code}.csv")
            
            if not os.path.exists(etf_file):
                continue  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ£€æŸ¥
            
            try:
                # è¯»å–æ–‡ä»¶ç¬¬ä¸€è¡Œæ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»Šå¤©çš„æ—¥æœŸ
                with open(etf_file, 'r', encoding='utf-8') as f:
                    f.readline()  # è·³è¿‡è¡¨å¤´
                    first_data_line = f.readline().strip()
                    
                if first_data_line and today_date not in first_data_line:
                    return True, f"æœ¬åœ°{output_dir}æ•°æ®ä¸å®Œæ•´ï¼Œéœ€è¦é‡æ–°å¤„ç†"
                    
            except Exception as e:
                return True, f"æ£€æŸ¥æœ¬åœ°æ•°æ®æ—¶å‡ºé”™ï¼Œéœ€è¦é‡æ–°å¤„ç†: {e}"
    
    return False, "å·²æ˜¯æœ€æ–°"


def run_processor_with_temp_data(temp_file_path: str) -> bool:
    """ä½¿ç”¨ä¸´æ—¶æ•°æ®è¿è¡Œå¤„ç†è„šæœ¬"""
    try:
        # åˆ›å»ºä¸´æ—¶çš„æºæ•°æ®ç›®å½•ç»“æ„
        temp_source_dir = tempfile.mkdtemp(prefix="etf_temp_source_")
        temp_filename = os.path.basename(temp_file_path)
        
        # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶æºç›®å½•
        shutil.copy2(temp_file_path, os.path.join(temp_source_dir, temp_filename))
        
        # ä¿®æ”¹å¤„ç†è„šæœ¬çš„æºç›®å½•é…ç½®ï¼ˆä¸´æ—¶ï¼‰
        date_str = temp_filename[:8]  # YYYYMMDD
        cmd = [
            "python", PROCESSOR_SCRIPT, 
            "--mode", "range", 
            "--start-date", date_str, 
            "--end-date", date_str,
            "--temp-source-dir", temp_source_dir  # ä¼ é€’ä¸´æ—¶ç›®å½•
        ]
        
        print(f"ğŸ”„ è¿è¡Œå¢é‡å¤„ç†: {' '.join(cmd)}")
        result = subprocess.run(cmd, cwd=CURRENT_DIR, capture_output=True, text=True)
        
        # æ¸…ç†ä¸´æ—¶æºç›®å½•
        shutil.rmtree(temp_source_dir, ignore_errors=True)
        
        if result.returncode == 0:
            print("âœ“ æ•°æ®å¤„ç†å®Œæˆ")
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            if result.stdout:
                lines = result.stdout.strip().split('\n')
                # åªæ˜¾ç¤ºé‡è¦çš„è¾“å‡ºä¿¡æ¯
                for line in lines[-5:]:  # æ˜¾ç¤ºæœ€å5è¡Œ
                    if any(keyword in line for keyword in ['å®Œæˆ', 'æˆåŠŸ', 'å¤„ç†', 'ç”Ÿæˆ', 'ç»Ÿè®¡']):
                        print(f"  {line}")
            return True
        else:
            print(f"âœ— æ•°æ®å¤„ç†å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âœ— è¿è¡Œå¤„ç†è„šæœ¬å¤±è´¥: {e}")
        return False


def daily_incremental_sync():
    """æ¯æ—¥å¢é‡åŒæ­¥ - ä¸´æ—¶å¤„ç†ç‰ˆæœ¬"""
    today_file = get_today_filename()
    print(f"ğŸš€ ETFæ—¥æ›´æ–° - æ£€æŸ¥ä»Šå¤©çš„æ•°æ®: {today_file}")
    
    # åˆå§‹åŒ– bypy
    try:
        bp = ByPy()
        print("âœ“ ç™¾åº¦ç½‘ç›˜è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— ç™¾åº¦ç½‘ç›˜è¿æ¥å¤±è´¥: {e}")
        return False
    
    # åˆå§‹åŒ–å“ˆå¸Œç®¡ç†å™¨
    hash_manager = None
    if HashManager:
        try:
            hash_manager = HashManager()
            print("âœ“ å“ˆå¸Œç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ å“ˆå¸Œç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            hash_manager = None
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    need_update, reason = should_update_data(today_file, hash_manager)
    
    if not need_update:
        print(f"âœ… ä»Šå¤©çš„æ•°æ®å·²æ˜¯æœ€æ–° ({reason})")
        print("æ— éœ€æ›´æ–°ï¼Œä¿æŒç°æœ‰æ•°æ®")
        return True
    
    print(f"ğŸ“‹ éœ€è¦æ›´æ–°: {today_file} ({reason})")
    
    # æ£€æŸ¥ç™¾åº¦ç½‘ç›˜ä¸­æ˜¯å¦æœ‰ä»Šå¤©çš„æ–‡ä»¶
    print(f"ğŸ” æ£€æŸ¥ç™¾åº¦ç½‘ç›˜ä¸­çš„ {today_file}...")
    remote_exists = check_remote_file_exists(bp, today_file)
    
    if not remote_exists:
        print(f"âŒ ç™¾åº¦ç½‘ç›˜ä¸­æ²¡æœ‰ä»Šå¤©çš„æ–‡ä»¶: {today_file}")
        print("å¯èƒ½åŸå› ï¼š")
        print("1. ä»Šå¤©æ˜¯éäº¤æ˜“æ—¥ï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ï¼‰")
        print("2. æ•°æ®å°šæœªä¸Šä¼ ")
        return False
    
    print(f"âœ“ æ‰¾åˆ°è¿œç¨‹æ–‡ä»¶: {today_file}")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix="etf_daily_temp_")
    print(f"ğŸ“ åˆ›å»ºä¸´æ—¶å¤„ç†ç›®å½•: {temp_dir}")
    
    try:
        # ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
        temp_file_path = download_to_temp(bp, today_file, temp_dir, hash_manager)
        
        if not temp_file_path:
            print("âš ï¸ æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            return False
        
        print(f"ğŸ“Š ä¸´æ—¶æ–‡ä»¶å¤§å°: {os.path.getsize(temp_file_path)} å­—èŠ‚")
        
        # å¤„ç†æ•°æ®
        print("ğŸ”„ å¼€å§‹å¢é‡å¤„ç†...")
        if run_processor_with_temp_data(temp_file_path):
            print("ğŸ‰ ä»Šæ—¥å¢é‡æ›´æ–°å®Œæˆï¼")
            return True
        else:
            print("âš ï¸ æ•°æ®å¤„ç†å¤±è´¥")
            return False
            
    finally:
        # ç¡®ä¿æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)
            print(f"ğŸ§½ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")


def detect_missing_data(days_back=15) -> Dict[str, List[str]]:
    """æ£€æµ‹æœ€è¿‘Nå¤©çš„ç¼ºå¤±æ•°æ®"""
    print(f"ğŸš€ å¼€å§‹æ£€æµ‹æœ€è¿‘{days_back}å¤©çš„ç¼ºå¤±æ•°æ®...")
    
    # 1. ç”Ÿæˆè¦æ£€æŸ¥çš„æ–‡ä»¶å
    target_filenames = get_recent_filenames(days_back)
    
    # 2. åˆå§‹åŒ–ç™¾åº¦ç½‘ç›˜å’Œhashç®¡ç†å™¨
    try:
        bp = ByPy()
        hash_manager = HashManager() if HashManager else None
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return {'missing': [], 'existing': [], 'processed': []}
    
    # 3. æ‰¹é‡æ£€æŸ¥è¿œç¨‹å­˜åœ¨çš„æ–‡ä»¶
    remote_existing = batch_check_remote_files(bp, target_filenames)
    
    # 4. æ£€æŸ¥æœ¬åœ°å¤„ç†çŠ¶æ€
    local_status = check_local_processed_status(remote_existing, hash_manager)
    
    # 5. æ‰¾å‡ºç¼ºå¤±çš„æ–‡ä»¶
    missing_files = [f for f in remote_existing if not local_status.get(f, False)]
    processed_files = [f for f in remote_existing if local_status.get(f, False)]
    
    result = {
        'missing': missing_files,        # éœ€è¦ä¸‹è½½å¤„ç†çš„
        'existing': remote_existing,     # è¿œç¨‹å­˜åœ¨çš„
        'processed': processed_files     # å·²å¤„ç†çš„
    }
    
    # 6. æ˜¾ç¤ºç»“æœ
    show_detection_result(result, days_back)
    
    return result


def show_detection_result(result: Dict[str, List[str]], days_back: int):
    """æ˜¾ç¤ºæ£€æµ‹ç»“æœ"""
    missing = result['missing']
    existing = result['existing'] 
    processed = result['processed']
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æœ€è¿‘{days_back}å¤©æ•°æ®æ£€æµ‹ç»“æœ")
    print("=" * 50)
    
    print(f"â˜ï¸ è¿œç¨‹æ–‡ä»¶æ€»æ•°: {len(existing)}")
    print(f"âœ… å·²å¤„ç†æ–‡ä»¶æ•°: {len(processed)}")
    print(f"âŒ ç¼ºå¤±æ–‡ä»¶æ•°: {len(missing)}")
    
    if missing:
        print(f"\nğŸ“‹ ç¼ºå¤±æ–‡ä»¶æ˜ç»†:")
        # æŒ‰æ—¥æœŸæ’åºæ˜¾ç¤ºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_missing = sorted(missing, reverse=True)
        for filename in sorted_missing:
            date_str = filename[:8]
            formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            weekday = datetime.strptime(date_str, '%Y%m%d').strftime('%A')
            print(f"   ğŸ“… {filename} ({formatted_date}, {weekday})")
        
        print(f"\nğŸ’¡ å»ºè®®å‘½ä»¤:")
        print(f"   python auto_daily_sync.py --mode fill-missing")
    else:
        print("\nğŸ‰ æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±!")
    
    print("=" * 50)


def process_single_missing_file(filename: str) -> bool:
    """å¤„ç†å•ä¸ªç¼ºå¤±æ–‡ä»¶ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰"""
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix="etf_missing_")
    
    try:
        bp = ByPy()
        hash_manager = HashManager() if HashManager else None
        
        # ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
        temp_file_path = download_to_temp(bp, filename, temp_dir, hash_manager)
        
        if temp_file_path:
            # å¤„ç†æ•°æ®ï¼ˆå¤ç”¨ç°æœ‰å‡½æ•°ï¼‰
            return run_processor_with_temp_data(temp_file_path)
        
        return False
        
    except Exception as e:
        print(f"âœ— å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return False
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)


def batch_fill_missing_files(missing_files: List[str]) -> bool:
    """æ‰¹é‡è¡¥é½ç¼ºå¤±æ–‡ä»¶"""
    if not missing_files:
        print("âœ… æ— ç¼ºå¤±æ–‡ä»¶éœ€è¦è¡¥é½")
        return True
    
    print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡è¡¥é½ {len(missing_files)} ä¸ªç¼ºå¤±æ–‡ä»¶...")
    
    # æŒ‰æ—¥æœŸæ’åºï¼Œå…ˆå¤„ç†æœ€æ–°çš„ç¼ºå¤±æ•°æ®
    sorted_missing = sorted(missing_files, reverse=True)
    
    success_count = 0
    for i, filename in enumerate(sorted_missing, 1):
        print(f"\n[{i}/{len(missing_files)}] å¤„ç† {filename}...")
        
        # ä½¿ç”¨ç°æœ‰çš„ä¸‹è½½å¤„ç†é€»è¾‘
        if process_single_missing_file(filename):
            success_count += 1
    
    print(f"\nğŸ‰ æ‰¹é‡è¡¥æ¼å®Œæˆ: {success_count}/{len(missing_files)}")
    return success_count > 0


def smart_daily_update(days_back=15):
    """æ™ºèƒ½æ—¥æ›´æ–°ï¼šå…ˆè¡¥æ¼ï¼Œå†æ›´æ–°ä»Šå¤©"""
    print("ğŸš€ æ™ºèƒ½æ—¥æ›´æ–°æ¨¡å¼...")
    
    # 1. æ£€æµ‹ç¼ºå¤±
    result = detect_missing_data(days_back)
    
    # 2. è¡¥é½ç¼ºå¤±
    missing_success = True
    if result['missing']:
        print(f"\nå‘ç° {len(result['missing'])} ä¸ªç¼ºå¤±æ–‡ä»¶ï¼Œå¼€å§‹è¡¥é½...")
        missing_success = batch_fill_missing_files(result['missing'])
    
    # 3. æ‰§è¡Œä»Šå¤©çš„æ›´æ–°
    print(f"\nğŸ“… æ‰§è¡Œä»Šæ—¥å¸¸è§„æ›´æ–°...")
    today_success = daily_incremental_sync()
    
    # 4. æ±‡æ€»ç»“æœ
    if missing_success and today_success:
        print("ğŸ‰ æ™ºèƒ½æ›´æ–°å®Œå…¨æˆåŠŸï¼")
        return True
    elif today_success:
        print("âœ… ä»Šæ—¥æ›´æ–°æˆåŠŸï¼Œä½†ç¼ºå¤±æ•°æ®è¡¥é½å¯èƒ½æœ‰é—®é¢˜")
        return True
    else:
        print("âš ï¸ æ™ºèƒ½æ›´æ–°éƒ¨åˆ†å¤±è´¥")
        return False


def test_connection():
    """æµ‹è¯•ç™¾åº¦ç½‘ç›˜è¿æ¥"""
    print("ğŸ”§ æµ‹è¯•ç™¾åº¦ç½‘ç›˜è¿æ¥...")
    
    try:
        bp = ByPy()
        bp.info()
        print("âœ“ è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— è¿æ¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ£€æŸ¥ä»Šå¤©çš„æ–‡ä»¶
    today_file = get_today_filename()
    print(f"\nğŸ” æ£€æŸ¥ä»Šå¤©çš„æ–‡ä»¶: {today_file}")
    
    remote_exists = check_remote_file_exists(bp, today_file)
    if remote_exists:
        print(f"âœ“ ç™¾åº¦ç½‘ç›˜ä¸­å­˜åœ¨: {today_file}")
    else:
        print(f"âŒ ç™¾åº¦ç½‘ç›˜ä¸­ä¸å­˜åœ¨: {today_file}")
    
    # æµ‹è¯•å“ˆå¸Œç®¡ç†å™¨
    if HashManager:
        try:
            hash_manager = HashManager()
            print(f"\nğŸ“Š å“ˆå¸Œç®¡ç†å™¨çŠ¶æ€:")
            hash_manager.print_status()
            
            need_update, reason = should_update_data(today_file, hash_manager)
            if need_update:
                print(f"âš ï¸ {today_file} éœ€è¦æ›´æ–°: {reason}")
            else:
                print(f"âœ… {today_file} å·²æ˜¯æœ€æ–°: {reason}")
        except Exception as e:
            print(f"âš ï¸ å“ˆå¸Œç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
    
    return True


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ETFæ—¥æ›´æ–°è‡ªåŠ¨åŒæ­¥è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰')
    parser.add_argument('--mode', 
                        choices=['daily', 'test', 'check-missing', 'fill-missing', 'smart-update'], 
                        default='daily',
                        help='è¿è¡Œæ¨¡å¼: daily(æ¯æ—¥æ›´æ–°), test(æµ‹è¯•è¿æ¥), check-missing(æ£€æŸ¥ç¼ºå¤±), fill-missing(è¡¥é½ç¼ºå¤±), smart-update(æ™ºèƒ½æ›´æ–°)')
    parser.add_argument('--days-back', type=int, default=15, 
                        help='æ£€æŸ¥æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆé»˜è®¤15å¤©ï¼‰')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        success = test_connection()
        if not success:
            sys.exit(1)
    elif args.mode == 'check-missing':
        # åªæ£€æµ‹ï¼Œä¸å¤„ç†
        detect_missing_data(args.days_back)
    elif args.mode == 'fill-missing':
        # æ£€æµ‹å¹¶è¡¥æ¼
        result = detect_missing_data(args.days_back)
        if result['missing']:
            success = batch_fill_missing_files(result['missing'])
            if not success:
                sys.exit(1)
        else:
            print("âœ… æ— ç¼ºå¤±æ•°æ®éœ€è¦è¡¥é½")
    elif args.mode == 'smart-update':
        # æ™ºèƒ½æ›´æ–°ï¼šå…ˆæ£€æµ‹è¡¥æ¼ï¼Œå†æ›´æ–°ä»Šå¤©
        success = smart_daily_update(args.days_back)
        if not success:
            sys.exit(1)
    else:  # daily
        success = daily_incremental_sync()
        if not success:
            sys.exit(1) 