#!/usr/bin/env python3
"""
ETF æ—¥æ›´æ–°æ•°æ®å¤„ç†è„šæœ¬
åŸºäº ETF_æŒ‰æ—¥æœŸ ç›®å½•çš„æ•°æ®ï¼Œç”ŸæˆæŒ‰ä»£ç åˆ†ç¦»çš„ä¸‰ç§å¤æƒæ•°æ®
1. è¯»å– ETF_æŒ‰æ—¥æœŸ çš„æ‰€æœ‰ CSV æ–‡ä»¶
2. ä½¿ç”¨å¤æƒå› å­è®¡ç®—ä¸‰ç§å¤æƒä»·æ ¼
3. æŒ‰ ETF ä»£ç åˆ†ç¦»æ•°æ®ï¼Œç”Ÿæˆä¸‰ä¸ªå¤æƒç›®å½•
4. æ”¯æŒå¢é‡æ›´æ–°å’Œå…¨é‡é‡å»º
"""

import os
import sys
import pandas as pd
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional
from pathlib import Path
import argparse

# é…ç½®å¸¸é‡
DAILY_DATA_DIR = "./æŒ‰æ—¥æœŸ_æºæ•°æ®"  # æŒ‰æ—¥æœŸæ•°æ®ç›®å½•ï¼ˆé»˜è®¤å€¼ï¼Œå·²åºŸå¼ƒï¼‰
OUTPUT_BASE_DIR = "."  # è¾“å‡ºåŸºç¡€ç›®å½•
CATEGORIES = ["0_ETFæ—¥K(å‰å¤æƒ)", "0_ETFæ—¥K(åå¤æƒ)", "0_ETFæ—¥K(é™¤æƒ)"]

# å…¨å±€å˜é‡ï¼Œç”¨äºä¸´æ—¶ç›®å½•æ”¯æŒ
TEMP_SOURCE_DIR = None

# å­—æ®µæ˜ å°„ï¼šä»æŒ‰æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºæŒ‰ä»£ç æ ¼å¼
DATE_FORMAT_FIELDS = ["æ—¥æœŸ", "ä»£ç ", "åç§°", "å¼€ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æ”¶ç›˜ä»·", "ä¸Šæ—¥æ”¶ç›˜", "æ¶¨è·Œ", "æ¶¨å¹…%", "æˆäº¤é‡(æ‰‹æ•°)", "æˆäº¤é¢(åƒå…ƒ)", "å¤æƒå› å­"]
CODE_FORMAT_FIELDS = ["ä»£ç ", "æ—¥æœŸ", "å¼€ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æ”¶ç›˜ä»·", "ä¸Šæ—¥æ”¶ç›˜", "æ¶¨è·Œ", "æ¶¨å¹…%", "æˆäº¤é‡(æ‰‹æ•°)", "æˆäº¤é¢(åƒå…ƒ)"]


def ensure_output_directories():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    for category in CATEGORIES:
        category_dir = os.path.join(OUTPUT_BASE_DIR, category)
        os.makedirs(category_dir, exist_ok=True)
        print(f"âœ“ ç¡®ä¿ç›®å½•å­˜åœ¨: {category}")


def get_daily_csv_files(start_date: Optional[str] = None, end_date: Optional[str] = None) -> List[str]:
    """
    è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„CSVæ–‡ä»¶åˆ—è¡¨
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)ï¼ŒNoneè¡¨ç¤ºä»æœ€æ—©å¼€å§‹
        end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)ï¼ŒNoneè¡¨ç¤ºåˆ°æœ€æ–°
    
    Returns:
        æ’åºåçš„CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•æˆ–é»˜è®¤ç›®å½•
    source_dir = TEMP_SOURCE_DIR if TEMP_SOURCE_DIR else DAILY_DATA_DIR
    
    if not os.path.exists(source_dir):
        print(f"é”™è¯¯ï¼šæ•°æ®ç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return []
    
    csv_files = []
    for filename in os.listdir(source_dir):
        if filename.endswith('.csv') and len(filename) == 12:  # YYYYMMDD.csv
            date_str = filename[:8]
            
            # æ—¥æœŸèŒƒå›´è¿‡æ»¤
            if start_date and date_str < start_date:
                continue
            if end_date and date_str > end_date:
                continue
                
            csv_files.append(os.path.join(source_dir, filename))
    
    return sorted(csv_files)


def normalize_etf_code(etf_code: str) -> str:
    """
    æ ‡å‡†åŒ–ETFä»£ç æ ¼å¼ï¼šç§»é™¤.SZ/.SHåç¼€ï¼Œç»Ÿä¸€ä¸º6ä½æ•°å­—æ ¼å¼
    
    Args:
        etf_code: åŸå§‹ETFä»£ç ï¼ˆå¦‚ 159001.SZ æˆ– 159001ï¼‰
    
    Returns:
        æ ‡å‡†åŒ–åçš„ETFä»£ç ï¼ˆå¦‚ 159001ï¼‰
    """
    if isinstance(etf_code, str):
        # ç§»é™¤.SZã€.SHç­‰åç¼€
        return etf_code.split('.')[0]
    return str(etf_code)


def calculate_adjusted_prices(price: float, factor: float) -> Dict[str, float]:
    """
    æ ¹æ®å¤æƒå› å­è®¡ç®—ä¸‰ç§å¤æƒä»·æ ¼
    
    Args:
        price: åŸå§‹ä»·æ ¼ï¼ˆé™¤æƒä»·æ ¼ï¼‰
        factor: å¤æƒå› å­
    
    Returns:
        åŒ…å«ä¸‰ç§å¤æƒä»·æ ¼çš„å­—å…¸
    """
    return {
        'forward': price / factor,    # å‰å¤æƒ = é™¤æƒä»·æ ¼ / å¤æƒå› å­
        'backward': price * factor,   # åå¤æƒ = é™¤æƒä»·æ ¼ Ã— å¤æƒå› å­
        'no_adjust': price           # é™¤æƒ = åŸå§‹ä»·æ ¼
    }


def process_daily_file(csv_file: str) -> Dict[str, Dict[str, List]]:
    """
    å¤„ç†å•ä¸ªæ—¥æœŸæ–‡ä»¶ï¼Œè¿”å›æŒ‰ä»£ç åˆ†ç»„çš„ä¸‰ç§å¤æƒæ•°æ®
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
    
    Returns:
        {
            'forward': {etf_code: [row_data], ...},
            'backward': {etf_code: [row_data], ...}, 
            'no_adjust': {etf_code: [row_data], ...}
        }
    """
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file, encoding='utf-8')
        
        if df.empty:
            print(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {os.path.basename(csv_file)}")
            return {'forward': {}, 'backward': {}, 'no_adjust': {}}
        
        # æ£€æŸ¥å¹¶å¤„ç†æ–‡ä»¶å†…çš„é‡å¤æ•°æ®
        before_count = len(df)
        df = df.drop_duplicates(subset=['ä»£ç ', 'æ—¥æœŸ'], keep='last')
        after_count = len(df)
        
        if before_count > after_count:
            print(f"ğŸ§¹ {os.path.basename(csv_file)}: æ–‡ä»¶å†…å»é‡ {before_count} â†’ {after_count} æ¡è®°å½•")
        
        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ["æ—¥æœŸ", "ä»£ç ", "å¼€ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æ”¶ç›˜ä»·", "ä¸Šæ—¥æ”¶ç›˜", "æ¶¨è·Œ", "æ¶¨å¹…%", "æˆäº¤é‡(æ‰‹æ•°)", "æˆäº¤é¢(åƒå…ƒ)", "å¤æƒå› å­"]
        missing_fields = [field for field in required_fields if field not in df.columns]
        if missing_fields:
            print(f"âš ï¸ ç¼ºå°‘å¿…è¦å­—æ®µ {missing_fields}: {os.path.basename(csv_file)}")
            return {'forward': {}, 'backward': {}, 'no_adjust': {}}
        
        result = {'forward': {}, 'backward': {}, 'no_adjust': {}}
        
        for _, row in df.iterrows():
            etf_code = row['ä»£ç ']
            factor = float(row['å¤æƒå› å­'])
            
            # éœ€è¦å¤æƒè°ƒæ•´çš„ä»·æ ¼å­—æ®µ
            price_fields = ['å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'ä¸Šæ—¥æ”¶ç›˜']
            
            # å¤„ç†ä¸‰ç§å¤æƒç±»å‹
            for adj_type in ['forward', 'backward', 'no_adjust']:
                if etf_code not in result[adj_type]:
                    result[adj_type][etf_code] = []
                
                # æ„å»ºè¾“å‡ºè¡Œæ•°æ®
                row_data = [etf_code, row['æ—¥æœŸ']]  # ä»£ç ,æ—¥æœŸ
                
                # æ·»åŠ ä»·æ ¼å­—æ®µï¼ˆæ ¹æ®å¤æƒç±»å‹è°ƒæ•´ï¼‰
                for field in price_fields:
                    original_price = float(row[field])
                    adjusted_prices = calculate_adjusted_prices(original_price, factor)
                    row_data.append(adjusted_prices[adj_type])
                
                # æ·»åŠ å…¶ä»–å­—æ®µï¼ˆä¸éœ€è¦å¤æƒè°ƒæ•´ï¼‰
                row_data.extend([
                    row['æ¶¨è·Œ'],
                    row['æ¶¨å¹…%'], 
                    row['æˆäº¤é‡(æ‰‹æ•°)'],
                    row['æˆäº¤é¢(åƒå…ƒ)']
                ])
                
                result[adj_type][etf_code].append(row_data)
        
        return result
        
    except Exception as e:
        print(f"âœ— å¤„ç†æ–‡ä»¶å¤±è´¥ {os.path.basename(csv_file)}: {e}")
        return {'forward': {}, 'backward': {}, 'no_adjust': {}}


def merge_and_save_etf_data(all_data: Dict[str, Dict[str, List]], mode: str = 'incremental'):
    """
    åˆå¹¶å¹¶ä¿å­˜ETFæ•°æ®åˆ°å¯¹åº”çš„æ–‡ä»¶
    
    Args:
        all_data: æ‰€æœ‰å¤„ç†åçš„æ•°æ®
        mode: 'incremental' å¢é‡æ›´æ–°, 'rebuild' å…¨é‡é‡å»º
    """
    category_map = {
        'forward': "0_ETFæ—¥K(å‰å¤æƒ)",
        'backward': "0_ETFæ—¥K(åå¤æƒ)", 
        'no_adjust': "0_ETFæ—¥K(é™¤æƒ)"
    }
    
    for adj_type, category in category_map.items():
        category_dir = os.path.join(OUTPUT_BASE_DIR, category)
        
        for etf_code, rows in all_data[adj_type].items():
            if not rows:
                continue
            
            # æ ‡å‡†åŒ–ETFä»£ç ç”¨äºæ–‡ä»¶åï¼ˆç§»é™¤.SZ/.SHåç¼€ï¼‰
            normalized_code = normalize_etf_code(etf_code)
            etf_file = os.path.join(category_dir, f"{normalized_code}.csv")
            
            # åˆ›å»ºDataFrame
            new_df = pd.DataFrame(rows, columns=CODE_FORMAT_FIELDS)
            
            # å¯¹æ–°æ•°æ®è¿›è¡Œå»é‡ï¼ˆé˜²æ­¢åŒä¸€å¤©ä¸‹è½½çš„æ•°æ®æœ‰é‡å¤ï¼‰
            before_count = len(new_df)
            new_df = new_df.drop_duplicates(subset=['ä»£ç ', 'æ—¥æœŸ'], keep='last')
            after_count = len(new_df)
            
            if before_count > after_count:
                print(f"ğŸ§¹ {normalized_code}: æ–°æ•°æ®å»é‡ {before_count} â†’ {after_count} æ¡è®°å½•")
            
            if mode == 'incremental' and os.path.exists(etf_file):
                # å¢é‡æ¨¡å¼ï¼šè¯»å–ç°æœ‰æ•°æ®å¹¶åˆå¹¶
                try:
                    existing_df = pd.read_csv(etf_file, encoding='utf-8', dtype=str)
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    new_df = new_df.astype(str)
                    existing_df = existing_df.astype(str)
                    
                    # åˆå¹¶æ•°æ®å¹¶å»é‡ï¼ˆæŒ‰ä»£ç +æ—¥æœŸç»„åˆå»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®ï¼‰
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    
                    # æŒ‰ä»£ç å’Œæ—¥æœŸå»é‡ï¼Œä¿ç•™æœ€åä¸€ä¸ªï¼ˆæœ€æ–°çš„æ•°æ®è¦†ç›–æ—§çš„ï¼‰
                    combined_df = combined_df.drop_duplicates(subset=['ä»£ç ', 'æ—¥æœŸ'], keep='last')
                    
                    # æŒ‰æ—¥æœŸæ’åºï¼ˆé™åºï¼Œæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
                    combined_df['æ—¥æœŸ'] = combined_df['æ—¥æœŸ'].astype(str)
                    combined_df = combined_df.sort_values('æ—¥æœŸ', ascending=False)
                    
                    print(f"ğŸ”„ {normalized_code}: åˆå¹¶å {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡å®Œæˆï¼‰")
                    
                except Exception as e:
                    print(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ {normalized_code}.csv: {e}ï¼Œä½¿ç”¨æ–°æ•°æ®")
                    combined_df = new_df
            else:
                # é‡å»ºæ¨¡å¼æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼šç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                combined_df = new_df
                
                # å³ä½¿æ˜¯é‡å»ºæ¨¡å¼ï¼Œä¹Ÿè¦æ£€æŸ¥å¹¶å»é™¤é‡å¤æ•°æ®
                before_count = len(combined_df)
                combined_df = combined_df.drop_duplicates(subset=['ä»£ç ', 'æ—¥æœŸ'], keep='last')
                after_count = len(combined_df)
                
                if before_count > after_count:
                    print(f"ğŸ§¹ {normalized_code}: é‡å»ºæ¨¡å¼å»é‡ {before_count} â†’ {after_count} æ¡è®°å½•")
                
                # æŒ‰æ—¥æœŸæ’åºï¼ˆé™åºï¼Œæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
                combined_df['æ—¥æœŸ'] = combined_df['æ—¥æœŸ'].astype(str)
                combined_df = combined_df.sort_values('æ—¥æœŸ', ascending=False)
            
            # ä¿å­˜æ–‡ä»¶
            combined_df.to_csv(etf_file, index=False, encoding='utf-8-sig')
            
        print(f"âœ“ å®Œæˆ {category}: {len(all_data[adj_type])} ä¸ªETF")


def get_latest_dates(n_days: int = 5) -> List[str]:
    """è·å–æœ€è¿‘Nå¤©çš„æ—¥æœŸåˆ—è¡¨ï¼ˆYYYYMMDDæ ¼å¼ï¼‰"""
    dates = []
    current_date = datetime.now()
    
    for i in range(n_days):
        date_str = (current_date - timedelta(days=i)).strftime('%Y%m%d')
        dates.append(date_str)
    
    return dates


def main():
    global TEMP_SOURCE_DIR
    
    parser = argparse.ArgumentParser(description='ETFæ—¥æ›´æ–°æ•°æ®å¤„ç†è„šæœ¬')
    parser.add_argument('--mode', choices=['daily', 'rebuild', 'range'], default='daily',
                        help='è¿è¡Œæ¨¡å¼: daily(æ—¥æ›´æ–°), rebuild(å…¨é‡é‡å»º), range(æŒ‡å®šèŒƒå›´)')
    parser.add_argument('--start-date', type=str, help='å¼€å§‹æ—¥æœŸ (YYYYMMDD)')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸ (YYYYMMDD)')
    parser.add_argument('--days', type=int, default=5, help='æ—¥æ›´æ–°æ¨¡å¼ä¸‹å¤„ç†æœ€è¿‘å‡ å¤©çš„æ•°æ®')
    parser.add_argument('--temp-source-dir', type=str, help='ä¸´æ—¶æºæ•°æ®ç›®å½•ï¼ˆç”¨äºä¸´æ—¶å¤„ç†ï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æœæä¾›äº†ä¸´æ—¶ç›®å½•ï¼Œä½¿ç”¨å®ƒ
    if args.temp_source_dir:
        TEMP_SOURCE_DIR = args.temp_source_dir
        print(f"ğŸ”„ ä½¿ç”¨ä¸´æ—¶æºç›®å½•: {TEMP_SOURCE_DIR}")
    
    print(f"ğŸš€ ETFæ—¥æ›´æ–°æ•°æ®å¤„ç†å¼€å§‹ - æ¨¡å¼: {args.mode}")
    source_dir = TEMP_SOURCE_DIR if TEMP_SOURCE_DIR else DAILY_DATA_DIR
    print(f"ğŸ“ æºæ•°æ®ç›®å½•: {source_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_BASE_DIR}")
    if TEMP_SOURCE_DIR:
        print(f"ğŸ”„ ä¸´æ—¶å¤„ç†æ¨¡å¼: å¤„ç†å®Œæˆåå°†è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
    print()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_output_directories()
    
    # æ ¹æ®æ¨¡å¼è·å–æ–‡ä»¶åˆ—è¡¨
    if args.mode == 'daily':
        # æ—¥æ›´æ–°ï¼šå¤„ç†æœ€è¿‘Nå¤©çš„æ•°æ®
        recent_dates = get_latest_dates(args.days)
        csv_files = []
        for date_str in recent_dates:
            file_path = os.path.join(source_dir, f"{date_str}.csv")
            if os.path.exists(file_path):
                csv_files.append(file_path)
        csv_files.sort()
        mode = 'incremental'
        print(f"ğŸ“… æ—¥æ›´æ–°æ¨¡å¼: å¤„ç†æœ€è¿‘ {args.days} å¤©çš„æ•°æ®")
        
    elif args.mode == 'rebuild':
        # å…¨é‡é‡å»ºï¼šå¤„ç†æ‰€æœ‰æ•°æ®
        csv_files = get_daily_csv_files()
        mode = 'rebuild'
        print(f"ğŸ”„ å…¨é‡é‡å»ºæ¨¡å¼: å¤„ç†æ‰€æœ‰å†å²æ•°æ®")
        
    elif args.mode == 'range':
        # æŒ‡å®šèŒƒå›´ï¼šå¤„ç†æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
        if not args.start_date:
            print("é”™è¯¯ï¼šèŒƒå›´æ¨¡å¼éœ€è¦æŒ‡å®š --start-date")
            return
        csv_files = get_daily_csv_files(args.start_date, args.end_date)
        mode = 'incremental'
        print(f"ğŸ“Š èŒƒå›´æ¨¡å¼: {args.start_date} åˆ° {args.end_date or 'æœ€æ–°'}")
    
    if not csv_files:
        print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(csv_files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
    print()
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    all_data = {'forward': {}, 'backward': {}, 'no_adjust': {}}
    
    for i, csv_file in enumerate(csv_files, 1):
        filename = os.path.basename(csv_file)
        print(f"[{i}/{len(csv_files)}] å¤„ç† {filename}...")
        
        daily_data = process_daily_file(csv_file)
        
        # åˆå¹¶æ•°æ®
        for adj_type in all_data:
            for etf_code, rows in daily_data[adj_type].items():
                if etf_code not in all_data[adj_type]:
                    all_data[adj_type][etf_code] = []
                all_data[adj_type][etf_code].extend(rows)
    
    print()
    print("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶...")
    
    # ä¿å­˜æ•°æ®
    merge_and_save_etf_data(all_data, mode)
    
    # ç»Ÿè®¡ç»“æœ
    total_etfs = len(set().union(*[data.keys() for data in all_data.values()]))
    total_records = sum(len(rows) for data in all_data.values() for rows in data.values())
    
    print()
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(csv_files)}")
    print(f"   - ETFæ•°é‡: {total_etfs}")
    print(f"   - æ€»è®°å½•æ•°: {total_records}")
    print(f"   - ç”Ÿæˆç›®å½•: {', '.join(CATEGORIES)}")
    print()
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("   - æ—¥æ›´æ–°: python daily_etf_processor.py --mode daily")
    print("   - å…¨é‡é‡å»º: python daily_etf_processor.py --mode rebuild")
    print("   - æŒ‡å®šèŒƒå›´: python daily_etf_processor.py --mode range --start-date 20250601 --end-date 20250630")


if __name__ == "__main__":
    main() 